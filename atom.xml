<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Masonnpe&#39;s Blog</title>
  
  <subtitle>愿所有的坚持终不被辜负</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://masonnpe.github.io/"/>
  <updated>2018-11-25T15:12:23.509Z</updated>
  <id>https://masonnpe.github.io/</id>
  
  <author>
    <name>Masonnpe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL主从复制</title>
    <link href="https://masonnpe.github.io/2018/11/25/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"/>
    <id>https://masonnpe.github.io/2018/11/25/数据库/MySQL主从复制/</id>
    <published>2018-11-25T14:47:52.983Z</published>
    <updated>2018-11-25T15:12:23.509Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MySQL主从复制原理"><a href="#MySQL主从复制原理" class="headerlink" title="MySQL主从复制原理"></a>MySQL主从复制原理</h2><p><img src=".\MySQL主从复制原理.png" alt=""></p><a id="more"></a><p>主库将增删改操作写binlog日志，从库连接到主库之后有一个IO线程，将主库的binlog日志拷贝到本地，写入到一个中继日志中，接着从库中有一个SQL线程会从中继日志读取binlog日志，并执行binlog日志中的内容，也就是在本地再次执行一遍SQL，这样就可以保证自己跟主库的数据是一样的。这里有一个非常重要的点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。</p><p>如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。所以MySQL在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。</p><p>半同步复制（semi-sync复制），指的就是主库写入binlog日志之后，就会立即将数据同步到从库，从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库的ack之后才会认为写操作完成了。</p><p>并行复制，指的是从库开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。</p><h2 id="MySQL主从同步延时"><a href="#MySQL主从同步延时" class="headerlink" title="MySQL主从同步延时"></a>MySQL主从同步延时</h2><p> 一般在读远远多于写的场景下来用这个MySQL主从同步，而且读的时候一般对数据时效性要求没那么高。主从同步延时问题，会导致一些线上的bug难以发现。可以通过show status，Seconds_Behind_Master，看到从库复制主库的数据落后了多少ms。</p><p>虽然可以用MySQL的并行复制，但是那是库级别的并行，时候作用不是很大。所以对于那种写了之后立马就要保证可以查到的场景，采用强制读主库的方式，这样就可以保证可以读到数据。也可以重写业务代码比如第二句SQL不依赖第一句SQL，就直接更新不查询，如果依赖就先读判断结果是否为空，如果为空就报错下次重试。</p><h2 id="实现MySQL的读写分离"><a href="#实现MySQL的读写分离" class="headerlink" title="实现MySQL的读写分离"></a>实现MySQL的读写分离</h2><p>基于主从复制架构，简单来说就是搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;MySQL主从复制原理&quot;&gt;&lt;a href=&quot;#MySQL主从复制原理&quot; class=&quot;headerlink&quot; title=&quot;MySQL主从复制原理&quot;&gt;&lt;/a&gt;MySQL主从复制原理&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;.\MySQL主从复制原理.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据库" scheme="https://masonnpe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>MyBatis缓存</title>
    <link href="https://masonnpe.github.io/2018/11/25/MyBatis/MyBatis%E7%BC%93%E5%AD%98/"/>
    <id>https://masonnpe.github.io/2018/11/25/MyBatis/MyBatis缓存/</id>
    <published>2018-11-25T09:57:09.299Z</published>
    <updated>2018-11-25T15:20:50.472Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>MyBatis是常见的Java数据库访问层框架。在日常工作中，开发人员多数情况下是使用MyBatis的默认缓存配置，但是MyBatis缓存机制有一些不足之处，在使用中容易引起脏数据，形成一些潜在的隐患。个人在业务开发中也处理过一些由于MyBatis缓存引发的开发问题，带着个人的兴趣，希望从应用及源码的角度为读者梳理MyBatis缓存机制。<br>本次分析中涉及到的代码和数据库表均放在GitHub上，地址： <a href="https://github.com/kailuncen/mybatis-cache-demo" target="_blank" rel="noopener">mybatis-cache-demo</a> 。</p><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p>本文按照以下顺序展开。</p><ul><li>一级缓存介绍及相关配置。</li><li>一级缓存工作流程及源码分析。</li><li>一级缓存总结。</li><li>二级缓存介绍及相关配置。</li><li>二级缓存源码分析。</li><li>二级缓存总结。</li><li>全文总结。</li></ul><a id="more"></a><h3 id="一级缓存"><a href="#一级缓存" class="headerlink" title="一级缓存"></a>一级缓存</h3><h4 id="一级缓存介绍"><a href="#一级缓存介绍" class="headerlink" title="一级缓存介绍"></a>一级缓存介绍</h4><p>在应用运行过程中，我们有可能在一次数据库会话中，执行多次查询条件完全相同的SQL，MyBatis提供了一级缓存的方案优化这部分场景，如果是相同的SQL语句，会优先命中一级缓存，避免直接对数据库进行查询，提高性能。具体执行过程如下图所示。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-29-47.jpg" alt="img"><br>每个SqlSession中持有了Executor，每个Executor中有一个LocalCache。当用户发起查询时，MyBatis根据当前执行的语句生成MappedStatement，在Local Cache进行查询，如果缓存命中的话，直接返回结果给用户，如果缓存没有命中的话，查询数据库，结果写入Local Cache，最后返回结果给用户。具体实现类的类关系图如下图所示。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-23-16-13-29.jpg" alt="img"></p><h4 id="一级缓存配置"><a href="#一级缓存配置" class="headerlink" title="一级缓存配置"></a>一级缓存配置</h4><p>我们来看看如何使用MyBatis一级缓存。开发者只需在MyBatis的配置文件中，添加如下语句，就可以使用一级缓存。共有两个选项，SESSION或者STATEMENT，默认是SESSION级别，即在一个MyBatis会话中执行的所有语句，都会共享这一个缓存。一种是STATEMENT级别，可以理解为缓存只对当前执行的这一个Statement有效。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt;</span><br></pre></td></tr></table></figure><h4 id="一级缓存实验"><a href="#一级缓存实验" class="headerlink" title="一级缓存实验"></a>一级缓存实验</h4><p>接下来通过实验，了解MyBatis一级缓存的效果，每个单元测试后都请恢复被修改的数据。<br>首先是创建示例表student，创建对应的POJO类和增改的方法，具体可以在entity包和mapper包中查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE `student` (</span><br><span class="line">  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(200) COLLATE utf8_bin DEFAULT NULL,</span><br><span class="line">  `age` tinyint(3) unsigned DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COLLATE=utf8_bin;</span><br></pre></td></tr></table></figure><p>在以下实验中，id为1的学生名称是凯伦。</p><h5 id="实验1"><a href="#实验1" class="headerlink" title="实验1"></a>实验1</h5><p>开启一级缓存，范围为会话级别，调用三次getStudentById，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void getStudentById() throws Exception &#123;</span><br><span class="line">        SqlSession sqlSession = factory.openSession(true); // 自动提交事务</span><br><span class="line">        StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class);</span><br><span class="line">        System.out.println(studentMapper.getStudentById(1));</span><br><span class="line">        System.out.println(studentMapper.getStudentById(1));</span><br><span class="line">        System.out.println(studentMapper.getStudentById(1));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>执行结果：<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-30-06.jpg" alt="img"><br>我们可以看到，只有第一次真正查询了数据库，后续的查询使用了一级缓存。</p><h5 id="实验2"><a href="#实验2" class="headerlink" title="实验2"></a>实验2</h5><p>增加了对数据库的修改操作，验证在一次数据库会话中，如果对数据库发生了修改操作，一级缓存是否会失效。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void addStudent() throws Exception &#123;</span><br><span class="line">        SqlSession sqlSession = factory.openSession(true); // 自动提交事务</span><br><span class="line">        StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class);</span><br><span class="line">        System.out.println(studentMapper.getStudentById(1));</span><br><span class="line">        System.out.println(&quot;增加了&quot; + studentMapper.addStudent(buildStudent()) + &quot;个学生&quot;);</span><br><span class="line">        System.out.println(studentMapper.getStudentById(1));</span><br><span class="line">        sqlSession.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行结果：<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-30-25.jpg" alt="img"><br>我们可以看到，在修改操作后执行的相同查询，查询了数据库，<strong>一级缓存失效</strong>。</p><h5 id="实验3"><a href="#实验3" class="headerlink" title="实验3"></a>实验3</h5><p>开启两个SqlSession，在sqlSession1中查询数据，使一级缓存生效，在sqlSession2中更新数据库，验证一级缓存只在数据库会话内部共享。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testLocalCacheScope() throws Exception &#123;</span><br><span class="line">        SqlSession sqlSession1 = factory.openSession(true); </span><br><span class="line">        SqlSession sqlSession2 = factory.openSession(true); </span><br><span class="line"></span><br><span class="line">        StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class);</span><br><span class="line">        StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class);</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1));</span><br><span class="line">        System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1));</span><br><span class="line">        System.out.println(&quot;studentMapper2更新了&quot; + studentMapper2.updateStudentName(&quot;小岑&quot;,1) + &quot;个学生的数据&quot;);</span><br><span class="line">        System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1));</span><br><span class="line">        System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-30-44.jpg" alt="img"><br>sqlSession2更新了id为1的学生的姓名，从凯伦改为了小岑，但session1之后的查询中，id为1的学生的名字还是凯伦，出现了脏数据，也证明了之前的设想，一级缓存只在数据库会话内部共享。</p><h4 id="一级缓存工作流程-amp-源码分析"><a href="#一级缓存工作流程-amp-源码分析" class="headerlink" title="一级缓存工作流程&amp;源码分析"></a>一级缓存工作流程&amp;源码分析</h4><p>那么，一级缓存的工作流程是怎样的呢？我们从源码层面来学习一下。</p><h5 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h5><p>一级缓存执行的时序图，如下图所示。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-31-11.png" alt="img"></p><h5 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h5><p>接下来将对MyBatis查询相关的核心类和一级缓存的源码进行走读。这对后面学习二级缓存也有帮助。<br><strong>SqlSession</strong>： 对外提供了用户和数据库之间交互需要的所有方法，隐藏了底层的细节。默认实现类是DefaultSqlSession。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-31-26.jpg" alt="img"></p><p><strong>Executor</strong>： SqlSession向用户提供操作数据库的方法，但和数据库操作有关的职责都会委托给Executor。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-31-47.jpg" alt="img"></p><p>如下图所示，Executor有若干个实现类，为Executor赋予了不同的能力，大家可以根据类名，自行学习每个类的基本作用。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-32-09.jpg" alt="img"></p><p>在一级缓存的源码分析中，主要学习BaseExecutor的内部实现。<br><strong>BaseExecutor</strong>： BaseExecutor是一个实现了Executor接口的抽象类，定义若干抽象方法，在执行的时候，把具体的操作委托给子类进行执行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">protected abstract int doUpdate(MappedStatement ms, Object parameter) throws SQLException;</span><br><span class="line">protected abstract List&lt;BatchResult&gt; doFlushStatements(boolean isRollback) throws SQLException;</span><br><span class="line">protected abstract &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException;</span><br><span class="line">protected abstract &lt;E&gt; Cursor&lt;E&gt; doQueryCursor(MappedStatement ms, Object parameter, RowBounds rowBounds, BoundSql boundSql) throws SQLException;</span><br></pre></td></tr></table></figure><p>在一级缓存的介绍中提到对Local Cache的查询和写入是在Executor内部完成的。在阅读BaseExecutor的代码后发现Local Cache是BaseExecutor内部的一个成员变量，如下代码所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public abstract class BaseExecutor implements Executor &#123;</span><br><span class="line">protected ConcurrentLinkedQueue&lt;DeferredLoad&gt; deferredLoads;</span><br><span class="line">protected PerpetualCache localCache;</span><br></pre></td></tr></table></figure><p><strong>Cache</strong>： MyBatis中的Cache接口，提供了和缓存相关的最基本的操作，如下图所示。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-32-35.jpg" alt="img"><br>有若干个实现类，使用装饰器模式互相组装，提供丰富的操控缓存的能力，部分实现类如下图所示。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-23-16-33-35.jpg" alt="img"><br>BaseExecutor成员变量之一的PerpetualCache，是对Cache接口最基本的实现，其实现非常简单，内部持有HashMap，对一级缓存的操作实则是对HashMap的操作。如下代码所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public class PerpetualCache implements Cache &#123;</span><br><span class="line">  private String id;</span><br><span class="line">  private Map&lt;Object, Object&gt; cache = new HashMap&lt;Object, Object&gt;();</span><br></pre></td></tr></table></figure><p>在阅读相关核心类代码后，从源代码层面对一级缓存工作中涉及到的相关代码，出于篇幅的考虑，对源码做适当删减，读者朋友可以结合本文，后续进行更详细的学习。<br>为执行和数据库的交互，首先需要初始化SqlSession，通过DefaultSqlSessionFactory开启SqlSession：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123;</span><br><span class="line">    ............</span><br><span class="line">    final Executor executor = configuration.newExecutor(tx, execType);     </span><br><span class="line">    return new DefaultSqlSession(configuration, executor, autoCommit);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在初始化SqlSesion时，会使用Configuration类创建一个全新的Executor，作为DefaultSqlSession构造函数的参数，创建Executor代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123;</span><br><span class="line">    executorType = executorType == null ? defaultExecutorType : executorType;</span><br><span class="line">    executorType = executorType == null ? ExecutorType.SIMPLE : executorType;</span><br><span class="line">    Executor executor;</span><br><span class="line">    if (ExecutorType.BATCH == executorType) &#123;</span><br><span class="line">      executor = new BatchExecutor(this, transaction);</span><br><span class="line">    &#125; else if (ExecutorType.REUSE == executorType) &#123;</span><br><span class="line">      executor = new ReuseExecutor(this, transaction);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      executor = new SimpleExecutor(this, transaction);</span><br><span class="line">    &#125;</span><br><span class="line">    // 尤其可以注意这里，如果二级缓存开关开启的话，是使用CahingExecutor装饰BaseExecutor的子类</span><br><span class="line">    if (cacheEnabled) &#123;</span><br><span class="line">      executor = new CachingExecutor(executor);                      </span><br><span class="line">    &#125;</span><br><span class="line">    executor = (Executor) interceptorChain.pluginAll(executor);</span><br><span class="line">    return executor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SqlSession创建完毕后，根据Statment的不同类型，会进入SqlSession的不同方法中，如果是Select语句的话，最后会执行到SqlSession的selectList，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123;</span><br><span class="line">      MappedStatement ms = configuration.getMappedStatement(statement);</span><br><span class="line">      return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SqlSession把具体的查询职责委托给了Executor。如果只开启了一级缓存的话，首先会进入BaseExecutor的query方法。代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123;</span><br><span class="line">    BoundSql boundSql = ms.getBoundSql(parameter);</span><br><span class="line">    CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);</span><br><span class="line">    return query(ms, parameter, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上述代码中，会先根据传入的参数生成CacheKey，进入该方法查看CacheKey是如何生成的，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CacheKey cacheKey = new CacheKey();</span><br><span class="line">cacheKey.update(ms.getId());</span><br><span class="line">cacheKey.update(rowBounds.getOffset());</span><br><span class="line">cacheKey.update(rowBounds.getLimit());</span><br><span class="line">cacheKey.update(boundSql.getSql());</span><br><span class="line">//后面是update了sql中带的参数</span><br><span class="line">cacheKey.update(value);</span><br></pre></td></tr></table></figure><p>在上述的代码中，将MappedStatement的Id、sql的offset、Sql的limit、Sql本身以及Sql中的参数传入了CacheKey这个类，最终构成CacheKey。以下是这个类的内部结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">private static final int DEFAULT_MULTIPLYER = 37;</span><br><span class="line">private static final int DEFAULT_HASHCODE = 17;</span><br><span class="line"></span><br><span class="line">private int multiplier;</span><br><span class="line">private int hashcode;</span><br><span class="line">private long checksum;</span><br><span class="line">private int count;</span><br><span class="line">private List&lt;Object&gt; updateList;</span><br><span class="line"></span><br><span class="line">public CacheKey() &#123;</span><br><span class="line">    this.hashcode = DEFAULT_HASHCODE;</span><br><span class="line">    this.multiplier = DEFAULT_MULTIPLYER;</span><br><span class="line">    this.count = 0;</span><br><span class="line">    this.updateList = new ArrayList&lt;Object&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先是成员变量和构造函数，有一个初始的hachcode和乘数，同时维护了一个内部的updatelist。在CacheKey的update方法中，会进行一个hashcode和checksum的计算，同时把传入的参数添加进updatelist中。如下代码所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public void update(Object object) &#123;</span><br><span class="line">    int baseHashCode = object == null ? 1 : ArrayUtil.hashCode(object); </span><br><span class="line">    count++;</span><br><span class="line">    checksum += baseHashCode;</span><br><span class="line">    baseHashCode *= count;</span><br><span class="line">    hashcode = multiplier * hashcode + baseHashCode;</span><br><span class="line"></span><br><span class="line">    updateList.add(object);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同时重写了CacheKey的equals方法，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public boolean equals(Object object) &#123;</span><br><span class="line">    .............</span><br><span class="line">    for (int i = 0; i &lt; updateList.size(); i++) &#123;</span><br><span class="line">      Object thisObject = updateList.get(i);</span><br><span class="line">      Object thatObject = cacheKey.updateList.get(i);</span><br><span class="line">      if (!ArrayUtil.equals(thisObject, thatObject)) &#123;</span><br><span class="line">        return false;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>除去hashcode，checksum和count的比较外，只要updatelist中的元素一一对应相等，那么就可以认为是CacheKey相等。只要两条SQL的下列五个值相同，即可以认为是相同的SQL。</p><blockquote><p>Statement Id + Offset + Limmit + Sql + Params</p></blockquote><p>BaseExecutor的query方法继续往下走，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null;</span><br><span class="line">if (list != null) &#123;</span><br><span class="line">    // 这个主要是处理存储过程用的。</span><br><span class="line">    handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">    list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果查不到的话，就从数据库查，在queryFromDatabase中，会对localcache进行写入。<br>在query方法执行的最后，会判断一级缓存级别是否是STATEMENT级别，如果是的话，就清空缓存，这也就是STATEMENT级别的一级缓存无法共享localCache的原因。代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123;</span><br><span class="line">        clearLocalCache();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在源码分析的最后，我们确认一下，如果是insert/delete/update方法，缓存就会刷新的原因。<br>SqlSession的insert方法和delete方法，都会统一走update的流程，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public int insert(String statement, Object parameter) &#123;</span><br><span class="line">    return update(statement, parameter);</span><br><span class="line">  &#125;</span><br><span class="line">   @Override</span><br><span class="line">  public int delete(String statement) &#123;</span><br><span class="line">    return update(statement, null);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>update方法也是委托给了Executor执行。BaseExecutor的执行方法如下所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public int update(MappedStatement ms, Object parameter) throws SQLException &#123;</span><br><span class="line">    ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing an update&quot;).object(ms.getId());</span><br><span class="line">    if (closed) &#123;</span><br><span class="line">      throw new ExecutorException(&quot;Executor was closed.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    clearLocalCache();</span><br><span class="line">    return doUpdate(ms, parameter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每次执行update前都会清空localCache。</p><p>至此，一级缓存的工作流程讲解以及源码分析完毕。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ol><li>MyBatis一级缓存的生命周期和SqlSession一致。</li><li>MyBatis一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。</li><li>MyBatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement。</li></ol><h3 id="二级缓存"><a href="#二级缓存" class="headerlink" title="二级缓存"></a>二级缓存</h3><h4 id="二级缓存介绍"><a href="#二级缓存介绍" class="headerlink" title="二级缓存介绍"></a>二级缓存介绍</h4><p>在上文中提到的一级缓存中，其最大的共享范围就是一个SqlSession内部，如果多个SqlSession之间需要共享缓存，则需要使用到二级缓存。开启二级缓存后，会使用CachingExecutor装饰Executor，进入一级缓存的查询流程前，先在CachingExecutor进行二级缓存的查询，具体的工作流程如下所示。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-33-42.png" alt="img"></p><p>二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。<br>当开启缓存后，数据的查询执行的流程就是 二级缓存 -&gt; 一级缓存 -&gt; 数据库。</p><h4 id="二级缓存配置"><a href="#二级缓存配置" class="headerlink" title="二级缓存配置"></a>二级缓存配置</h4><p>要正确的使用二级缓存，需完成如下配置的。</p><ol><li><p>在MyBatis的配置文件中开启二级缓存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;</span><br></pre></td></tr></table></figure></li><li><p>在MyBatis的映射XML中配置cache或者 cache-ref 。</p></li></ol><p>cache标签用于声明这个namespace使用二级缓存，并且可以自定义配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;cache/&gt;</span><br></pre></td></tr></table></figure><ul><li>type：cache使用的类型，默认是PerpetualCache，这在一级缓存中提到过。</li><li>eviction： 定义回收的策略，常见的有FIFO，LRU。</li><li>flushInterval： 配置一定时间自动刷新缓存，单位是毫秒。</li><li>size： 最多缓存对象的个数。</li><li>readOnly： 是否只读，若配置可读写，则需要对应的实体类能够序列化。</li><li>blocking： 若缓存中找不到对应的key，是否会一直blocking，直到有对应的数据进入缓存。</li></ul><p>cache-ref代表引用别的命名空间的Cache配置，两个命名空间的操作使用的是同一个Cache。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;cache-ref namespace=&quot;mapper.StudentMapper&quot;/&gt;</span><br></pre></td></tr></table></figure><h4 id="二级缓存实验"><a href="#二级缓存实验" class="headerlink" title="二级缓存实验"></a>二级缓存实验</h4><p>接下来我们通过实验，了解MyBatis二级缓存在使用上的一些特点。<br>在本实验中，id为1的学生名称初始化为点点。</p><h5 id="实验1-1"><a href="#实验1-1" class="headerlink" title="实验1"></a>实验1</h5><p>测试二级缓存效果，不提交事务，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCacheWithoutCommitOrClose() throws Exception &#123;</span><br><span class="line">        SqlSession sqlSession1 = factory.openSession(true); </span><br><span class="line">        SqlSession sqlSession2 = factory.openSession(true); </span><br><span class="line"></span><br><span class="line">        StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class);</span><br><span class="line">        StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class);</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1));</span><br><span class="line">        System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行结果：<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-34-04.jpg" alt="img"><br>我们可以看到，当sqlsession没有调用commit()方法时，二级缓存并没有起到作用。</p><h5 id="实验2-1"><a href="#实验2-1" class="headerlink" title="实验2"></a>实验2</h5><p>测试二级缓存效果，当提交事务时，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCacheWithCommitOrClose() throws Exception &#123;</span><br><span class="line">        SqlSession sqlSession1 = factory.openSession(true); </span><br><span class="line">        SqlSession sqlSession2 = factory.openSession(true); </span><br><span class="line"></span><br><span class="line">        StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class);</span><br><span class="line">        StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class);</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1));</span><br><span class="line">        sqlSession1.commit();</span><br><span class="line">        System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-34-25.jpg" alt="img"><br>从图上可知，sqlsession2的查询，使用了缓存，缓存的命中率是0.5。</p><h5 id="实验3-1"><a href="#实验3-1" class="headerlink" title="实验3"></a>实验3</h5><p>测试update操作是否会刷新该namespace下的二级缓存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCacheWithUpdate() throws Exception &#123;</span><br><span class="line">        SqlSession sqlSession1 = factory.openSession(true); </span><br><span class="line">        SqlSession sqlSession2 = factory.openSession(true); </span><br><span class="line">        SqlSession sqlSession3 = factory.openSession(true); </span><br><span class="line"></span><br><span class="line">        StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class);</span><br><span class="line">        StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class);</span><br><span class="line">        StudentMapper studentMapper3 = sqlSession3.getMapper(StudentMapper.class);</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1));</span><br><span class="line">        sqlSession1.commit();</span><br><span class="line">        System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));</span><br><span class="line"></span><br><span class="line">        studentMapper3.updateStudentName(&quot;方方&quot;,1);</span><br><span class="line">        sqlSession3.commit();</span><br><span class="line">        System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-34-48.jpg" alt="img"><br>我们可以看到，在sqlSession3更新数据库，并提交事务后，sqlsession2的StudentMapper namespace下的查询走了数据库，没有走Cache。</p><h5 id="实验4"><a href="#实验4" class="headerlink" title="实验4"></a>实验4</h5><p>验证MyBatis的二级缓存不适应用于映射文件中存在多表查询的情况。<br>通常我们会为每个单表创建单独的映射文件，由于MyBatis的二级缓存是基于namespace的，多表查询语句所在的namspace无法感应到其他namespace中的语句对多表查询中涉及的表进行的修改，引发脏数据问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCacheWithDiffererntNamespace() throws Exception &#123;</span><br><span class="line">        SqlSession sqlSession1 = factory.openSession(true); </span><br><span class="line">        SqlSession sqlSession2 = factory.openSession(true); </span><br><span class="line">        SqlSession sqlSession3 = factory.openSession(true); </span><br><span class="line"></span><br><span class="line">        StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class);</span><br><span class="line">        StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class);</span><br><span class="line">        ClassMapper classMapper = sqlSession3.getMapper(ClassMapper.class);</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentByIdWithClassInfo(1));</span><br><span class="line">        sqlSession1.close();</span><br><span class="line">        System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentByIdWithClassInfo(1));</span><br><span class="line"></span><br><span class="line">        classMapper.updateClassName(&quot;特色一班&quot;,1);</span><br><span class="line">        sqlSession3.commit();</span><br><span class="line">        System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentByIdWithClassInfo(1));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行结果：<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-35-29.jpg" alt="img"><br>在这个实验中，我们引入了两张新的表，一张class，一张classroom。class中保存了班级的id和班级名，classroom中保存了班级id和学生id。我们在StudentMapper中增加了一个查询方法getStudentByIdWithClassInfo，用于查询学生所在的班级，涉及到多表查询。在ClassMapper中添加了updateClassName，根据班级id更新班级名的操作。<br>当sqlsession1的studentmapper查询数据后，二级缓存生效。保存在StudentMapper的namespace下的cache中。当sqlSession3的classMapper的updateClassName方法对class表进行更新时，updateClassName不属于StudentMapper的namespace，所以StudentMapper下的cache没有感应到变化，没有刷新缓存。当StudentMapper中同样的查询再次发起时，从缓存中读取了脏数据。</p><h5 id="实验5"><a href="#实验5" class="headerlink" title="实验5"></a>实验5</h5><p>为了解决实验4的问题呢，可以使用Cache ref，让ClassMapper引用StudenMapper命名空间，这样两个映射文件对应的Sql操作都使用的是同一块缓存了。<br>执行结果：<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-35-45.jpg" alt="img"><br>不过这样做的后果是，缓存的粒度变粗了，多个Mapper namespace下的所有操作都会对缓存使用造成影响。</p><h4 id="二级缓存源码分析"><a href="#二级缓存源码分析" class="headerlink" title="二级缓存源码分析"></a>二级缓存源码分析</h4><p>MyBatis二级缓存的工作流程和前文提到的一级缓存类似，只是在一级缓存处理前，用CachingExecutor装饰了BaseExecutor的子类，在委托具体职责给delegate之前，实现了二级缓存的查询和写入功能，具体类关系图如下图所示。<br><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-23-16-57-08.jpg" alt="img"></p><h5 id="源码分析-1"><a href="#源码分析-1" class="headerlink" title="源码分析"></a>源码分析</h5><p>源码分析从CachingExecutor的query方法展开，源代码走读过程中涉及到的知识点较多，不能一一详细讲解，读者朋友可以自行查询相关资料来学习。<br>CachingExecutor的query方法，首先会从MappedStatement中获得在配置初始化时赋予的Cache。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cache cache = ms.getCache();</span><br></pre></td></tr></table></figure><p>本质上是装饰器模式的使用，具体的装饰链是</p><blockquote><p>SynchronizedCache -&gt; LoggingCache -&gt; SerializedCache -&gt; LruCache -&gt; PerpetualCache。</p></blockquote><p><img src="https://tech.meituan.com/img/mybatis-cache/2017-11-15-16-36-08.jpg" alt="img"></p><p>以下是具体这些Cache实现类的介绍，他们的组合为Cache赋予了不同的能力。</p><ul><li>SynchronizedCache： 同步Cache，实现比较简单，直接使用synchronized修饰方法。</li><li>LoggingCache： 日志功能，装饰类，用于记录缓存的命中率，如果开启了DEBUG模式，则会输出命中率日志。</li><li>SerializedCache： 序列化功能，将值序列化后存到缓存中。该功能用于缓存返回一份实例的Copy，用于保存线程安全。</li><li>LruCache： 采用了Lru算法的Cache实现，移除最近最少使用的key/value。</li><li>PerpetualCache： 作为为最基础的缓存类，底层实现比较简单，直接使用了HashMap。</li></ul><p>然后是判断是否需要刷新缓存，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flushCacheIfRequired(ms);</span><br></pre></td></tr></table></figure><p>在默认的设置中SELECT语句不会刷新缓存，insert/update/delte会刷新缓存。进入该方法。代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private void flushCacheIfRequired(MappedStatement ms) &#123;</span><br><span class="line">    Cache cache = ms.getCache();</span><br><span class="line">    if (cache != null &amp;&amp; ms.isFlushCacheRequired()) &#123;      </span><br><span class="line">      tcm.clear(cache);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MyBatis的CachingExecutor持有了TransactionalCacheManager，即上述代码中的tcm。<br>TransactionalCacheManager中持有了一个Map，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">private Map&lt;Cache, TransactionalCache&gt; transactionalCaches = new HashMap&lt;Cache, TransactionalCache&gt;();</span><br></pre></td></tr></table></figure><p>这个Map保存了Cache和用TransactionalCache包装后的Cache的映射关系。<br>TransactionalCache实现了Cache接口，CachingExecutor会默认使用他包装初始生成的Cache，作用是如果事务提交，对缓存的操作才会生效，如果事务回滚或者不提交事务，则不对缓存产生影响。<br>在TransactionalCache的clear，有以下两句。清空了需要在提交时加入缓存的列表，同时设定提交时清空缓存，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void clear() &#123;</span><br><span class="line">    clearOnCommit = true;</span><br><span class="line">    entriesToAddOnCommit.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CachingExecutor继续往下走，ensureNoOutParams主要是用来处理存储过程的，暂时不用考虑。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123;</span><br><span class="line">    ensureNoOutParams(ms, parameterObject, boundSql);</span><br></pre></td></tr></table></figure><p>之后会尝试从tcm中获取缓存的列表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key);</span><br></pre></td></tr></table></figure><p>在getObject方法中，会把获取值的职责一路传递，最终到PerpetualCache。如果没有查到，会把key加入Miss集合，这个主要是为了统计命中率。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Object object = delegate.getObject(key);</span><br><span class="line">if (object == null) &#123;</span><br><span class="line">    entriesMissedInCache.add(key);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CachingExecutor继续往下走，如果查询到数据，则调用tcm.putObject方法，往缓存中放入值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if (list == null) &#123;</span><br><span class="line">    list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);</span><br><span class="line">    tcm.putObject(cache, key, list); // issue #578 and #116</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>tcm的put方法也不是直接操作缓存，只是在把这次的数据和key放入待提交的Map中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void putObject(Object key, Object object) &#123;</span><br><span class="line">    entriesToAddOnCommit.put(key, object);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从以上的代码分析中，我们可以明白，如果不调用commit方法的话，由于TranscationalCache的作用，并不会对二级缓存造成直接的影响。因此我们看看Sqlsession的commit方法中做了什么。代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void commit(boolean force) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      executor.commit(isCommitOrRollbackRequired(force));</span><br></pre></td></tr></table></figure><p>因为我们使用了CachingExecutor，首先会进入CachingExecutor实现的commit方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void commit(boolean required) throws SQLException &#123;</span><br><span class="line">    delegate.commit(required);</span><br><span class="line">    tcm.commit();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>会把具体commit的职责委托给包装的Executor。主要是看下tcm.commit()，tcm最终又会调用到TrancationalCache。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void commit() &#123;</span><br><span class="line">    if (clearOnCommit) &#123;</span><br><span class="line">      delegate.clear();</span><br><span class="line">    &#125;</span><br><span class="line">    flushPendingEntries();</span><br><span class="line">    reset();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看到这里的clearOnCommit就想起刚才TrancationalCache的clear方法设置的标志位，真正的清理Cache是放到这里来进行的。具体清理的职责委托给了包装的Cache类。之后进入flushPendingEntries方法。代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private void flushPendingEntries() &#123;</span><br><span class="line">    for (Map.Entry&lt;Object, Object&gt; entry : entriesToAddOnCommit.entrySet()) &#123;</span><br><span class="line">      delegate.putObject(entry.getKey(), entry.getValue());</span><br><span class="line">    &#125;</span><br><span class="line">    ................</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在flushPendingEntries中，将待提交的Map进行循环处理，委托给包装的Cache类，进行putObject的操作。<br>后续的查询操作会重复执行这套流程。如果是insert|update|delete的话，会统一进入CachingExecutor的update方法，其中调用了这个函数，代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">private void flushCacheIfRequired(MappedStatement ms)</span><br></pre></td></tr></table></figure><p>在二级缓存执行流程后就会进入一级缓存的执行流程，因此不再赘述。</p><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><ol><li>MyBatis的二级缓存相对于一级缓存来说，实现了SqlSession之间缓存数据的共享，同时粒度更加的细，能够到namespace级别，通过Cache接口实现类不同的组合，对Cache的可控性也更强。</li><li>MyBatis在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。</li><li>在分布式环境下，由于默认的MyBatis Cache实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将MyBatis的Cache接口实现，有一定的开发成本，直接使用Redis,Memcached等分布式缓存可能成本更低，安全性也更高。</li></ol><h3 id="全文总结"><a href="#全文总结" class="headerlink" title="全文总结"></a>全文总结</h3><p>本文对介绍了MyBatis一二级缓存的基本概念，并从应用及源码的角度对MyBatis的缓存机制进行了分析。最后对MyBatis缓存机制做了一定的总结，个人建议MyBatis缓存特性在生产环境中进行关闭，单纯作为一个ORM框架使用可能更为合适。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;MyBatis是常见的Java数据库访问层框架。在日常工作中，开发人员多数情况下是使用MyBatis的默认缓存配置，但是MyBatis缓存机制有一些不足之处，在使用中容易引起脏数据，形成一些潜在的隐患。个人在业务开发中也处理过一些由于MyBatis缓存引发的开发问题，带着个人的兴趣，希望从应用及源码的角度为读者梳理MyBatis缓存机制。&lt;br&gt;本次分析中涉及到的代码和数据库表均放在GitHub上，地址： &lt;a href=&quot;https://github.com/kailuncen/mybatis-cache-demo&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;mybatis-cache-demo&lt;/a&gt; 。&lt;/p&gt;
&lt;h3 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h3&gt;&lt;p&gt;本文按照以下顺序展开。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一级缓存介绍及相关配置。&lt;/li&gt;
&lt;li&gt;一级缓存工作流程及源码分析。&lt;/li&gt;
&lt;li&gt;一级缓存总结。&lt;/li&gt;
&lt;li&gt;二级缓存介绍及相关配置。&lt;/li&gt;
&lt;li&gt;二级缓存源码分析。&lt;/li&gt;
&lt;li&gt;二级缓存总结。&lt;/li&gt;
&lt;li&gt;全文总结。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Mybatis" scheme="https://masonnpe.github.io/categories/Mybatis/"/>
    
    
  </entry>
  
  <entry>
    <title>volatile实现原理</title>
    <link href="https://masonnpe.github.io/2018/11/25/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/volatile%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
    <id>https://masonnpe.github.io/2018/11/25/并发编程/volatile实现原理/</id>
    <published>2018-11-25T09:55:12.444Z</published>
    <updated>2018-11-25T09:55:44.076Z</updated>
    
    <content type="html"><![CDATA[<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p>在对被<code>volatile</code>修饰的共享变量进行写操作时，会多出<code>Lock</code>前缀的指令。作用是：</p><ol><li>将当前处理器缓存行的数据写回到系统内存；</li><li>使得其他CPU里缓存了该内存地址的数据无效；</li><li>当处理器发现本地缓存失效后，就会从内存中重新读取该变量数据。</li></ol><p>通过这样的机制就使得每个线程都能获得该变量的最新值，从而避免出现数据脏读的现象。</p><blockquote><p>为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，操作完成后不知道何时将数据写回到内存。如果对声明了<code>volatile</code>的变量进行写操作，虚拟机就会向处理器发送一条<code>Lock</code>前缀的指令，将这个变量所在缓存行的数据写回到系统内存。在多处理器下为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议（每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期），当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。</p></blockquote><a id="more"></a><h2 id="volatile与重排序"><a href="#volatile与重排序" class="headerlink" title="volatile与重排序"></a>volatile与重排序</h2><p>为了实现<code>volatile</code>的内存语义，JMM会通过插入内存屏障指令来限制特定类型的编译器和处理器重排序</p><ul><li>在每个<code>volatile</code>读后面插入一个LoadLoad屏障：禁止下面所有的普通读操作和上面的<code>volatile</code>读重排序</li><li>在每个<code>volatile</code>读后面插入一个LoadStore屏障：禁止下面所有的普通写操作和上面的<code>volatile</code>读重排序</li><li>在每个<code>volatile</code>写前面插入一个StoreStore屏障：禁止上面的普通写和下面的<code>volatile</code>写重排序</li><li>在每个<code>volatile</code>写后面插入一个StoreLoad屏障：防止上面的<code>volatile</code>写与下面可能有的<code>volatile</code>读/写重排序</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;实现原理&quot;&gt;&lt;a href=&quot;#实现原理&quot; class=&quot;headerlink&quot; title=&quot;实现原理&quot;&gt;&lt;/a&gt;实现原理&lt;/h2&gt;&lt;p&gt;在对被&lt;code&gt;volatile&lt;/code&gt;修饰的共享变量进行写操作时，会多出&lt;code&gt;Lock&lt;/code&gt;前缀的指令。作用是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将当前处理器缓存行的数据写回到系统内存；&lt;/li&gt;
&lt;li&gt;使得其他CPU里缓存了该内存地址的数据无效；&lt;/li&gt;
&lt;li&gt;当处理器发现本地缓存失效后，就会从内存中重新读取该变量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过这样的机制就使得每个线程都能获得该变量的最新值，从而避免出现数据脏读的现象。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，操作完成后不知道何时将数据写回到内存。如果对声明了&lt;code&gt;volatile&lt;/code&gt;的变量进行写操作，虚拟机就会向处理器发送一条&lt;code&gt;Lock&lt;/code&gt;前缀的指令，将这个变量所在缓存行的数据写回到系统内存。在多处理器下为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议（每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期），当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="并发编程" scheme="https://masonnpe.github.io/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="volatile" scheme="https://masonnpe.github.io/tags/volatile/"/>
    
  </entry>
  
  <entry>
    <title>ConcurrentHashMap的工作原理及代码实现</title>
    <link href="https://masonnpe.github.io/2018/11/12/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/ConcurrentHashMap%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://masonnpe.github.io/2018/11/12/并发编程/ConcurrentHashMap的工作原理及代码实现/</id>
    <published>2018-11-11T16:36:28.196Z</published>
    <updated>2018-11-25T15:22:18.373Z</updated>
    
    <content type="html"><![CDATA[<p>1.8以后的锁的颗粒度，是加在链表头上的，这个是个思路上的突破。</p><h2 id="为什么需要ConcurrentHashMap"><a href="#为什么需要ConcurrentHashMap" class="headerlink" title="为什么需要ConcurrentHashMap"></a>为什么需要ConcurrentHashMap</h2><p>Hashtable本身比较低效，因为它的实现基本就是将put、get、size等各种方法加上synchronized。这就导致了所有并发操作都要竞争同一把锁，大大降低了并发操作的效率。<br>HashMap不是线程安全的，并发情况会导致类似CPU占用100%等一些问题。Collections提供的同步包装器只是利用输入Map构造了另一个同步版本，所有操作虽然不再声明成为synchronized方法，但是还是利用了“this”作为互斥的mutex，没有真正意义上的改进！<br>所以Hashtable或者同步包装版本，都只是适合在非高度并发的场景下。</p><a id="more"></a><h2 id="ConcurrentHashMap分析"><a href="#ConcurrentHashMap分析" class="headerlink" title="ConcurrentHashMap分析"></a>ConcurrentHashMap分析</h2><p>早期ConcurrentHashMap，其实现是基于分段锁，也就是将内部进行分段（Segment），里面则是HashEntry的数组，和HashMap类似，哈希相同的条目也是以链表形式存放。HashEntry内部使用volatile的value字段来保证可见性，也利用了不可变对象的机制以改进利用Unsafe提供的底层能力，比如volatile access，去直接完成部分操作，以最优化性能，毕竟Unsafe中的很多操作都是JVM intrinsic优化过的。核心是利用分段设计，在进行并发操作的时候，只需要锁定相应段，这样就有效避免了类似Hashtable整体同步的问题，大大提高了性能。<br>对于put操作，首先是通过二次哈希避免哈希冲突，然后以Unsafe调用方式，直接获取相应的Segment，然后进行线程安全的put操作：<br>所以，从上面的源码清晰的看出，在进行并发写操作时：<br>ConcurrentHashMap会获取冲入锁，以保证数据一致性，Segment本身就是基于ReentrantLock的扩展实现，所以，在并发修改期间，相应Segment是被锁定的。<br>如果不进行同步，简单的计算所有Segment的总值，可能会因为并发put，导致结果不准确，但是直接锁定所有Segment进行计算，就会变得非常昂贵。其实，分离锁也限制了Map的初始化等操作。<br>所以，ConcurrentHashMap的实现是通过重试机制（RETRIES_BEFORE_LOCK，指定重试次数2），来试图获得可靠值。如果没有监控到发生变化（通过对比Segment.modCount），就直接返回，否则获取锁进行操作。<br>下面我来对比一下，在Java 8中ConcurrentHashMap<br>数据存储利用volatile来保证可见性。<br>使用CAS等操作，在特定场景进行无锁并发操作。<br>使用Unsafe、LongAdder之类底层手段，进行极端情况的优化。<br>先看看现在的数据存储内部实现，我们可以发现Key是fnal的，因为在生命周期中，一个条目的Key发生变化是不可能的；与此同时val，则声明为volatile，以保证可见性。<br>直接看并发的put是如何实现的。<br>fnal V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException();<br>int hash = spread(key.hashCode());<br>int binCount = 0;<br>for (Node[] tab = table;;) {<br>Node f; int n, i, fh; K fk; V fv;<br>if (tab == null || (n = tab.length) == 0)<br>tab = initTable();<br>else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) {<br>// 利用CAS去进行无锁线程安全操作，如果bin是空的<br>if (casTabAt(tab, i, null, new Node(hash, key, value)))<br>break;<br>}<br>else if ((fh = f.hash) == MOVED)<br>tab = helpTransfer(tab, f);<br>else if (onlyIfAbsent // 不加锁，进行检查<br>&amp;&amp; fh == hash<br>&amp;&amp; ((fk = f.key) == key || (fk != null &amp;&amp; key.equals(fk)))<br>&amp;&amp; (fv = f.val) != null)<br>return fv;<br>else {<br>V oldVal = null;<br>synchronized (f) {<br>// 细粒度的同步修改操作…<br>}<br>}<br>// Bin超过阈值，进行树化<br>if (binCount != 0) {<br>if (binCount &gt;= TREEIFY_THRESHOLD)<br>treeifyBin(tab, i);<br>if (oldVal != null)<br>return oldVal;<br>break;<br>}<br>}<br>}<br>addCount(1L, binCount);<br>return null;<br>}<br>初始化操作实现在initTable里面，这是一个典型的CAS使用场景，利用volatile的sizeCtl作为互斥手段：如果发现竞争性的初始化，就spin在那里，等待条件恢复；否则利用CAS设<br>置排他标志。如果成功则进行初始化；否则重试。<br>请参考下面代码：<br>private fnal Node[] initTable() {<br>Node[] tab; int sc;<br>while ((tab = table) == null || tab.length == 0) {<br>// 如果发现冲突，进行spin等待<br>if ((sc = sizeCtl) &lt; 0)<br>Thread.yield();<br>// CAS成功返回true，则进入真正的初始化逻辑<br>else if (U.compareAndSetInt(this, SIZECTL, sc, -1)) {<br>try {<br>if ((tab = table) == null || tab.length == 0) {<br>int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY;<br>@SuppressWarnings(“unchecked”)<br>Node[] nt = (Node[])new Node[n];<br>table = tab = nt;<br>sc = n - (n &gt;&gt;&gt; 2);<br>}<br>} fnally {<br>sizeCtl = sc;<br>}<br>break;<br>}<br>}<br>return tab;<br>}<br>当bin为空时，同样是没有必要锁定，也是以CAS操作去放置。<br>你有没有注意到，在同步逻辑上，它使用的是synchronized，而不是通常建议的ReentrantLock之类，这是为什么呢？现代JDK中，synchronized已经被不断优化，可以不再过分<br>极客时间<br>担心性能差异，另外，相比于ReentrantLock，它可以减少内存消耗，这是个非常大的优势。<br>与此同时，更多细节实现通过使用Unsafe进行了优化，例如tabAt就是直接利用getObjectAcquire，避免间接调用的开销。<br>satic fnal  Node tabAt(Node[] tab, int i) {<br>return (Node)U.getObjectAcquire(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);<br>}<br>再看看，现在是如何实现size操作的。阅读代码你会发现，真正的逻辑是在sumCount方法中， 那么sumCount做了什么呢？<br>fnal long sumCount() {<br>CounterCell[] as = counterCells; CounterCell a;<br>long sum = baseCount;<br>if (as != null) {<br>for (int i = 0; i &lt; as.length; ++i) {<br>if ((a = as[i]) != null)<br>sum += a.value;<br>}<br>}<br>return sum;<br>}<br>我们发现，虽然思路仍然和以前类似，都是分而治之的进行计数，然后求和处理，但实现却基于一个奇怪的CounterCell。 难道它的数值，就更加准确吗？数据一致性是怎么保证<br>的？<br>satic fnal class CounterCell {<br>volatile long value;<br>CounterCell(long x) { value = x; }<br>}<br>其实，对于CounterCell的操作，是基于java.util.concurrent.atomic.LongAdder进行的，是一种JVM利用空间换取更高效率的方法，利用了Striped64内部的复杂逻辑。这个东<br>西非常小众，大多数情况下，建议还是使用AtomicLong，足以满足绝大部分应用的性能需求。<br>今天我从线程安全问题开始，概念性的总结了基本容器工具，分析了早期同步容器的问题，进而分析了Java 7和Java 8中ConcurrentHashMap是如何设计实现的，希<br>望ConcurrentHashMap的并发技巧对你在日常开发可以有所帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1.8以后的锁的颗粒度，是加在链表头上的，这个是个思路上的突破。&lt;/p&gt;
&lt;h2 id=&quot;为什么需要ConcurrentHashMap&quot;&gt;&lt;a href=&quot;#为什么需要ConcurrentHashMap&quot; class=&quot;headerlink&quot; title=&quot;为什么需要ConcurrentHashMap&quot;&gt;&lt;/a&gt;为什么需要ConcurrentHashMap&lt;/h2&gt;&lt;p&gt;Hashtable本身比较低效，因为它的实现基本就是将put、get、size等各种方法加上synchronized。这就导致了所有并发操作都要竞争同一把锁，大大降低了并发操作的效率。&lt;br&gt;HashMap不是线程安全的，并发情况会导致类似CPU占用100%等一些问题。Collections提供的同步包装器只是利用输入Map构造了另一个同步版本，所有操作虽然不再声明成为synchronized方法，但是还是利用了“this”作为互斥的mutex，没有真正意义上的改进！&lt;br&gt;所以Hashtable或者同步包装版本，都只是适合在非高度并发的场景下。&lt;/p&gt;
    
    </summary>
    
      <category term="并发编程" scheme="https://masonnpe.github.io/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="ConcurrentHashMap" scheme="https://masonnpe.github.io/tags/ConcurrentHashMap/"/>
    
  </entry>
  
  <entry>
    <title>Docker与Java</title>
    <link href="https://masonnpe.github.io/2018/11/12/Docker/Docker%E4%B8%8EJava/"/>
    <id>https://masonnpe.github.io/2018/11/12/Docker/Docker与Java/</id>
    <published>2018-11-11T16:25:30.725Z</published>
    <updated>2018-11-25T15:23:22.241Z</updated>
    
    <content type="html"><![CDATA[<p>对于Java来说，Docker毕竟是一个较新的环境，例如，其内存、CPU等资源限制是通过CGroup（Control Group）实现的，早期的JDK版本（8u131之前）并不能识别这些限<br>制，进而会导致一些基础问题：<br>如果未配置合适的JVM堆和元数据区、直接内存等参数，Java就有可能试图使用超过容器限制的内存，最终被容器OOM kill，或者自身发生OOM。<br>错误判断了可获取的CPU资源，例如，Docker限制了CPU的核数，JVM就可能设置不合适的GC并行线程数等。<br>从应用打包、发布等角度出发，JDK自身就比较大，生成的镜像就更为臃肿，当我们的镜像非常多的时候，镜像的存储等开销就比较明显了。<br>如果考虑到微服务、Serverless等新的架构和场景，Java自身的大小、内存占用、启动速度，都存在一定局限性，因为Java早期的优化大多是针对长时间运行的大型服务器端应<br>用。<br>Java在容器环境的局限性来源，Docker到底有什么特别？<br>虽然看起来Docker之类容器和虚拟机非常相似，例如，它也有自己的shell，能独立安装软件包，运行时与其他容器互不干扰。但是，如果深入分析你会发现，Docker并不是一种完<br>全的虚拟化技术，而更是一种轻量级的隔离技术。<br><a id="more"></a></p><p>从技术角度，基于namespace，Docker为每个容器提供了单独的命名空间，对网络、PID、用户、IPC通信、文件系统挂载点等实<br>现了隔离。对于CPU、内存、磁盘IO等计算资源，则是通过CGroup进行管理。<br>Docker仅在类似Linux内核之上实现了有限的隔离和虚拟化，并不是像传统虚拟化软件那样，独立运行一个新的操作系统。如果是虚拟化的操作系统，不管是Java还是其他程序，只要调用的是同一个系统API，都可以透明地获取所需的信息，基本不需要额外的兼容性改变。容器虽然省略了虚拟操作系统的开销，实现了轻量级的目标，但也带来了额外复杂性，它限制对于应用不是透明的，需要用户理解Docker的新行为。<br>第一，容器环境对于计算资源的管理方式是全新的，CGroup作为相对比较新的技术，历史版本的Java显然并不能自然地理解相应的资源限制。<br>第二，namespace对于容器内的应用细节增加了一些微妙的差异，比如jcmd、jstack等工具会依赖于“/proc//”下面提供的部分信息，但是Docker的设计改变了这部分信息的原有<br>结构，我们需要对原有工具进行修改以适应这种变化。<br>从JVM运行机制的角度，为什么这些“沟通障碍”会导致OOM等问题呢？<br>你可以思考一下，这个问题实际是反映了JVM如何根据系统资源（内存、CPU等）情况，在启动时设置默认参数。<br>这就是所谓的Ergonomics机制，例如：<br>JVM会大概根据检测到的内存大小，设置最初启动时的堆大小为系统内存的1/64；并将堆最大值，设置为系统内存的1/4。<br>而JVM检测到系统的CPU核数，则直接影响到了Parallel GC的并行线程数目和JIT complier线程数目，甚至是我们应用中ForkJoinPool等机制的并行等级。<br>这些默认参数，是根据通用场景选择的初始值。但是由于容器环境的差异，Java的判断很可能是基于错误信息而做出的。这就类似，我以为我住的是整栋别墅，实际上却只有一个房<br>间是给我住的。<br>更加严重的是，JVM的一些原有诊断或备用机制也会受到影响。为保证服务的可用性，一种常见的选择是依赖“-XX:OnOutOfMemoryError”功能，通过调用处理脚本的形式来做一<br>些补救措施，比如自动重启服务等。但是，这种机制是基于fork实现的，当Java进程已经过度提交内存时，fork新的进程往往已经不可能正常运行了。<br>根据前面的总结，似乎问题非常棘手，那我们在实践中，如何解决这些问题呢？<br>首先，如果你能够升级到最新的JDK版本，这个问题就迎刃而解了。<br>针对这种情况，JDK 9中引入了一些实验性的参数，以方便Docker和Java“沟通”，例如针对内存限制，可以使用下面的参数设置：<br>-XX:+UnlockExperimentalVMOptions<br>-XX:+UseCGroupMemoryLimitForHeap<br>注意，这两个参数是顺序敏感的，并且只支持Linux环境。而对于CPU核心数限定，Java已经被修正为可以正确理解“–cpuset-cpus”等设置，无需单独设置参数。<br>如果你可以切换到JDK 10或者更新的版本，问题就更加简单了。Java对容器（Docker）的支持已经比较完善，默认就会自适应各种资源限制和实现差异。前面提到的实验性参<br>数“UseCGroupMemoryLimitForHeap”已经被标记为废弃。<br>与此同时，新增了参数用以明确指定CPU核心的数目。<br>-XX:ActiveProcessorCount=N<br>如果实践中发现有问题，也可以使用“-XX:-UseContainerSupport”，关闭Java的容器支持特性，这可以作为一种防御性机制，避免新特性破坏原有基础功能。当然，也欢迎你<br>向OpenJDK社区反馈问题。<br>幸运的是，JDK 9中的实验性改进已经被移植到Oracle JDK 8u131之中，你可以直接下载相应镜像，并配置“UseCGroupMemoryLimitForHeap”，后续很有可能还会进一步<br>将JDK 10中相关的增强，应用到JDK 8最新的更新中。<br>但是，如果我暂时只能使用老版本的JDK怎么办？<br>我这里有几个建议：<br>明确设置堆、元数据区等内存区域大小，保证Java进程的总大小可控。<br>例如，我们可能在环境中，这样限制容器内存：<br>$ docker run -it –rm –name yourcontainer -p 8080:8080 -m 800M repo/your-java-container:openjdk<br>极客时间<br>那么，就可以额外配置下面的环境变量，直接指定JVM堆大小。<br>-e JAVA_OPTIONS=’-Xmx300m’<br>明确配置GC和JIT并行线程数目，以避免二者占用过多计算资源。<br>-XX:ParallelGCThreads<br>-XX:CICompilerCount<br>除了我前面介绍的OOM等问题，在很多场景中还发现Java在Docker环境中，似乎会意外使用Swap。具体原因待查，但很有可能也是因为Ergonomics机制失效导致的，我建议配<br>置下面参数，明确告知JVM系统内存限额。<br>-XX:MaxRAM=<code>cat /sys/fs/cgroup/memory/memory.limit_in_bytes</code><br>也可以指定Docker运行参数，例如：<br>–memory-swappiness=0<br>这是受操作系统Swappiness机制影响，当内存消耗达到一定门限，操作系统会试图将不活跃的进程换出（Swap out），上面的参数有显式关闭Swap的作用。所以可以看<br>到，Java在Docker中的使用，从操作系统、内核到JVM自身机制，需要综合运用我们所掌握的知识。<br>回顾我在专栏第25讲JVM内存区域的介绍，JVM内存消耗远不止包括堆，很多时候仅仅设置Xmx是不够的，MaxRAM也有助于JVM合理分配其他内存区域。如果应用需要设置更<br>多Java启动参数，但又不确定什么数值合理，可以试试一些社区提供的工具，但要注意通用工具的局限性。<br>更进一步来说，对于容器镜像大小的问题，如果你使用的是JDK 9以后的版本，完全可以使用jlink工具定制最小依赖的Java运行环境，将JDK裁剪为几十M的大小，这样运行起来并<br>不困难。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于Java来说，Docker毕竟是一个较新的环境，例如，其内存、CPU等资源限制是通过CGroup（Control Group）实现的，早期的JDK版本（8u131之前）并不能识别这些限&lt;br&gt;制，进而会导致一些基础问题：&lt;br&gt;如果未配置合适的JVM堆和元数据区、直接内存等参数，Java就有可能试图使用超过容器限制的内存，最终被容器OOM kill，或者自身发生OOM。&lt;br&gt;错误判断了可获取的CPU资源，例如，Docker限制了CPU的核数，JVM就可能设置不合适的GC并行线程数等。&lt;br&gt;从应用打包、发布等角度出发，JDK自身就比较大，生成的镜像就更为臃肿，当我们的镜像非常多的时候，镜像的存储等开销就比较明显了。&lt;br&gt;如果考虑到微服务、Serverless等新的架构和场景，Java自身的大小、内存占用、启动速度，都存在一定局限性，因为Java早期的优化大多是针对长时间运行的大型服务器端应&lt;br&gt;用。&lt;br&gt;Java在容器环境的局限性来源，Docker到底有什么特别？&lt;br&gt;虽然看起来Docker之类容器和虚拟机非常相似，例如，它也有自己的shell，能独立安装软件包，运行时与其他容器互不干扰。但是，如果深入分析你会发现，Docker并不是一种完&lt;br&gt;全的虚拟化技术，而更是一种轻量级的隔离技术。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="https://masonnpe.github.io/categories/Docker/"/>
    
    
      <category term="Docker" scheme="https://masonnpe.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>分布式ID</title>
    <link href="https://masonnpe.github.io/2018/11/12/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8FID/"/>
    <id>https://masonnpe.github.io/2018/11/12/分布式/分布式ID/</id>
    <published>2018-11-11T16:16:42.188Z</published>
    <updated>2018-11-11T16:18:04.064Z</updated>
    
    <content type="html"><![CDATA[<h2 id="雪花算法"><a href="#雪花算法" class="headerlink" title="雪花算法"></a>雪花算法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">public class Snowflake &#123;</span><br><span class="line"></span><br><span class="line">  public static final int NODE_SHIFT = 10;</span><br><span class="line">  public static final int SEQ_SHIFT = 12;</span><br><span class="line"></span><br><span class="line">  public static final short MAX_NODE = 1024;</span><br><span class="line">  public static final short MAX_SEQUENCE = 4096;</span><br><span class="line"></span><br><span class="line">  private short sequence;</span><br><span class="line">  private long referenceTime;</span><br><span class="line"></span><br><span class="line">  private int node;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * A snowflake is designed to operate as a singleton instance within the context of a node.</span><br><span class="line">   * If you deploy different nodes, supplying a unique node id will guarantee the uniqueness</span><br><span class="line">   * of ids generated concurrently on different nodes.</span><br><span class="line">   *</span><br><span class="line">   * @param node This is an id you use to differentiate different nodes.</span><br><span class="line">   */</span><br><span class="line">  public Snowflake(int node) &#123;</span><br><span class="line">    if (node &lt; 0 || node &gt; MAX_NODE) &#123;</span><br><span class="line">      throw new IllegalArgumentException(String.format(&quot;node must be between %s and %s&quot;, 0, MAX_NODE));</span><br><span class="line">    &#125;</span><br><span class="line">    this.node = node;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Generates a k-ordered unique 64-bit integer. Subsequent invocations of this method will produce</span><br><span class="line">   * increasing integer values.</span><br><span class="line">   *</span><br><span class="line">   * @return The next 64-bit integer.</span><br><span class="line">   */</span><br><span class="line">  public long next() &#123;</span><br><span class="line"></span><br><span class="line">    long currentTime = System.currentTimeMillis();</span><br><span class="line">    long counter;</span><br><span class="line"></span><br><span class="line">    synchronized(this) &#123;</span><br><span class="line"></span><br><span class="line">      if (currentTime &lt; referenceTime) &#123;</span><br><span class="line">        throw new RuntimeException(String.format(&quot;Last referenceTime %s is after reference time %s&quot;, referenceTime, currentTime));</span><br><span class="line">      &#125; else if (currentTime &gt; referenceTime) &#123;</span><br><span class="line">        this.sequence = 0;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        if (this.sequence &lt; Snowflake.MAX_SEQUENCE) &#123;</span><br><span class="line">          this.sequence++;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          throw new RuntimeException(&quot;Sequence exhausted at &quot; + this.sequence);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      counter = this.sequence;</span><br><span class="line">      referenceTime = currentTime;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return currentTime &lt;&lt; NODE_SHIFT &lt;&lt; SEQ_SHIFT | node &lt;&lt; SEQ_SHIFT | counter;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int node = 1;</span><br><span class="line">Snowflake s = new Snowflake(node);</span><br><span class="line">long id = s.next();</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;雪花算法&quot;&gt;&lt;a href=&quot;#雪花算法&quot; class=&quot;headerlink&quot; title=&quot;雪花算法&quot;&gt;&lt;/a&gt;雪花算法&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pr
      
    
    </summary>
    
      <category term="分布式" scheme="https://masonnpe.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="分布式ID" scheme="https://masonnpe.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8FID/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://masonnpe.github.io/2018/11/12/Elasticsearch/ES%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/"/>
    <id>https://masonnpe.github.io/2018/11/12/Elasticsearch/ES基本用法/</id>
    <published>2018-11-11T16:15:54.949Z</published>
    <updated>2018-11-11T16:16:08.175Z</updated>
    
    <content type="html"><![CDATA[<p>document  数据 需要有唯一key</p><p>index    表  具有相同字段的docment组成</p><p>node  es实例</p><p>cluster 集群</p><p>_source 原始json数据</p><p>restful api</p><p>倒排索引</p><p> 单词词典  记录所有单词</p><p>倒排列表  在文档中的位置 偏移量offset</p><h4 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h4><p>character filters  去p标签  字符替换</p><p>tokenizer 切分 成单词</p><p>token filters  转小写  删除the   of   的 这 那  这种没实际意义的词</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">POST _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"analyzer"</span>:<span class="string">"standard"</span>,</span><br><span class="line">  <span class="attr">"filter"</span>: [<span class="string">"lowercase"</span>], </span><br><span class="line">  <span class="attr">"text"</span>:<span class="string">"HeLLo wORld!"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>result</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"tokens"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"token"</span>: <span class="string">"hello"</span>,</span><br><span class="line">      <span class="attr">"start_offset"</span>: <span class="number">0</span>,</span><br><span class="line">      <span class="attr">"end_offset"</span>: <span class="number">5</span>,</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"&lt;ALPHANUM&gt;"</span>,</span><br><span class="line">      <span class="attr">"position"</span>: <span class="number">0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"token"</span>: <span class="string">"world"</span>,</span><br><span class="line">      <span class="attr">"start_offset"</span>: <span class="number">6</span>,</span><br><span class="line">      <span class="attr">"end_offset"</span>: <span class="number">11</span>,</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"&lt;ALPHANUM&gt;"</span>,</span><br><span class="line">      <span class="attr">"position"</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>自带分词器</p><p>standard 默认分词器  </p><p>simple   非字母字符切分（空格 -之类的）  小写处理</p><p>whitespace   空格切分</p><p>stop    删除语气词  the  of</p><p>keyword  不做分词  </p><p>pattern  正则表达式自定义   默认\w+,非字符做分割</p><p>language  不同语言的分词</p><p>中文分词 IK   jieba  </p><p>基于自然语言  模型 算法  hanpl thulac</p><h4 id="自定义分词"><a href="#自定义分词" class="headerlink" title="自定义分词"></a>自定义分词</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">PUT test_index</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"settings"</span>: &#123;</span><br><span class="line">    <span class="attr">"analysis"</span>: &#123;</span><br><span class="line">      <span class="attr">"analyzer"</span>: &#123;</span><br><span class="line">        <span class="attr">"my_analyzer"</span>:&#123;</span><br><span class="line">          <span class="attr">"type"</span>:<span class="string">"custom"</span>,</span><br><span class="line">          <span class="attr">"tokenizer"</span>:<span class="string">"standard"</span>,</span><br><span class="line">          <span class="attr">"char_filter"</span>: [<span class="string">"html_strip"</span>],</span><br><span class="line">          <span class="attr">"filter"</span>:[<span class="string">"lowercase"</span>]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">POST /test_index/_analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"analyzer"</span>:<span class="string">"my_analyzer"</span>,</span><br><span class="line">  <span class="attr">"text"</span>:<span class="string">"HeLLo wORld!"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>自定义mapping</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">PUT my_index</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"mappings"</span>: &#123;</span><br><span class="line">    <span class="attr">"doc"</span>:&#123;</span><br><span class="line">      <span class="attr">"dynamic"</span>:<span class="literal">false</span>,</span><br><span class="line">      <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"title"</span>:&#123;</span><br><span class="line">          <span class="attr">"type"</span>: <span class="string">"text"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"name"</span>:&#123;</span><br><span class="line">          <span class="attr">"type"</span>: <span class="string">"keyword"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"age"</span>:&#123;</span><br><span class="line">          <span class="attr">"type"</span>: <span class="string">"integer"</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PUT my_index/doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>:<span class="string">"mason"</span>,</span><br><span class="line">  <span class="attr">"age"</span>:<span class="number">1</span>,</span><br><span class="line">  <span class="attr">"title"</span>:<span class="string">"mason good"</span>,</span><br><span class="line">  <span class="attr">"money"</span>:<span class="number">3123</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">_all 所有字段匹配</span><br><span class="line">GET my_index/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"query"</span>: &#123;</span><br><span class="line">    <span class="attr">"match"</span>: &#123;</span><br><span class="line">      <span class="attr">"_all"</span>: <span class="string">"mason"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">"index": false 不会被查询</span><br><span class="line">PUT my_index</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"mappings"</span>: &#123;</span><br><span class="line">    <span class="attr">"doc"</span>:&#123;</span><br><span class="line">      <span class="attr">"dynamic"</span>:<span class="literal">false</span>,</span><br><span class="line">      <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"title"</span>:&#123;</span><br><span class="line">          <span class="attr">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">          <span class="attr">"index"</span>: <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;document  数据 需要有唯一key&lt;/p&gt;
&lt;p&gt;index    表  具有相同字段的docment组成&lt;/p&gt;
&lt;p&gt;node  es实例&lt;/p&gt;
&lt;p&gt;cluster 集群&lt;/p&gt;
&lt;p&gt;_source 原始json数据&lt;/p&gt;
&lt;p&gt;restful api&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://masonnpe.github.io/2018/11/12/SpringCloud/Ribbon/"/>
    <id>https://masonnpe.github.io/2018/11/12/SpringCloud/Ribbon/</id>
    <published>2018-11-11T16:10:48.092Z</published>
    <updated>2018-11-11T16:12:36.614Z</updated>
    
    <content type="html"><![CDATA[<p>ribbon是一个基于http和tcp的客户端负载均衡工具</p><p>eureka强调了cap中的ap    zookeeper强调 cp</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ribbon是一个基于http和tcp的客户端负载均衡工具&lt;/p&gt;
&lt;p&gt;eureka强调了cap中的ap    zookeeper强调 cp&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://masonnpe.github.io/2018/11/12/SpringCloud/Hystrix/"/>
    <id>https://masonnpe.github.io/2018/11/12/SpringCloud/Hystrix/</id>
    <published>2018-11-11T16:10:26.188Z</published>
    <updated>2018-11-11T16:11:07.971Z</updated>
    
    <content type="html"><![CDATA[<p>因为网络问题或者服务自身问题出现调用故障或延迟，会直接导致调用方的堆外服务也出现延迟，请求不断鞥家，调用方等待响应形成任务挤压，导致自身服务瘫痪。为了解决连锁故障，要使用断路器服务保护机制</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;因为网络问题或者服务自身问题出现调用故障或延迟，会直接导致调用方的堆外服务也出现延迟，请求不断鞥家，调用方等待响应形成任务挤压，导致自身服务瘫痪。为了解决连锁故障，要使用断路器服务保护机制&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://masonnpe.github.io/2018/11/12/SpringCloud/Eureka/"/>
    <id>https://masonnpe.github.io/2018/11/12/SpringCloud/Eureka/</id>
    <published>2018-11-11T16:09:42.006Z</published>
    <updated>2018-11-11T16:12:20.655Z</updated>
    
    <content type="html"><![CDATA[<p>当ribbon与eureka联合使用时ribbon的服务配置清单ribbonserverlist会被discoveryEnabledNIWSServerlist重写，扩展成从eureka注册中心获取服务端列表</p><p>服务治理机制 注册中心互相注册组成高可用集群</p><p>失效剔除   eureka会创建一个定时任务，默认每隔一段时间当清单中超时的没有预约的服务剔除</p><p>自我保护  服务再注册是会维护一个心跳连接，eureka会将实例注册信息保存起来,让这些实例不过期，客户端必须要有容错机制，如请求重试，断路器等等</p><p>服务提供者  启动的时候会发送rest请求将自己注册到eureka上，同时带上自身服务的一些元数据，eureka接受到rest请求后，将元数据信息存储再一个双层map，第一层key是服务名，第二层key是具体服务的实例名</p><p>服务同步  服务提供者注册到不同的服务中心，提供者注册时会将请求装发到集群中其他的注册中心，实现同步</p><p>服务续约   注册完服务后，服务提供者会维护一个心跳，防止eureka将服务实例从服务列表中剔除</p><p>服务获取 启动消费者时，发送一个rest请求给注册中心，来获取上面注册的服务清单，eureka每隔一段时间返回给客户端</p><p>服务调用  消费端获取清单后，通过服务名可以获取具体提供服务的实例名和元数据信息，ribbon会默认采用轮询的方式进行调用</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当ribbon与eureka联合使用时ribbon的服务配置清单ribbonserverlist会被discoveryEnabledNIWSServerlist重写，扩展成从eureka注册中心获取服务端列表&lt;/p&gt;
&lt;p&gt;服务治理机制 注册中心互相注册组成高可用集群&lt;/p
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Scala集合</title>
    <link href="https://masonnpe.github.io/2018/11/11/Scala/Scala%E9%9B%86%E5%90%88/"/>
    <id>https://masonnpe.github.io/2018/11/11/Scala/Scala集合/</id>
    <published>2018-11-11T15:35:05.506Z</published>
    <updated>2018-11-11T15:36:21.899Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>new Array<a href=""></a>   </p><p>Array()    </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">var arr=new Array[String](5)</span><br><span class="line">arr(1)=&quot;hello&quot;</span><br><span class="line">println(arr.length)</span><br><span class="line">var arr2=Array(&quot;hello&quot;,&quot;world&quot;)</span><br><span class="line">println(arr2.mkString(&quot;-&quot;))</span><br><span class="line">var d=scala.collection.mutable.ArrayBuffer[String]()       //  可变的</span><br><span class="line">d += &quot;a&quot;</span><br><span class="line">d += (&quot;1&quot;,&quot;2&quot;,&quot;3&quot;)</span><br><span class="line">d ++= arr                                             // 加其他数组</span><br><span class="line">d.insert(5,&quot;mason&quot;)                                   // 插入指定位置</span><br><span class="line">println(d)</span><br><span class="line">remove</span><br><span class="line">trimend</span><br></pre></td></tr></table></figure><h2 id="list"><a href="#list" class="headerlink" title="list"></a>list</h2><p>Nil=空的list</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var l=List(1,23,4,5,67)</span><br><span class="line">println(l.head)        //第一个元素</span><br><span class="line">println(l.tail)        //除了第一个</span><br><span class="line">var l2=1::Nil          //1是头  Nil是尾</span><br><span class="line"></span><br><span class="line">var lb=ListBuffer[Int]()  // 可变list</span><br><span class="line">lb+=(1,23,5,4)</span><br><span class="line">println(lb)</span><br></pre></td></tr></table></figure><p>set</p><p>map</p><p>option</p><p>some</p><p>none</p><p>tuple</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数组&quot;&gt;&lt;a href=&quot;#数组&quot; class=&quot;headerlink&quot; title=&quot;数组&quot;&gt;&lt;/a&gt;数组&lt;/h2&gt;&lt;p&gt;new Array&lt;a href=&quot;&quot;&gt;&lt;/a&gt;   &lt;/p&gt;
&lt;p&gt;Array()    &lt;/p&gt;
&lt;figure class=&quot;high
      
    
    </summary>
    
      <category term="Scala" scheme="https://masonnpe.github.io/categories/Scala/"/>
    
    
      <category term="Scala" scheme="https://masonnpe.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>对比HashMap、Hashtable、LinkedHashMap、TreeMap</title>
    <link href="https://masonnpe.github.io/2018/11/11/Java/%E5%AF%B9%E6%AF%94HashMap%E3%80%81Hashtable%E3%80%81LinkedHashMap%E3%80%81TreeMap/"/>
    <id>https://masonnpe.github.io/2018/11/11/Java/对比HashMap、Hashtable、LinkedHashMap、TreeMap/</id>
    <published>2018-11-11T13:48:01.884Z</published>
    <updated>2018-11-11T13:49:46.373Z</updated>
    
    <content type="html"><![CDATA[<p>HashTable中的key、value都不能为null；HashMap中的key、value可以为null，很显然只能有一个key为null的键值对，但是允许有多个值为null的键值对；TreeMap中当未实现Comparator 接口时，key 不可以为null；当实现 Comparator 接口时，若未对null情况进行判断，则key不可以为null，反之亦然。<br>TreeMap是利用红黑树来实现的（树中的每个节点的值，都会大于或等于它的左子树种的所有节点的值，并且小于或等于它的右子树中的所有节点的值），实现了SortMap接口，能够对保存的记录根据键进行排序。所以一般需要排序的情况下是选择TreeMap来进行，默认为升序排序方式（深度优先搜索），可自定义实现Comparator接口实现排序方式。</p><p>HashMap基于哈希思想实现对数据的读写。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。当两个不同的键对象的hashcode相同时，它们会储存在同一个bucket位置的链表中，可通过键对象的equals()方法用来找到键值对。如果链表大小超过阈值（TREEIFY_THRESHOLD, 8），链表就会被改造为树形结构</p><p>扩容时：Hashtable将容量变为原来的2倍加1；HashMap扩容将容量变为原来的2倍<code>newCap = oldCap &lt;&lt; 1</code></p><p>HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致，在并发环境可能出现无限循环占用CPU、size不准确等诡异的问题。可以使用如下方法进行同步</p><ol><li>可以用 Collections的synchronizedMap方法</li><li>使用ConcurrentHashMap类，相较于HashTable锁住的是对象整体， ConcurrentHashMap基于lock实现锁分段技术。首先将Map存放的数据分成一段一段的存储方式，然后给每一段数据分配一把锁，当一个线程占用锁访问其中一个段的数据时，其他段的数据也能被其他线程访问。ConcurrentHashMap不仅保证了多线程运行环境下的数据访问安全性，而且性能上有长足的提升。</li></ol><p>LinkedHashMap通常提供的是遍历顺序符合插入顺序，它的实现是通过为条目（键值对）维护一个双向链表。注意，通过特定构造函数，我们可以创建反映访问顺序的实例，所<br>谓的put、get、compute等，都算作“访问”。<br>这种行为适用于一些特定应用场景，例如，我们构建一个空间占用敏感的资源池，希望可以自动将最不常被访问的对象释放掉，这就可以利用LinkedHashMap提供的机制来实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import java.util.LinkedHashMap;</span><br><span class="line">import java.util.Map;</span><br><span class="line">public class LinkedHashMapSample &#123;</span><br><span class="line"> public satic void main(String[] args) &#123;</span><br><span class="line"> LinkedHashMap&lt;String, String&gt; accessOrderedMap = new LinkedHashMap&lt;&gt;(16, 0.75F, true)&#123;</span><br><span class="line"> @Override</span><br><span class="line"> protected boolean removeEldesEntry(Map.Entry&lt;String, String&gt; eldes) &#123; // 实现自定义删除策略，否则行为就和普遍Map没有区别</span><br><span class="line"> return size() &gt; 3;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;;</span><br><span class="line"> accessOrderedMap.put(&quot;Project1&quot;, &quot;Valhalla&quot;);</span><br><span class="line"> accessOrderedMap.put(&quot;Project2&quot;, &quot;Panama&quot;);</span><br><span class="line"> accessOrderedMap.put(&quot;Project3&quot;, &quot;Loom&quot;);</span><br><span class="line"> accessOrderedMap.forEach( (k,v) -&gt; &#123;</span><br><span class="line"> Sysem.out.println(k +&quot;:&quot; + v);</span><br><span class="line"> &#125;);</span><br><span class="line"> // 模拟访问</span><br><span class="line"> accessOrderedMap.get(&quot;Project2&quot;);</span><br><span class="line"> accessOrderedMap.get(&quot;Project2&quot;);</span><br><span class="line"> accessOrderedMap.get(&quot;Project3&quot;);</span><br><span class="line"> Sysem.out.println(&quot;Iterate over should be not afected:&quot;);</span><br><span class="line"> accessOrderedMap.forEach( (k,v) -&gt; &#123;</span><br><span class="line"> Sysem.out.println(k +&quot;:&quot; + v);</span><br><span class="line"> &#125;);</span><br><span class="line"> // 触发删除</span><br><span class="line"> accessOrderedMap.put(&quot;Project4&quot;, &quot;Mission Control&quot;);</span><br><span class="line"> Sysem.out.println(&quot;Oldes entry should be removed:&quot;);</span><br><span class="line"> accessOrderedMap.forEach( (k,v) -&gt; &#123;// 遍历顺序不变</span><br><span class="line"> Sysem.out.println(k +&quot;:&quot; + v);</span><br><span class="line"> &#125;);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>依据resize源码，不考虑极端情况（容量理论最大极限由MAXIMUM_CAPACITY指定，数值为 1&lt;&lt;30，也就是2的30次方），我们可以归纳为：<br>门限值等于（负载因子）x（容量），如果构建HashMap的时候没有指定它们，那么就是依据相应的默认常量值。<br>门限通常是以倍数进行调整 （newThr = oldThr &lt;&lt; 1），我前面提到，根据putVal中的逻辑，当元素个数超过门限大小时，则调整Map大小。<br>扩容后，需要将老的数组中的元素重新放置到新的数组，这是扩容的一个主要开销来源。<br>3.容量、负载因子和树化<br>前面我们快速梳理了一下HashMap从创建到放入键值对的相关逻辑，现在思考一下，为什么我们需要在乎容量和负载因子呢？<br>这是因为容量和负载系数决定了可用的桶的数量，空桶太多会浪费空间，如果使用的太满则会严重影响操作的性能。极端情况下，假设只有一个桶，那么它就退化成了链表，完全不<br>能提供所谓常数时间存的性能。<br>既然容量和负载因子这么重要，我们在实践中应该如何选择呢？<br>如果能够知道HashMap要存取的键值对数量，可以考虑预先设置合适的容量大小。具体数值我们可以根据扩容发生的条件来做简单预估，根据前面的代码分析，我们知道它需要符合<br>计算条件：<br>极客时间<br>负载因子 * 容量 &gt; 元素数量<br>所以，预先设置的容量需要满足，大于“预估元素数量/负载因子”，同时它是2的幂数，结论已经非常清晰了。<br>而对于负载因子，我建议：<br>如果没有特别需求，不要轻易进行更改，因为JDK自身的默认负载因子是非常符合通用场景的需求的。<br>如果确实需要调整，建议不要设置超过0.75的数值，因为会显著增加冲突，降低HashMap的性能。<br>如果使用太小的负载因子，按照上面的公式，预设容量值也进行调整，否则可能会导致更加频繁的扩容，增加无谓的开销，本身访问性能也会受影响。<br>我们前面提到了树化改造，对应逻辑主要在putVal和treeifyBin中。<br>fnal void treeifyBin(Node[] tab, int hash) {<br>int n, index; Node e;<br>if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY)<br>resize();<br>else if ((e = tab[index = (n - 1) &amp; hash]) != null) {<br>//树化改造逻辑<br>}<br>}<br>上面是精简过的treeifyBin示意，综合这两个方法，树化改造的逻辑就非常清晰了，可以理解为，当bin的数量大于TREEIFY_THRESHOLD时：<br>如果容量小于MIN_TREEIFY_CAPACITY，只会进行简单的扩容。<br>如果容量大于MIN_TREEIFY_CAPACITY ，则会进行树化改造。<br>那么，为什么HashMap要树化呢？<br>本质上这是个安全问题。因为在元素放置过程中，如果一个对象哈希冲突，都被放置到同一个桶里，则会形成一个链表，我们知道链表查询是线性的，会严重影响存取的性能。<br>而在现实世界，构造哈希冲突的数据并不是非常复杂的事情，恶意代码就可以利用这些数据大量与服务器端交互，导致服务器端CPU大量占用，这就构成了哈希碰撞拒绝服务攻击，<br>国内一线互联网公司就发生过类似攻击事件</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;HashTable中的key、value都不能为null；HashMap中的key、value可以为null，很显然只能有一个key为null的键值对，但是允许有多个值为null的键值对；TreeMap中当未实现Comparator 接口时，key 不可以为null；当实现
      
    
    </summary>
    
      <category term="Java" scheme="https://masonnpe.github.io/categories/Java/"/>
    
    
      <category term="HashMap" scheme="https://masonnpe.github.io/tags/HashMap/"/>
    
  </entry>
  
  <entry>
    <title>对比Vector、ArrayList、LinkedList有何区别</title>
    <link href="https://masonnpe.github.io/2018/11/11/Java/%E5%AF%B9%E6%AF%94Vector%E3%80%81ArrayList%E3%80%81LinkedList%E6%9C%89%E4%BD%95%E5%8C%BA%E5%88%AB/"/>
    <id>https://masonnpe.github.io/2018/11/11/Java/对比Vector、ArrayList、LinkedList有何区别/</id>
    <published>2018-11-11T13:47:33.533Z</published>
    <updated>2018-11-11T13:50:10.363Z</updated>
    
    <content type="html"><![CDATA[<p>Vector是Java早期提供的线程安全的动态数组，如果不需要线程安全，并不建议选择，毕竟同步是有额外开销的。Vector内部是使用对象数组来保存数据，可以根据需要自动的增加<br>容量，当数组已满时，会创建新的数组，并拷贝原有数组数据。<br>ArrayList是应用更加广泛的动态数组实现，它本身不是线程安全的，所以性能要好很多。与Vector近似，ArrayList也是可以根据需要调整容量，不过两者的调整逻辑有所区<br>别，Vector在扩容时会提高1倍，而ArrayList则是增加50%。<br>LinkedList顾名思义是Java提供的双向链表，所以它不需要像上面两种那样调整容量，它也不是线程安全的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Vector是Java早期提供的线程安全的动态数组，如果不需要线程安全，并不建议选择，毕竟同步是有额外开销的。Vector内部是使用对象数组来保存数据，可以根据需要自动的增加&lt;br&gt;容量，当数组已满时，会创建新的数组，并拷贝原有数组数据。&lt;br&gt;ArrayList是应用更加
      
    
    </summary>
    
      <category term="Java" scheme="https://masonnpe.github.io/categories/Java/"/>
    
    
      <category term="ArrayList" scheme="https://masonnpe.github.io/tags/ArrayList/"/>
    
  </entry>
  
  <entry>
    <title>对比synchronized、ReentrantLock</title>
    <link href="https://masonnpe.github.io/2018/11/11/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/%E5%AF%B9%E6%AF%94synchronized%E3%80%81ReentrantLock/"/>
    <id>https://masonnpe.github.io/2018/11/11/并发编程/对比synchronized、ReentrantLock/</id>
    <published>2018-11-11T13:40:18.130Z</published>
    <updated>2018-11-11T13:41:26.618Z</updated>
    
    <content type="html"><![CDATA[<p>ReentrantLock使用起来比较灵活，但是必须手动获取与释放锁，而synchronized不需要手动释放和开启锁<br>ReentrantLock只适用于代码块锁，而synchronized可用于修饰方法、代码块<br>ReentrantLock的优势体现在：<br>具备尝试非阻塞地获取锁的特性：当前线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取并持有锁<br>能被中断地获取锁的特性：与synchronized不同，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放<br>超时获取锁的特性：在指定的时间范围内获取锁；如果截止时间到了仍然无法获取锁，则返回<br>3 注意事项<br>在使用ReentrantLock类的时，一定要注意三点：<br>在fnally中释放锁，目的是保证在获取锁之后，最终能够被释放<br>不要将获取锁的过程写在try块内，因为如果在获取锁时发生了异常，异常抛出的同时，也会导致锁无故被释放。<br>ReentrantLock提供了一个newCondition的方法，以便用户在同一锁的情况下可以根据不同的情况执行等待或唤醒的动作</p><p>锁降级确实是会发生的，当JVM进入安全点（SafePoint）的时候，会检查是否有闲置的Monitor，然后试图进行降级。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ReentrantLock使用起来比较灵活，但是必须手动获取与释放锁，而synchronized不需要手动释放和开启锁&lt;br&gt;ReentrantLock只适用于代码块锁，而synchronized可用于修饰方法、代码块&lt;br&gt;ReentrantLock的优势体现在：&lt;br&gt;
      
    
    </summary>
    
      <category term="并发编程" scheme="https://masonnpe.github.io/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="ReentrantLock" scheme="https://masonnpe.github.io/tags/ReentrantLock/"/>
    
      <category term="synchronized" scheme="https://masonnpe.github.io/tags/synchronized/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://masonnpe.github.io/2018/11/06/Java/JDK%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    <id>https://masonnpe.github.io/2018/11/06/Java/JDK中的设计模式/</id>
    <published>2018-11-05T16:10:40.758Z</published>
    <updated>2018-11-05T16:20:17.927Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>结构型模式：</p><ul><li><p>适配器：用来把一个接口转化成另一个接口，如 java.util.Arrays#asList()。java.io.InputStreamReader(InputStream)java.io.OutputStreamWriter(OutputStream)</p></li><li><p>桥接模式：这个模式将抽象和抽象操作的实现进行了解耦，这样使得抽象和实现可以独立地变化，如JDBC；</p></li><li><p>组合模式：使得客户端看来单个对象和对象的组合是同等的。换句话说，某个类型的方法同时也接受自身类型作为参数，如 Map.putAll，List.addAll、Set.addAll。</p></li><li><p>装饰者模式：动态的给一个对象附加额外的功能，这也是子类的一种替代方式，如 java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap。java.io.BufferedInputStream(InputStream)java.io.DataInputStream(InputStream)java.io.BufferedOutputStream(OutputStream)</p></li><li><p>享元模式：使用缓存来加速大量小对象的访问时间，如 valueOf(int)。</p></li><li><p>代理模式：代理模式是用一个简单的对象来代替一个复杂的或者创建耗时的对象，如 java.lang.reflect.Proxy</p></li><li><h5 id="门面模式："><a href="#门面模式：" class="headerlink" title="门面模式："></a>门面模式：</h5><p>给一组组件，接口，抽象，或者子系统提供一个简单的接口。</p><ul><li><ul><li><ul><li>java.lang.Class</li><li>javax.faces.webapp.FacesServlet</li></ul></li></ul></li></ul></li></ul></li><li><p>创建模式:</p><ul><li><p>抽象工厂模式：抽象工厂模式提供了一个协议来生成一系列的相关或者独立的对象，而不用指定具体对象的类型，如 java.util.Calendar#getInstance()。java.util.Arrays#asList()java.util.ResourceBundle#getBundle()java.sql.DriverManager#getConnection()java.sql.Connection#createStatement()java.sql.Statement#executeQuery()</p></li><li><p>建造模式(Builder)：定义了一个新的类来构建另一个类的实例，以简化复杂对象的创建，如：java.lang.StringBuilder#append()。</p></li><li><p>工厂方法：就是 <strong>一个返*</strong> 回具体对象的方法，而不是多个，如 java.lang.Object#toString()、java.lang.Class#newInstance()。java.lang.Class#newInstance()java.lang.Class#forName()</p></li><li><p>原型模式：使得类的实例能够生成自身的拷贝、如：java.lang.Object#clone()。java.lang.Object#clone()java.lang.Cloneable</p></li><li><h5 id="单例模式"><a href="#单例模式" class="headerlink" title="单例模式"></a>单例模式</h5><p>用来确保类只有一个实例。</p><ul><li><ul><li><ul><li>java.lang.Runtime#getRuntime()</li></ul></li></ul></li></ul></li></ul></li><li><p>行为模式：</p><ul><li><p>责任链模式：通过把请求从一个对象传递到链条中下一个对象的方式，直到请求被处理完毕，以实现对象间的解耦。如 javax.servlet.Filter#doFilter()。 java.util.logging.Logger#log()</p></li><li><p>命令模式：将操作封装到对象内，以便存储，传递和返回，如：java.lang.Runnable。</p></li><li><p>解释器模式：定义了一个语言的语法，然后解析相应语法的语句，如，java.text.Format，java.text.Normalizer。</p></li><li><p>迭代器模式：提供一个一致的方法来顺序访问集合中的对象，如 java.util.Iterator。</p></li><li><p>中介者模式：通过使用一个中间对象来进行消息分发以及减少类之间的直接依赖，java.lang.reflect.Method#invoke()。java.util.Timerjava.util.concurrent.Executor#execute()java.util.concurrent.ExecutorService#submit()java.lang.reflect.Method#invoke()</p></li><li><p>空对象模式：如 java.util.Collections#emptyList()。</p></li><li><p>观察者模式：它使得一个对象可以灵活的将消息发送给感兴趣的对象，如 java.util.EventListener。javax.servlet.http.HttpSessionBindingListenerjavax.servlet.http.HttpSessionAttributeListenerjavax.faces.event.PhaseListener</p></li><li><p>模板方法模式：让子类可以重写方法的一部分，而不是整个重写，如 java.util.Collections#sort()。</p></li><li><h5 id="备忘录模式"><a href="#备忘录模式" class="headerlink" title="备忘录模式"></a>备忘录模式</h5><p>生成对象状态的一个快照，以便对象可以恢复原始状态而不用暴露自身的内容。Date对象通过自身内部的一个long值来实现备忘录模式。</p><ul><li><ul><li><ul><li>java.util.Date</li><li>java.io.Serializable</li></ul></li></ul></li></ul></li></ul></li><li><ul><li><h5 id="状态模"><a href="#状态模" class="headerlink" title="状态模"></a>状态模</h5></li></ul></li></ul><p>通过改变对象内部的状态，使得你可以在运行时动态改变一个对象的行为。</p><ul><li><ul><li><ul><li>java.util.Iterator</li><li>javax.faces.lifecycle.LifeCycle#execute()</li></ul></li></ul></li></ul><h5 id="策略模式"><a href="#策略模式" class="headerlink" title="策略模式"></a>策略模式</h5><p>使用这个模式来将一组算法封装成一系列对象。通过传递这些对象可以灵活的改变程序的功能。</p><ul><li><ul><li><ul><li>java.util.Comparator#compare()</li><li>javax.servlet.http.HttpServlet</li><li>javax.servlet.Filter#doFilter()</li></ul></li></ul></li></ul><h5 id="模板方法模式"><a href="#模板方法模式" class="headerlink" title="模板方法模式"></a>模板方法模式</h5><p>让子类可以重写方法的一部分，而不是整个重写，你可以控制子类需要重写那些操作。</p><ul><li><ul><li><ul><li>java.util.Collections#sort()</li><li>java.io.InputStream#skip()</li><li>java.io.InputStream#read()</li><li>java.util.AbstractList#indexOf()</li></ul></li></ul></li></ul><h5 id="访问者模式"><a href="#访问者模式" class="headerlink" title="访问者模式"></a>访问者模式</h5><p>提供一个方便的可维护的方式来操作一组对象。它使得你在不改变操作的对象前提下，可以修改或者扩展对象的行为。</p><ul><li><ul><li><ul><li>javax.lang.model.element.Element and javax.lang.model.element.ElementVisitor</li><li>javax.lang.model.type.TypeMirror and javax.lang.model.type.TypeVisitor</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;结构型模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;适配器：用来把一个接口转化成另一个接口，如 java.util.Arrays#asList()。java.io.InputStreamReader(InputStream)java.io.OutputStrea
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Mybatis中的设计模式</title>
    <link href="https://masonnpe.github.io/2018/11/06/MyBatis/Mybatis%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    <id>https://masonnpe.github.io/2018/11/06/MyBatis/Mybatis中的设计模式/</id>
    <published>2018-11-05T16:08:28.250Z</published>
    <updated>2018-11-25T15:20:38.092Z</updated>
    
    <content type="html"><![CDATA[<p>一、装饰模式</p><p>最明显的就是cache包下面的实现</p><p>Cahe、LoggingCache、LruCache、TransactionalCahe…等</p><p>以LoggingCache为例，UML图</p><p>​                              </p><p>Cache cache  = new LoggingCache(new PerpetualCache(“cacheid”));<br>一层层包装就使得默认cache实现PerpetualCache具有附加的功能，比如上面的log功能。<br>二、建造者模式</p><p>BaseBuilder、XMLMapperBuilder</p><p>三、工厂方法</p><p>SqlSessionFactory</p><p>四、适配器模式</p><p>Log、LogFactory</p><p>五、模板方法</p><p>BaseExecutor、SimpleExecutor</p><p>六、动态代理</p><p>Plugin 见7图</p><p>7、责任链模式</p><p>Interceptor、InterceptorChain</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、装饰模式&lt;/p&gt;
&lt;p&gt;最明显的就是cache包下面的实现&lt;/p&gt;
&lt;p&gt;Cahe、LoggingCache、LruCache、TransactionalCahe…等&lt;/p&gt;
&lt;p&gt;以LoggingCache为例，UML图&lt;/p&gt;
&lt;p&gt;​               
      
    
    </summary>
    
      <category term="Mybatis" scheme="https://masonnpe.github.io/categories/Mybatis/"/>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://masonnpe.github.io/2018/11/06/Spring/Spring%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    <id>https://masonnpe.github.io/2018/11/06/Spring/Spring中的设计模式/</id>
    <published>2018-11-05T16:07:20.924Z</published>
    <updated>2018-11-05T16:07:38.688Z</updated>
    
    <content type="html"><![CDATA[<p><strong>1. 简单工厂</strong></p><p>又叫做静态工厂方法（StaticFactory Method）模式，但不属于23种GOF设计模式之一。</p><p>简单工厂模式的实质是由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。</p><p><em>Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得Bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。</em></p><p><strong>2. 工厂方法（Factory Method）</strong></p><p>定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使一个类的实例化延迟到其子类。</p><p><em>Spring中的FactoryBean就是典型的工厂方法模式</em>。如下图：</p><p><img src="https://images0.cnblogs.com/blog2015/521294/201505/172221089043885.png" alt="img"></p><p><strong>3. 单例（Singleton）</strong></p><p>保证一个类仅有一个实例，并提供一个访问它的全局访问点。</p><p>Spring中的单例模式完成了后半句话，即提供了全局的访问点BeanFactory。但没有从构造器级别去控制单例，这是因为Spring管理的是是任意的Java对象。</p><p><strong>4. 适配器（Adapter）</strong></p><p>将一个类的接口转换成客户希望的另外一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。</p><p>Spring中在对于AOP的处理中有Adapter模式的例子，见如下图：<img src="https://images0.cnblogs.com/blog2015/521294/201505/172222151238548.png" alt="img"></p><p>由于Advisor链需要的是MethodInterceptor（拦截器）对象，所以每一个Advisor中的Advice都要适配成对应的MethodInterceptor对象。</p><p><strong>5.包装器（Decorator）</strong></p><p>动态地给一个对象添加一些额外的职责。就增加功能来说，Decorator模式相比生成子类更为灵活。</p><p><img src="https://images0.cnblogs.com/blog2015/521294/201505/172223026393497.png" alt="img"></p><p>Spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。基本上都是动态地给一个对象添加一些额外的职责。</p><p><strong>6. 代理（Proxy）</strong></p><p>为其他对象提供一种代理以控制对这个对象的访问。</p><p>从结构上来看和Decorator模式类似，但Proxy是控制，更像是一种对功能的限制，而Decorator是增加职责。</p><p><img src="https://images0.cnblogs.com/blog2015/521294/201505/172223379049538.png" alt="img"></p><p>Spring的Proxy模式在aop中有体现，比如JdkDynamicAopProxy和Cglib2AopProxy。</p><p><strong>7.观察者（Observer）</strong></p><p>定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。</p><p><img src="https://images0.cnblogs.com/blog2015/521294/201505/172224113421820.png" alt="img"></p><p>Spring中Observer模式常用的地方是listener的实现。如ApplicationListener。</p><p><strong>8. 策略（Strategy）</strong></p><p>定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。</p><p>Spring中在实例化对象的时候用到Strategy模式，见如下图：</p><p><img src="https://images0.cnblogs.com/blog2015/521294/201505/172224299513506.png" alt="img"></p><p>在SimpleInstantiationStrategy中有如下代码说明了策略模式的使用情况：</p><p><img src="https://images0.cnblogs.com/blog2015/521294/201505/172224414206242.png" alt="img"></p><p><strong>9.模板方法（Template Method）</strong></p><p>定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。Template Method使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。</p><p>Template Method模式一般是需要继承的。这里想要探讨另一种对Template Method的理解。Spring中的JdbcTemplate，在用这个类时并不想去继承这个类，因为这个类的方法太多，但是我们还是想用到JdbcTemplate已有的稳定的、公用的数据库连接，那么我们怎么办呢？我们可以把变化的东西抽出来作为一个参数传入JdbcTemplate的方法中。但是变化的东西是一段代码，而且这段代码会用到JdbcTemplate中的变量。怎么办？那我们就用回调对象吧。在这个回调对象中定义一个操纵JdbcTemplate中变量的方法，我们去实现这个方法，就把变化的东西集中到这里了。然后我们再传入这个回调对象到JdbcTemplate，从而完成了调用。这可能是Template Method不需要继承的另一种实现方式吧。</p><p>以下是一个具体的例子：</p><p>JdbcTemplate中的execute方法：</p><p><img src="https://images0.cnblogs.com/blog2015/521294/201505/172224551864583.png" alt="img"></p><p>JdbcTemplate执行execute方法：</p><p><img src="https://images0.cnblogs.com/blog2015/521294/201505/172225336542968.png" alt="img"></p><p>知识只有共享才能传播，才能推崇出新的知识，才能学到更多，这里写的每一篇文字/博客，基本都是从网上查询了一下资料然后记录下来，也有些是原滋原味搬了过来，也有时加了一些自己的想法</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;1. 简单工厂&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;又叫做静态工厂方法（StaticFactory Method）模式，但不属于23种GOF设计模式之一。&lt;/p&gt;
&lt;p&gt;简单工厂模式的实质是由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。&lt;/p&gt;
&lt;p
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://masonnpe.github.io/2018/11/06/Netty/NIO/"/>
    <id>https://masonnpe.github.io/2018/11/06/Netty/NIO/</id>
    <published>2018-11-05T16:03:55.804Z</published>
    <updated>2018-11-11T16:15:13.389Z</updated>
    
    <content type="html"><![CDATA[<p><strong>初识NIO：</strong></p><p>​    在 JDK 1. 4 中 新 加入 了 NIO( New Input/ Output) 类, 引入了一种基于通道和缓冲区的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆的 DirectByteBuffer 对象作为这块内存的引用进行操作，避免了在 Java 堆和 Native 堆中来回复制数据。</p><p>​    NIO 是一种同步非阻塞的 IO 模型。同步是指线程不断轮询 IO 事件是否就绪，非阻塞是指线程在等待 IO 的时候，可以同时做其他任务。同步的核心就是 Selector，Selector 代替了线程本身轮询 IO 事件，避免了阻塞同时减少了不必要的线程消耗；非阻塞的核心就是通道和缓冲区，当 IO 事件就绪时，可以通过写道缓冲区，保证 IO 的成功，而无需线程阻塞式地等待。</p><p><strong>Buffer：</strong></p><p>​    为什么说NIO是基于缓冲区的IO方式呢？因为，当一个链接建立完成后，IO的数据未必会马上到达，为了当数据到达时能够正确完成IO操作，在BIO（阻塞IO）中，等待IO的线程必须被阻塞，以全天候地执行IO操作。为了解决这种IO方式低效的问题，引入了缓冲区的概念，当数据到达时，可以预先被写入缓冲区，再由缓冲区交给线程，因此线程无需阻塞地等待IO。</p><p><strong>通道：</strong></p><p>​    当执行：SocketChannel.write(Buffer)，便将一个 buffer 写到了一个通道中。如果说缓冲区还好理解，通道相对来说就更加抽象。网上博客难免有写不严谨的地方，容易使初学者感到难以理解。</p><p>​    引用 Java NIO 中权威的说法：通道是 I/O 传输发生时通过的入口，而缓冲区是这些数 据传输的来源或目标。对于离开缓冲区的传输，您想传递出去的数据被置于一个缓冲区，被传送到通道。对于传回缓冲区的传输，一个通道将数据放置在您所提供的缓冲区中。</p><p>​    例如 有一个服务器通道 ServerSocketChannel serverChannel，一个客户端通道 SocketChannel clientChannel；服务器缓冲区：serverBuffer，客户端缓冲区：clientBuffer。</p><p>​    当服务器想向客户端发送数据时，需要调用：clientChannel.write(serverBuffer)。当客户端要读时，调用 clientChannel.read(clientBuffer)</p><p>​    当客户端想向服务器发送数据时，需要调用：serverChannel.write(clientBuffer)。当服务器要读时，调用 serverChannel.read(serverBuffer)</p><p>​    这样，通道和缓冲区的关系似乎更好理解了。在实践中，未必会出现这种双向连接的蠢事（然而这确实存在的，后面的内容还会涉及），但是可以理解为在NIO中：如果想将Data发到目标端，则需要将存储该Data的Buffer，写入到目标端的Channel中，然后再从Channel中读取数据到目标端的Buffer中。</p><p><strong>Selector：</strong></p><p>​    通道和缓冲区的机制，使得线程无需阻塞地等待IO事件的就绪，但是总是要有人来监管这些IO事件。这个工作就交给了selector来完成，这就是所谓的同步。</p><p>​    Selector允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用Selector就会很方便。</p><p>​    要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪，这就是所说的轮询。一旦这个方法返回，线程就可以处理这些事件。</p><p>​    Selector中注册的感兴趣事件有：</p><ul><li>OP_ACCEPT</li><li>OP_CONNECT </li><li>OP_READ </li><li>OP_WRITE</li></ul><p><strong>优化：</strong></p><p>​    一种优化方式是：将Selector进一步分解为Reactor，将不同的感兴趣事件分开，每一个Reactor只负责一种感兴趣的事件。这样做的好处是：1、分离阻塞级别，减少了轮询的时间；2、线程无需遍历set以找到自己感兴趣的事件，因为得到的set中仅包含自己感兴趣的事件。</p><p><img src="http://imglf1.ph.126.net/9AtBKwQ8vHsko1RPRb0sew==/6631645009304717344.jpg" alt="img"></p><p><strong>NIO和epoll：</strong></p><p>​    epoll是Linux内核的IO模型。我想一定有人想问，AIO听起来比NIO更加高大上，为什么不使用AIO？AIO其实也有应用，但是有一个问题就是，Linux是不支持AIO的，因此基于AIO的程序运行在Linux上的效率相比NIO反而更低。而Linux是最主要的服务器OS，因此相比AIO，目前NIO的应用更加广泛。</p><p>​    说到这里，可能你已经明白了，epoll一定和NIO有着很深的因缘。没错，如果仔细研究epoll的技术内幕，你会发现它确实和NIO非常相似，都是基于“通道”和缓冲区的，也有selector，只是在epoll中，通道实际上是操作系统的“管道”。和NIO不同的是，NIO中，解放了线程，但是需要由selector阻塞式地轮询IO事件的就绪；而epoll中，IO事件就绪后，会自动发送消息，通知selector：“我已经就绪了。”可以认为，Linux的epoll是一种效率更高的NIO。</p><p><strong>NIO轶事：</strong></p><p>​    一篇有意思的<a href="http://blog.csdn.net/haoel/article/details/2224069" target="_blank" rel="noopener">博客</a>，讲的 Java selector.open() 的时候，会创建一个自己和自己的链接（windows上是tcp，linux上是通道）</p><p>​    这么做的原因：可以从 Apache Mina 中窥探。在 Mina 中，有如下机制：</p><ol><li>Mina框架会创建一个Work对象的线程。</li><li>Work对象的线程的run()方法会从一个队列中拿出一堆Channel，然后使用Selector.select()方法来侦听是否有数据可以读/写。</li><li>最关键的是，在select的时候，如果队列有新的Channel加入，那么，Selector.select()会被唤醒，然后重新select最新的Channel集合。</li><li>要唤醒select方法，只需要调用Selector的wakeup()方法。</li></ol><p>​    而一个阻塞在select上的线程有以下三种方式可以被唤醒：</p><ol><li>有数据可读/写，或出现异常。</li><li>阻塞时间到，即time out。</li><li>收到一个non-block的信号。可由kill或pthread_kill发出。</li></ol><p>​    首先 2 可以排除，而第三种方式，只在linux中存在。因此，Java NIO为什么要创建一个自己和自己的链接：就是如果想要唤醒select，只需要朝着自己的这个loopback连接发点数据过去，于是，就可以唤醒阻塞在select上的线程了。</p><p><a href="https://blog.csdn.net/xidianliuy/article/details/51612676" target="_blank" rel="noopener">《Java NIO编写Socket服务器的一个例子》</a></p><p>1.Java NIO概览<br>首先，熟悉一下NIO的主要组成部分：<br>Bufer，高效的数据容器，除了布尔类型，所有原始数据类型都有相应的Bufer实现。<br>Channel，类似在Linux之类操作系统上看到的文件描述符，是NIO中被用来支持批量式IO操作的一种抽象。<br>File或者Socket，通常被认为是比较高层次的抽象，而Channel则是更加操作系统底层的一种抽象，这也使得NIO得以充分利用现代操作系统底层机制，获得特定场景的性能优<br>化，例如，DMA（Direct Memory Access）等。不同层次的抽象是相互关联的，我们可以通过Socket获取Channel，反之亦然。<br>Selector，是NIO实现多路复用的基础，它提供了一种高效的机制，可以检测到注册在Selector上的多个Channel中，是否有Channel处于就绪状态，进而实现了单线程对<br>多Channel的高效管理。<br>Selector同样是基于底层操作系统机制，不同模式、不同版本都存在区别，例如，在最新的代码库里，相关实现如下：<br>Linux上依赖于epoll（<a href="http://hg.openjdk.java.net/jdk/jdk/fle/d8327f838b88/src/java.base/linux/classes/sun/nio/ch/EPollSelectorImpl.java）。" target="_blank" rel="noopener">http://hg.openjdk.java.net/jdk/jdk/fle/d8327f838b88/src/java.base/linux/classes/sun/nio/ch/EPollSelectorImpl.java）。</a><br>Windows上NIO2（AIO）模式则是依赖于iocp（<a href="http://hg.openjdk.java.net/jdk/jdk/fle/d8327f838b88/src/java.base/windows/classes/sun/nio/ch/Iocp.java）。" target="_blank" rel="noopener">http://hg.openjdk.java.net/jdk/jdk/fle/d8327f838b88/src/java.base/windows/classes/sun/nio/ch/Iocp.java）。</a><br>Chartset，提供Unicode字符串定义，NIO也提供了相应的编解码器等，例如，通过下面的方式进行字符串到ByteBufer的转换：<br>Charset.defaultCharset().encode(“Hello world!”));<br>2.NIO能解决什么问题？<br>下面我通过一个典型场景，来分析为什么需要NIO，为什么需要多路复用。设想，我们需要实现一个服务器应用，只简单要求能够同时服务多个客户端请求即可。<br>使用java.io和java.net中的同步、阻塞式API，可以简单实现。<br>public class DemoServer extends Thread {<br>private ServerSocket serverSocket;<br>public int getPort() {<br>return serverSocket.getLocalPort();<br>}<br>public void run() {<br>try {<br>极客时间<br>serverSocket = new ServerSocket(0);<br>while (true) {<br>Socket socket = serverSocket.accept();<br>RequesHandler requesHandler = new RequesHandler(socket);<br>requesHandler.sart();<br>}<br>} catch (IOException e) {<br>e.printStackTrace();<br>} fnally {<br>if (serverSocket != null) {<br>try {<br>serverSocket.close();<br>} catch (IOException e) {<br>e.printStackTrace();<br>}<br>;<br>}<br>}<br>}<br>public satic void main(String[] args) throws IOException {<br>DemoServer server = new DemoServer();<br>server.sart();<br>try (Socket client = new Socket(InetAddress.getLocalHos(), server.getPort())) {<br>BuferedReader buferedReader = new BuferedReader(new InputStreamReader(client.getInputStream()));<br>buferedReader.lines().forEach(s -&gt; Sysem.out.println(s));<br>}<br>}<br>}<br>// 简化实现，不做读取，直接发送字符串<br>class RequesHandler extends Thread {<br>private Socket socket;<br>RequesHandler(Socket socket) {<br>this.socket = socket;<br>}<br>@Override<br>public void run() {<br>try (PrintWriter out = new PrintWriter(socket.getOutputStream());) {<br>out.println(“Hello world!”);<br>out.fush();<br>} catch (Exception e) {<br>e.printStackTrace();<br>}<br>}<br>}<br>其实现要点是：<br>服务器端启动ServerSocket，端口0表示自动绑定一个空闲端口。<br>调用accept方法，阻塞等待客户端连接。<br>利用Socket模拟了一个简单的客户端，只进行连接、读取、打印。<br>当连接建立后，启动一个单独线程负责回复客户端请求。<br>这样，一个简单的Socket服务器就被实现出来了。<br>思考一下，这个解决方案在扩展性方面，可能存在什么潜在问题呢？<br>大家知道Java语言目前的线程实现是比较重量级的，启动或者销毁一个线程是有明显开销的，每个线程都有单独的线程栈等结构，需要占用非常明显的内存，所以，每一个Client启<br>动一个线程似乎都有些浪费。<br>那么，稍微修正一下这个问题，我们引入线程池机制来避免浪费。<br>serverSocket = new ServerSocket(0);<br>executor = Executors.newFixedThreadPool(8);<br>while (true) {<br>Socket socket = serverSocket.accept();<br>RequesHandler requesHandler = new RequesHandler(socket);<br>executor.execute(requesHandler);<br>}<br>这样做似乎好了很多，通过一个固定大小的线程池，来负责管理工作线程，避免频繁创建、销毁线程的开销，这是我们构建并发服务的典型方式。这种工作方式，可以参考下图来理<br>解。<br>极客时间<br>如果连接数并不是非常多，只有最多几百个连接的普通应用，这种模式往往可以工作的很好。但是，如果连接数量急剧上升，这种实现方式就无法很好地工作了，因为线程上下文切<br>换开销会在高并发时变得很明显，这是同步阻塞方式的低扩展性劣势。<br>NIO引入的多路复用机制，提供了另外一种思路，请参考我下面提供的新的版本。<br>public class NIOServer extends Thread {<br>public void run() {<br>try (Selector selector = Selector.open();<br>ServerSocketChannel serverSocket = ServerSocketChannel.open();) {// 创建Selector和Channel<br>serverSocket.bind(new InetSocketAddress(InetAddress.getLocalHos(), 8888));<br>serverSocket.confgureBlocking(false);<br>// 注册到Selector，并说明关注点<br>serverSocket.regiser(selector, SelectionKey.OP_ACCEPT);<br>while (true) {<br>selector.select();// 阻塞等待就绪的Channel，这是关键点之一<br>Set selectedKeys = selector.selectedKeys();<br>Iterator iter = selectedKeys.iterator();<br>while (iter.hasNext()) {<br>SelectionKey key = iter.next();<br>// 生产系统中一般会额外进行就绪状态检查<br>sayHelloWorld((ServerSocketChannel) key.channel());<br>iter.remove();<br>}<br>}<br>} catch (IOException e) {<br>e.printStackTrace();<br>}<br>}<br>private void sayHelloWorld(ServerSocketChannel server) throws IOException {<br>try (SocketChannel client = server.accept();) { client.write(Charset.defaultCharset().encode(“Hello world!”));<br>}<br>}<br>// 省略了与前面类似的main<br>}<br>这个非常精简的样例掀开了NIO多路复用的面纱，我们可以分析下主要步骤和元素：<br>首先，通过Selector.open()创建一个Selector，作为类似调度员的角色。<br>然后，创建一个ServerSocketChannel，并且向Selector注册，通过指定SelectionKey.OP_ACCEPT，告诉调度员，它关注的是新的连接请求。<br>注意，为什么我们要明确配置非阻塞模式呢？这是因为阻塞模式下，注册操作是不允许的，会抛出IllegalBlockingModeException异常。<br>Selector阻塞在select操作，当有Channel发生接入请求，就会被唤醒。<br>在sayHelloWorld方法中，通过SocketChannel和Bufer进行数据操作，在本例中是发送了一段字符串。<br>可以看到，在前面两个样例中，IO都是同步阻塞模式，所以需要多线程以实现多任务处理。而NIO则是利用了单线程轮询事件的机制，通过高效地定位就绪的Channel，来决定做什<br>么，仅仅select阶段是阻塞的，可以有效避免大量客户端连接时，频繁线程切换带来的问题，应用的扩展能力有了非常大的提高。下面这张图对这种实现思路进行了形象地说明。<br>极客时间<br>在Java 7引入的NIO 2中，又增添了一种额外的异步IO模式，利用事件和回调，处理Accept、Read等操作。 AIO实现看起来是类似这样子：<br>AsynchronousServerSocketChannel serverSock = AsynchronousServerSocketChannel.open().bind(sockAddr);<br>serverSock.accept(serverSock, new CompletionHandler&lt;&gt;() { //为异步操作指定CompletionHandler回调函数<br>@Override<br>public void completed(AsynchronousSocketChannel sockChannel, AsynchronousServerSocketChannel serverSock) {<br>serverSock.accept(serverSock, this);<br>// 另外一个 write（sock，CompletionHandler{}）<br>sayHelloWorld(sockChannel, Charset.defaultCharset().encode<br>(“Hello World!”));<br>}<br>// 省略其他路径处理方法…<br>});<br>鉴于其编程要素（如Future、CompletionHandler等），我们还没有进行准备工作，为避免理解困难，我会在专栏后面相关概念补充后的再进行介绍，尤其<br>是Reactor、Proactor模式等方面将在Netty主题一起分析，这里我先进行概念性的对比：<br>基本抽象很相似，AsynchronousServerSocketChannel对应于上面例子中的ServerSocketChannel；AsynchronousSocketChannel则对应SocketChannel。<br>业务逻辑的关键在于，通过指定CompletionHandler回调接口，在accept/read/write等关键节点，通过事件机制调用，这是非常不同的一种编程思路。<br>今天我初步对Java提供的IO机制进行了介绍，概要地分析了传统同步IO和NIO的主要组成，并根据典型场景，通过不同的IO模式进行了实现与拆解。专栏下一讲，我还将继续分<br>析Java IO的主题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;初识NIO：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​    在 JDK 1. 4 中 新 加入 了 NIO( New Input/ Output) 类, 引入了一种基于通道和缓冲区的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Zero-Copy</title>
    <link href="https://masonnpe.github.io/2018/11/06/Netty/Zero-Copy/"/>
    <id>https://masonnpe.github.io/2018/11/06/Netty/Zero-Copy/</id>
    <published>2018-11-05T16:00:47.622Z</published>
    <updated>2018-11-05T16:02:28.434Z</updated>
    
    <content type="html"><![CDATA[<p>多个物理分离的buffer，通过逻辑上合并成为一个，从而避免了数据在内存之间的拷贝。</p><p>所谓的 <code>Zero-copy</code>, 就是在操作数据时, 不需要将数据 buffer 从一个内存区域拷贝到另一个内存区域. 因为少了一次内存的拷贝, 因此 CPU 的效率就得到的提升.</p><p>在 OS 层面上的 <code>Zero-copy</code> 通常指避免在 <code>用户态(User-space)</code> 与 <code>内核态(Kernel-space)</code> 之间来回拷贝数据. 例如 Linux 提供的 <code>mmap</code> 系统调用, 它可以将一段用户空间内存映射到内核空间, 当映射成功后, 用户对这段内存区域的修改可以直接反映到内核空间; 同样地, 内核空间对这段区域的修改也直接反映用户空间. 正因为有这样的映射关系, 我们就不需要在 <code>用户态(User-space)</code> 与 <code>内核态(Kernel-space)</code> 之间拷贝数据, 提高了数据传输的效率.</p><p>而需要注意的是, Netty 中的 <code>Zero-copy</code> 与上面我们所提到到 OS 层面上的 <code>Zero-copy</code> 不太一样, Netty的 <code>Zero-coyp</code>完全是在用户态(Java 层面)的, 它的 <code>Zero-copy</code> 的更多的是偏向于 <code>优化数据操作</code> 这样的概念.</p><p>Netty 的 <code>Zero-copy</code> 体现在如下几个个方面:</p><ul><li>Netty 提供了 <code>CompositeByteBuf</code> 类, 它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf, 避免了各个 ByteBuf 之间的拷贝.</li><li>通过 wrap 操作, 我们可以将 byte[] 数组、ByteBuf、ByteBuffer等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作.</li><li>ByteBuf 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf, 避免了内存的拷贝.</li><li>通过 <code>FileRegion</code> 包装的<code>FileChannel.tranferTo</code> 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 <code>Channel</code>, 避免了传统通过循环 write 方式导致的内存拷贝问题.</li></ul><p>下面我们就来简单了解一下这几种常见的零拷贝操作.</p><h2 id="通过-CompositeByteBuf-实现零拷贝"><a href="#通过-CompositeByteBuf-实现零拷贝" class="headerlink" title="通过 CompositeByteBuf 实现零拷贝"></a>通过 CompositeByteBuf 实现零拷贝</h2><p>假设我们有一份协议数据, 它由头部和消息体组成, 而头部和消息体是分别存放在两个 ByteBuf 中的, 即:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ByteBuf header = ...</span><br><span class="line">ByteBuf body = ...</span><br></pre></td></tr></table></figure><p>我们在代码处理中, 通常希望将 header 和 body 合并为一个 ByteBuf, 方便处理, 那么通常的做法是:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ByteBuf allBuf = Unpooled.buffer(header.readableBytes() + body.readableBytes());</span><br><span class="line">allBuf.writeBytes(header);</span><br><span class="line">allBuf.writeBytes(body);</span><br></pre></td></tr></table></figure><p>可以看到, 我们将 header 和 body 都拷贝到了新的 allBuf 中了, 这无形中增加了两次额外的数据拷贝操作了.</p><p>那么有没有更加高效优雅的方式实现相同的目的呢? 我们来看一下 <code>CompositeByteBuf</code> 是如何实现这样的需求的吧.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ByteBuf header = ...</span><br><span class="line">ByteBuf body = ...</span><br><span class="line"></span><br><span class="line">CompositeByteBuf compositeByteBuf = Unpooled.compositeBuffer();</span><br><span class="line">compositeByteBuf.addComponents(true, header, body);</span><br></pre></td></tr></table></figure><p>上面代码中, 我们定义了一个 <code>CompositeByteBuf</code> 对象, 然后调用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public CompositeByteBuf addComponents(boolean increaseWriterIndex, ByteBuf... buffers) &#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法将 <code>header</code> 与 <code>body</code> 合并为一个逻辑上的 ByteBuf, 即:</p><p><img src="https://images2015.cnblogs.com/blog/1006163/201611/1006163-20161122125123065-1400680861.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">不过需要注意的是, 虽然看起来 CompositeByteBuf 是由两个 ByteBuf 组合而成的, 不过在 CompositeByteBuf 内部, 这两个 ByteBuf 都是单独存在的, CompositeByteBuf 只是逻辑上是一个整体.</span><br></pre></td></tr></table></figure><p>上面 <code>CompositeByteBuf</code> 代码还以一个地方值得注意的是, 我们调用 <code>addComponents(boolean increaseWriterIndex, ByteBuf... buffers)</code> 来添加两个 ByteBuf, 其中第一个参数是 <code>true</code>, 表示当添加新的 ByteBuf 时, 自动递增 CompositeByteBuf 的 <code>writeIndex</code>.<br>如果我们调用的是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">compositeByteBuf.addComponents(header, body);</span><br></pre></td></tr></table></figure><p>那么其实 <code>compositeByteBuf</code> 的 <code>writeIndex</code> 仍然是0, 因此此时我们就不可能从 <code>compositeByteBuf</code> 中读取到数据, 这一点希望大家要特别注意.</p><p>除了上面直接使用 <code>CompositeByteBuf</code> 类外, 我们还可以使用 <code>Unpooled.wrappedBuffer</code> 方法, 它底层封装了 <code>CompositeByteBuf</code> 操作, 因此使用起来更加方便:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ByteBuf header = ...</span><br><span class="line">ByteBuf body = ...</span><br><span class="line"></span><br><span class="line">ByteBuf allByteBuf = Unpooled.wrappedBuffer(header, body);</span><br></pre></td></tr></table></figure><h2 id="通过-wrap-操作实现零拷贝"><a href="#通过-wrap-操作实现零拷贝" class="headerlink" title="通过 wrap 操作实现零拷贝"></a>通过 wrap 操作实现零拷贝</h2><p>例如我们有一个 byte 数组, 我们希望将它转换为一个 ByteBuf 对象, 以便于后续的操作, 那么传统的做法是将此 byte 数组拷贝到 ByteBuf 中, 即:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">byte[] bytes = ...</span><br><span class="line">ByteBuf byteBuf = Unpooled.buffer();</span><br><span class="line">byteBuf.writeBytes(bytes);</span><br></pre></td></tr></table></figure><p>显然这样的方式也是有一个额外的拷贝操作的, 我们可以使用 Unpooled 的相关方法, 包装这个 byte 数组, 生成一个新的 ByteBuf 实例, 而不需要进行拷贝操作. 上面的代码可以改为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">byte[] bytes = ...</span><br><span class="line">ByteBuf byteBuf = Unpooled.wrappedBuffer(bytes);</span><br></pre></td></tr></table></figure><p>可以看到, 我们通过 <code>Unpooled.wrappedBuffer</code> 方法来将 bytes 包装成为一个 UnpooledHeapByteBuf 对象, 而在包装的过程中, 是不会有拷贝操作的. 即最后我们生成的生成的 ByteBuf 对象是和 bytes 数组共用了同一个存储空间, 对 bytes 的修改也会反映到 ByteBuf 对象中.</p><p>Unpooled 工具类还提供了很多重载的 wrappedBuffer 方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public static ByteBuf wrappedBuffer(byte[] array)</span><br><span class="line">public static ByteBuf wrappedBuffer(byte[] array, int offset, int length)</span><br><span class="line"></span><br><span class="line">public static ByteBuf wrappedBuffer(ByteBuffer buffer)</span><br><span class="line">public static ByteBuf wrappedBuffer(ByteBuf buffer)</span><br><span class="line"></span><br><span class="line">public static ByteBuf wrappedBuffer(byte[]... arrays)</span><br><span class="line">public static ByteBuf wrappedBuffer(ByteBuf... buffers)</span><br><span class="line">public static ByteBuf wrappedBuffer(ByteBuffer... buffers)</span><br><span class="line"></span><br><span class="line">public static ByteBuf wrappedBuffer(int maxNumComponents, byte[]... arrays)</span><br><span class="line">public static ByteBuf wrappedBuffer(int maxNumComponents, ByteBuf... buffers)</span><br><span class="line">public static ByteBuf wrappedBuffer(int maxNumComponents, ByteBuffer... buffers)</span><br></pre></td></tr></table></figure><p>这些方法可以将一个或多个 buffer 包装为一个 ByteBuf 对象, 从而避免了拷贝操作.</p><h2 id="通过-slice-操作实现零拷贝"><a href="#通过-slice-操作实现零拷贝" class="headerlink" title="通过 slice 操作实现零拷贝"></a>通过 slice 操作实现零拷贝</h2><p>slice 操作和 wrap 操作刚好相反, <code>Unpooled.wrappedBuffer</code> 可以将多个 ByteBuf 合并为一个, 而 slice 操作可以将一个 ByteBuf <code>切片</code> 为多个共享一个存储区域的 ByteBuf 对象.<br>ByteBuf 提供了两个 slice 操作方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public ByteBuf slice();</span><br><span class="line">public ByteBuf slice(int index, int length);</span><br></pre></td></tr></table></figure><p>不带参数的 <code>slice</code> 方法等同于 <code>buf.slice(buf.readerIndex(), buf.readableBytes())</code> 调用, 即返回 buf 中可读部分的切片. 而 <code>slice(int index, int length)</code> 方法相对就比较灵活了, 我们可以设置不同的参数来获取到 buf 的不同区域的切片.</p><p>下面的例子展示了 <code>ByteBuf.slice</code> 方法的简单用法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ByteBuf byteBuf = ...</span><br><span class="line">ByteBuf header = byteBuf.slice(0, 5);</span><br><span class="line">ByteBuf body = byteBuf.slice(5, 10);</span><br></pre></td></tr></table></figure><p>用 <code>slice</code> 方法产生 header 和 body 的过程是没有拷贝操作的, header 和 body 对象在内部其实是共享了 byteBuf 存储空间的不同部分而已. 即:</p><p><img src="https://images2015.cnblogs.com/blog/1006163/201611/1006163-20161122125113893-136149474.png" alt="img"></p><h2 id="通过-FileRegion-实现零拷贝"><a href="#通过-FileRegion-实现零拷贝" class="headerlink" title="通过 FileRegion 实现零拷贝"></a>通过 FileRegion 实现零拷贝</h2><p>Netty 中使用 FileRegion 实现文件传输的零拷贝, 不过在底层 FileRegion 是依赖于 Java NIO <code>FileChannel.transfer</code> 的零拷贝功能.</p><p>首先我们从最基础的 Java IO 开始吧. 假设我们希望实现一个文件拷贝的功能, 那么使用传统的方式, 我们有如下实现:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public static void copyFile(String srcFile, String destFile) throws Exception &#123;</span><br><span class="line">    byte[] temp = new byte[1024];</span><br><span class="line">    FileInputStream in = new FileInputStream(srcFile);</span><br><span class="line">    FileOutputStream out = new FileOutputStream(destFile);</span><br><span class="line">    int length;</span><br><span class="line">    while ((length = in.read(temp)) != -1) &#123;</span><br><span class="line">        out.write(temp, 0, length);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    in.close();</span><br><span class="line">    out.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面是一个典型的读写二进制文件的代码实现了. 不用我说, 大家肯定都知道, 上面的代码中不断中源文件中读取定长数据到 temp 数组中, 然后再将 temp 中的内容写入目的文件, 这样的拷贝操作对于小文件倒是没有太大的影响, 但是如果我们需要拷贝大文件时, 频繁的内存拷贝操作就消耗大量的系统资源了.<br>下面我们来看一下使用 Java NIO 的 <code>FileChannel</code> 是如何实现零拷贝的:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public static void copyFileWithFileChannel(String srcFileName, String destFileName) throws Exception &#123;</span><br><span class="line">    RandomAccessFile srcFile = new RandomAccessFile(srcFileName, &quot;r&quot;);</span><br><span class="line">    FileChannel srcFileChannel = srcFile.getChannel();</span><br><span class="line"></span><br><span class="line">    RandomAccessFile destFile = new RandomAccessFile(destFileName, &quot;rw&quot;);</span><br><span class="line">    FileChannel destFileChannel = destFile.getChannel();</span><br><span class="line"></span><br><span class="line">    long position = 0;</span><br><span class="line">    long count = srcFileChannel.size();</span><br><span class="line"></span><br><span class="line">    srcFileChannel.transferTo(position, count, destFileChannel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到, 使用了 <code>FileChannel</code> 后, 我们就可以直接将源文件的内容直接拷贝(<code>transferTo</code>) 到目的文件中, 而不需要额外借助一个临时 buffer, 避免了不必要的内存操作.</p><p>有了上面的一些理论知识, 我们来看一下在 Netty 中是怎么使用 <code>FileRegion</code> 来实现零拷贝传输一个文件的:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123;</span><br><span class="line">    RandomAccessFile raf = null;</span><br><span class="line">    long length = -1;</span><br><span class="line">    try &#123;</span><br><span class="line">        // 1. 通过 RandomAccessFile 打开一个文件.</span><br><span class="line">        raf = new RandomAccessFile(msg, &quot;r&quot;);</span><br><span class="line">        length = raf.length();</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">        ctx.writeAndFlush(&quot;ERR: &quot; + e.getClass().getSimpleName() + &quot;: &quot; + e.getMessage() + &apos;\n&apos;);</span><br><span class="line">        return;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        if (length &lt; 0 &amp;&amp; raf != null) &#123;</span><br><span class="line">            raf.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ctx.write(&quot;OK: &quot; + raf.length() + &apos;\n&apos;);</span><br><span class="line">    if (ctx.pipeline().get(SslHandler.class) == null) &#123;</span><br><span class="line">        // SSL not enabled - can use zero-copy file transfer.</span><br><span class="line">        // 2. 调用 raf.getChannel() 获取一个 FileChannel.</span><br><span class="line">        // 3. 将 FileChannel 封装成一个 DefaultFileRegion</span><br><span class="line">        ctx.write(new DefaultFileRegion(raf.getChannel(), 0, length));</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        // SSL enabled - cannot use zero-copy file transfer.</span><br><span class="line">        ctx.write(new ChunkedFile(raf));</span><br><span class="line">    &#125;</span><br><span class="line">    ctx.writeAndFlush(&quot;\n&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码是 Netty 的一个例子, 其源码在 <strong>netty/example/src/main/java/io/netty/example/file/FileServerHandler.java</strong><br>可以看到, 第一步是通过 <code>RandomAccessFile</code> 打开一个文件, 然后 Netty 使用了 <code>DefaultFileRegion</code> 来封装一个 <code>FileChannel</code> 即:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new DefaultFileRegion(raf.getChannel(), 0, length)</span><br></pre></td></tr></table></figure><p>当有了 FileRegion 后, 我们就可以直接通过它将文件的内容直接写入 Channel 中, 而不需要像传统的做法: 拷贝文件内容到临时 buffer, 然后再将 buffer 写入 Channel. 通过这样的零拷贝操作, 无疑对传输大文件很有帮助.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;多个物理分离的buffer，通过逻辑上合并成为一个，从而避免了数据在内存之间的拷贝。&lt;/p&gt;
&lt;p&gt;所谓的 &lt;code&gt;Zero-copy&lt;/code&gt;, 就是在操作数据时, 不需要将数据 buffer 从一个内存区域拷贝到另一个内存区域. 因为少了一次内存的拷贝, 因此 
      
    
    </summary>
    
      <category term="Netty" scheme="https://masonnpe.github.io/categories/Netty/"/>
    
    
      <category term="Netty" scheme="https://masonnpe.github.io/tags/Netty/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch入门</title>
    <link href="https://masonnpe.github.io/2018/11/05/Elasticsearch/Elasticsearch%E5%85%A5%E9%97%A8/"/>
    <id>https://masonnpe.github.io/2018/11/05/Elasticsearch/Elasticsearch入门/</id>
    <published>2018-11-05T15:02:35.206Z</published>
    <updated>2018-11-05T15:05:45.669Z</updated>
    
    <content type="html"><![CDATA[<p> Elasticsearch是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。<br>Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单</p><h2 id="elasticsearch与solr比较"><a href="#elasticsearch与solr比较" class="headerlink" title="elasticsearch与solr比较"></a>elasticsearch与solr比较</h2><h3 id="solr"><a href="#solr" class="headerlink" title="solr"></a>solr</h3><p>优点<br>1、Solr有一个更大、更成熟的用户、开发和贡献者社区。<br>2、支持添加多种格式的索引，如：HTML、PDF、微软 Office 系列软件格式以及 JSON、XML、CSV 等纯文本格式。<br>3、Solr比较成熟、稳定。<br>4、不考虑建索引的同时进行搜索，速度更快。<br>缺点<br>建立索引时，搜索效率下降，实时索引搜索效率不高。</p><a id="more"></a><h3 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h3><p>优点<br>1、Elasticsearch是分布式的。不需要其他组件，分发是实时的，被叫做”Push replication”。<br>2、Elasticsearch 完全支持 Apache Lucene 的接近实时的搜索。<br>3、处理多租户（multitenancy）不需要特殊配置，而Solr则需要更多的高级设置。<br>4、Elasticsearch 采用 Gateway 的概念，使得完备份更加简单。<br>5、各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。<br>缺点<br>1、还不够自动，不适合当前新的Index Warmup API (参考：<a href="http://zhaoyanblog.com/archives/764.html" target="_blank" rel="noopener">http://zhaoyanblog.com/archives/764.html</a>)</p><p>总结：<br>1、当单纯的对已有数据进行搜索时，Solr更快。<br>2、当实时建立索引时, Solr会产生io阻塞，查询性能较差, Elasticsearch具有明显的优势。<br>3、随着数据量的增加，Solr的搜索效率会变得更低，而Elasticsearch却没有明显的变化。<br>4、Solr的架构不适合实时搜索的应用。<br>5、Solr 支持更多格式的数据，而 Elasticsearch 仅支持json文件格式<br>6、Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch<br>7、Solr 是传统搜索应用的有力解决方案，但 Elasticsearch 更适用于新兴的实时搜索应用  </p><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ul><li><a href="https://blog.csdn.net/laoyang360/article/details/52244917" target="_blank" rel="noopener">《Elasticsearch学习，请先看这一篇！》</a></li><li><p><a href="https://blog.csdn.net/cyony/article/details/65437708" target="_blank" rel="noopener">《Elasticsearch索引原理》</a></p></li><li><p><a href="https://blog.csdn.net/column/details/deep-elasticsearch.html" target="_blank" rel="noopener">死磕es</a></p></li><li><p><a href="https://blog.csdn.net/laoyang360/article/details/79293493" target="_blank" rel="noopener">es</a></p></li></ul><h3 id="elasticsearch资源汇总"><a href="#elasticsearch资源汇总" class="headerlink" title="elasticsearch资源汇总"></a>elasticsearch资源汇总</h3><ul><li><a href="https://github.com/dzharii/awesome-elasticsearch" target="_blank" rel="noopener">awesome-elasticsearch</a></li><li><a href="http://my.oschina.net/secisland/blog/636213" target="_blank" rel="noopener">插件汇总</a></li><li><a href="http://es.xiaoleilu.com/" target="_blank" rel="noopener">ES权威指南中文</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/getting-started.html" target="_blank" rel="noopener">ES权威指南英文</a></li><li><a href="https://github.com/floragunncom/search-guard" target="_blank" rel="noopener">ES安全search-guard</a></li><li><a href="http://www.searchtech.pro/" target="_blank" rel="noopener">elasticsearch相关介绍</a></li><li><a href="https://thinkbiganalytics.com/solr-vs-elastic-search/" target="_blank" rel="noopener">solr和elasticsearch对比</a></li><li><a href="https://www.easyice.cn/archives/207" target="_blank" rel="noopener">将 ELASTICSEARCH 写入速度优化到极限</a></li></ul><h3 id="剖析Elasticsearch集群系列"><a href="#剖析Elasticsearch集群系列" class="headerlink" title="剖析Elasticsearch集群系列"></a>剖析Elasticsearch集群系列</h3><ul><li><a href="http://insightdataengineering.com/blog/elasticsearch-crud/" target="_blank" rel="noopener">英文原文</a></li><li><a href="http://www.infoq.com/cn/articles/analysis-of-elasticsearch-cluster-part01?utm_campaign=rightbar_v2&amp;utm_source=infoq&amp;utm_medium=articles_link&amp;utm_content=link_text" target="_blank" rel="noopener">剖析Elasticsearch集群系列第一篇 存储模型和读写操作</a></li><li><a href="http://www.infoq.com/cn/articles/anatomy-of-an-elasticsearch-cluster-part02" target="_blank" rel="noopener">剖析Elasticsearch集群系列第二篇 分布式的三个C、translog和Lucene段</a></li><li><a href="http://www.infoq.com/cn/articles/anatomy-of-an-elasticsearch-cluster-part03?utm_campaign=rightbar_v2&amp;utm_source=infoq&amp;utm_medium=articles_link&amp;utm_content=link_text" target="_blank" rel="noopener">剖析Elasticsearch集群系列第三篇 近实时搜索、深层分页问题和搜索相关性权衡之道</a> </li><li><a href="https://segmentfault.com/a/1190000002803966" target="_blank" rel="noopener">nested 和parent使用介绍</a> </li></ul><h3 id="企业使用案例"><a href="#企业使用案例" class="headerlink" title="企业使用案例"></a>企业使用案例</h3><ul><li><a href="http://www.infoq.com/cn/articles/use-akka-kafka--build-analysis-engine?utm_source=infoq&amp;utm_medium=related_content_link&amp;utm_campaign=relatedContent_articles_clk" target="_blank" rel="noopener">使用Akka、Kafka和ElasticSearch等构建分析引擎</a></li><li><a href="http://chuansong.me/n/690173551706" target="_blank" rel="noopener">用Elasticsearch构建电商搜索平台，一个极有代表性的基础技术架构和算法实践案例</a></li><li><a href="http://h2ex.com/1584" target="_blank" rel="noopener">用Elasticsearch+Redis构建投诉监控系统，看Airbnb如何保证用户持续增长</a> </li><li><a href="http://www.10tiao.com/html/175/201707/2653548856/1.html" target="_blank" rel="noopener">基于Elasticsearch构建千亿流量日志搜索平台实战</a></li><li><a href="http://blog.csdn.net/jiao_fuyou/article/details/49663687" target="_blank" rel="noopener">Elasticsearch作为时间序列数据库</a></li></ul><h3 id="索引原理"><a href="#索引原理" class="headerlink" title="索引原理"></a>索引原理</h3><ul><li><a href="http://www.shaheng.me/blog/2015/06/elasticsearch--.html" target="_blank" rel="noopener">ES原理</a></li><li><a href="http://qindongliang.iteye.com/blog/2297280" target="_blank" rel="noopener">docvalues 介绍</a></li><li><a href="https://www.elastic.co/blog/found-dive-into-elasticsearch-storage" target="_blank" rel="noopener">ElasticSearch存储文件解析</a></li></ul><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><ul><li><a href="https://segmentfault.com/a/1190000004504225" target="_blank" rel="noopener">如何防止elasticsearch的脑裂问题</a></li></ul><h3 id="jar-conflic"><a href="#jar-conflic" class="headerlink" title="jar conflic"></a>jar conflic</h3><ul><li><a href="http://blog.csdn.net/sunshine920103/article/details/51659936" target="_blank" rel="noopener">通过maven-shade-plugin 解决Elasticsearch与hbase的jar包冲突问题</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Elasticsearch是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。&lt;br&gt;Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单&lt;/p&gt;
&lt;h2 id=&quot;elasticsearch与solr比较&quot;&gt;&lt;a href=&quot;#elasticsearch与solr比较&quot; class=&quot;headerlink&quot; title=&quot;elasticsearch与solr比较&quot;&gt;&lt;/a&gt;elasticsearch与solr比较&lt;/h2&gt;&lt;h3 id=&quot;solr&quot;&gt;&lt;a href=&quot;#solr&quot; class=&quot;headerlink&quot; title=&quot;solr&quot;&gt;&lt;/a&gt;solr&lt;/h3&gt;&lt;p&gt;优点&lt;br&gt;1、Solr有一个更大、更成熟的用户、开发和贡献者社区。&lt;br&gt;2、支持添加多种格式的索引，如：HTML、PDF、微软 Office 系列软件格式以及 JSON、XML、CSV 等纯文本格式。&lt;br&gt;3、Solr比较成熟、稳定。&lt;br&gt;4、不考虑建索引的同时进行搜索，速度更快。&lt;br&gt;缺点&lt;br&gt;建立索引时，搜索效率下降，实时索引搜索效率不高。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="https://masonnpe.github.io/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="https://masonnpe.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
</feed>
