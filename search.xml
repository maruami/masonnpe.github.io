<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SQL性能优化Explain]]></title>
    <url>%2F2018%2F12%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FSQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96Explain%2F</url>
    <content type="text"><![CDATA[简介MySQL 提供了一个 EXPLAIN 命令, 它可以对 SELECT 语句进行分析, 并输出 SELECT 执行的详细信息, 以供开发人员针对性优化。EXPLAIN 命令用法十分简单, 在 SELECT 语句前加上 Explain 就可以了。 准备建立两个测试用的表并添加相应的数据 1234567891011121314151617181920CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB DEFAULT CHARSET = utf8CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`)) ENGINE = InnoDB DEFAULT CHARSET = utf8 EXPLAIN 输出含义idSELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中内层的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. table表示查询涉及的表或衍生表 partitions匹配的分区 typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. type 常用类型type 常用的取值有: system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可.例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的. 1mysql&gt; explain select * from user_info where id = 2 eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如: 1mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询.例如下面这个例子中, 就使用到了 ref 类型的查询: 1mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5 range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. 1mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8 index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据.index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index. 1mysql&gt; EXPLAIN SELECT name FROM user_info 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免.下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的. 1mysql&gt; EXPLAIN SELECT age FROM user_info WHERE age = 20 type 类型的性能比较通常来说, 不同的 type 类型的性能关系如下:ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; systemALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的.而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快.后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key此字段是 MySQL 在当前查询时所真正使用到的索引. key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的栗子: 1mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id &lt; 3 AND product_name = &apos;p1&apos; AND productor = &apos;WHH&apos; 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 不过此查询语句 WHERE user_id &lt; 3 AND product_name = &#39;p1&#39; AND productor = &#39;WHH&#39; 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT &#39;0&#39;, 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: 12mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = &apos;p1&apos; 1 row in set, 1 warning (0.00 sec) 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = &#39;p1&#39; 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 ref哪个字段或常数与 key 一起被使用 rowsrows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. filtered表示此查询条件所过滤的数据的百分比 ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: 1mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY product_name 我们的索引是 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort.如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: 1mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库索引]]></title>
    <url>%2F2018%2F11%2F27%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[MySQL索引索引是在存储引擎层实现的，不同的存储引擎具有不同的索引类型和实现方式 B+Tree B+Tree是一种专门针对磁盘存储而优化的N叉排序树，以树的节点为单位存储在磁盘中，从根开始查找所需数据所在的节点编号和磁盘位置，将其加载到内存中然后继续查找，直到找到所需的数据。 B+Tree是大多数MySQL存储引擎默认的索引类型，是基于B-Tree和叶子节点顺序访问指针实现的，它所有叶子节点位于同一层，并且通过顺序访问指针来提高区间查询的性能。 进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找对应的数据 插入删除操作记录会破坏平衡树的平衡性，因此在插入删除操作之后，需要对树进行一个分裂、合并、旋转等操作来维护平衡性 B+Tree插入删除查找演示地址（需翻墙） 而许多NoSQL使用LSM树作为主要的数据结构，在LSM树上进行一次数据更新不需要磁盘访问，在内存即可完成，速度远超B+Tree。 数据库在做查询时IO消耗比较大，B+Tree每到一层就发生一次IO操作，为了减少IO操作，提高查询效率，要让这棵树尽可能的低 利于计算机的预读特性，内存和磁盘以片为单位读取数据，将连续的数据放到同一片里，读取会更快 红黑树也可以用来实现索引，但是树高，所以IO次数多 Hash 基于Hash表，查找速度很快，一般情况下查找的时间复杂度O(1)，仅需要一次查找就能定位数据。缺点： 失去有序性，无法用于排序和分组 只支持精确查找，无法用于部分查找和范围查找 在InnoDB引擎中，数据库自优化生成Hash索引 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，数据库根据查询情况，对使用频繁的索引自优化生成Hash索引 Fulltext 全文索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等，查找条件使用 MATCH AGAINST，而不是普通的 WHERE。InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引 R-Tree 空间数据索引,可以用于地理数据存储。空间数据索引会从所有维度来查找数据，可以有效地使用任意维度来进行组合查询，必须使用 GIS 相关的函数来维护数据 聚集索引 · 非聚集索引聚集索引 索引的键值逻辑顺序决定数据行的物理存储顺序 InnoDB主索引的叶子节点 data 域记录着完整的数据记录，因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找 所以不要使用过长的字段作为主键，过长的主索引会令辅助索引变得过大 推荐使用自增字段作为主键，否则非自增的主键会造成在插入新记录时，索引会为了维持B+Tree的特性而频繁的分裂调整 非聚集索引 索引的键值逻辑顺序，但数据行的物理存储地址不一定顺序 MyISAM叶子节点存放的是数据的物理地址而不是数据本身。在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用 单一索引 · 复合索引单一索引 新建索引的语句只实施在一列上，索引要建在数据区分度高的列上，像sex这种区分度低的就不需要单独建索引 复合索引 可以指定多个列作为索引列，组成复合索引。索引列数据区分度高的放在前面 按照where条件建索引，索引包含所有需要查询的字段的值 有复合索引unionindex(column1,column2)，就没有必要再建index(column1) 复合索引的索引列不要太多 优点 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 减少需要扫描的数据行数，加快数据的查询速度 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量 缺点 随着数据量的增加，建立和维护索引的时间和空间也会增加 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。会增加数据库写操作的成本，降低表更新的速度 会增加查询优化器的选择时间 如何创建索引在哪儿建索引 经常需要搜索的列上，可以加快搜索的速度 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 多表join关联列 select update delete where 的列 orderby groupby distinct中的字段 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 选择索引列的顺序 区分度最高的放最左侧 字段长度小的列放左 最频繁使用的列放左 不该在哪儿建索引 对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 对于那些只有数据值区分区很小的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 数据类型为text, image和bit数据类型的列不应该增加索引 当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 索引失效索引列是表达式的一部分或函数的参数不能使用索引 12select * from tbl_user where id+1=2;select * from tbl_user having max(id); 复合索引最左前缀不能使用索引 12create index index_id_username on tbl_user(id,username);select * from tbl_user where username='root'; 负向查询不能使用索引 1select name from user where id not in (1,3,4); 前导模糊查询不能使用索引 1select name from user where name like '%zhangsan' 索引原理局部性原理与磁盘预读：由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 explain通过MySQL工具explain+查询语句 select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transient]]></title>
    <url>%2F2018%2F11%2F27%2FJava%2Ftransient%2F</url>
    <content type="text"><![CDATA[对于实现Serializable接口的类，属性前添加关键字transient，序列化对象的时候，这个属性就不会被序列化 1transient Object[] elementData; 注意点 要使transient生效，类需要实现Serializable接口 transient关键字只能修饰变量，不能修饰方法和类 静态变量不管是否被transient修饰，均不能被序列化，反序列化后对象中static型变量的值为当前JVM中对应类中static变量的值 测试代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 static class TransientDemo implements Serializable &#123; private static final long serialVersionUID = -5818461504980738394L; private String name; private String password; private static String staticPassword; private transient static String transientAndStaticPassword;// get/set省略 &#125; public static void main(String[] args) &#123; TransientDemo transientDemo = new TransientDemo(); transientDemo.setName("root"); transientDemo.setPassword("*********"); transientDemo.setStaticPassword("*********"); transientDemo.setTransientAndStaticPassword("*********"); ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte[] b = null; try &#123; ObjectOutputStream oos = new ObjectOutputStream(baos); oos.writeObject(transientDemo); b = baos.toByteArray(); System.err.println("before name=" + transientDemo.getName()); System.err.println("before getPassword=" + transientDemo.getPassword()); System.err.println("before getStaticPassword=" + transientDemo.getStaticPassword()); System.err.println("before getTransientAndStaticPassword=" + transientDemo.getTransientAndStaticPassword()); TransientDemo.staticPassword = "我不是真的"; TransientDemo.transientAndStaticPassword = "我不是真的"; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; ByteArrayInputStream bais = new ByteArrayInputStream(b); try &#123; ObjectInputStream ois = new ObjectInputStream(bais); TransientDemo demo = (TransientDemo) ois.readObject(); System.err.println("after name=" + demo.getName()); System.err.println("after getPassword=" + demo.getPassword()); System.err.println("after getStaticPassword=" + demo.getStaticPassword()); System.err.println("after getTransientAndStaticPassword=" + demo.getTransientAndStaticPassword()); &#125; catch (IOException | ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; 打印结果 12345678before name=rootbefore getPassword=*********before getStaticPassword=*********before getTransientAndStaticPassword=*********after name=rootafter getPassword=*********after getStaticPassword=我不是真的after getTransientAndStaticPassword=我不是真的 自定义序列化方式被transient修饰的属性可以通过writeObject()方法，自定义属性的序列化方式 参考JDK1.8 ArrayList自定义了对elementData的序列化与反序列化 123456789101112131415161718192021222324252627282930313233343536373839404142434445transient Object[] elementData; // non-private to simplify nested class accessprivate void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); //只序列化被使用的数据 &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity int capacity = calculateCapacity(elementData, size); SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity); ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); //反序列化 &#125; &#125; &#125; ExternalizableExternalizable和Serializable相比，Externalizable是Serializable接口的子类 Serializable序列化时不会调用默认的构造器，而Externalizable序列化时会调用默认构造器的，否则会报java.io.InvalidClassException 实现了Serializable接口的类，默认会序列化对象的所有属性（包括private属性和引用的对象） Externalizable默认不序列化对象的任何属性，通过重写writeExternal()和readExternal()方法指定序列化哪些属性]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>transient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[static]]></title>
    <url>%2F2018%2F11%2F27%2FJava%2Fstatic%2F</url>
    <content type="text"><![CDATA[静态变量 静态变量属于类，可以直接通过类名来访问它 类的实例共享静态变量 静态方法 静态方法必须有实现 只能访问所属类的静态变量和静态方法，且方法中不能有 this 和 super 关键字 静态语句块 静态语句块只会在类初始化时运行一次 静态内部类 非静态内部类依赖于外部类的实例，而静态内部类不需要 静态内部类不能访问外部类的非静态的变量和方法 静态导包 在使用静态变量和方法时不用再指明类名 1234567import static java.util.concurrent.Executors.*;public abstract class Solution &#123; public static void main(String[] args) &#123; newCachedThreadPool(); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>static</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Set]]></title>
    <url>%2F2018%2F11%2F27%2FJava%2FSet%2F</url>
    <content type="text"><![CDATA[特点：不会存储重复的元素 HashSetObject的hashCode方法的话，hashCode会返回每个对象特有的序号（Java是依据对象的内存地址计算出的此序号），所以两个不同的对象的hashCode值是不可能相等的。 如果想要让两个不同的Object对象视为相等的，就必须重写类的hashCode方法和equals方法，同时也需要两个不同对象比较equals方法会返回true HashSet和ArrayList都有判断元素是否相同的方法 ArrayList使用boolean contains(Object o)–&gt;indexOf(Object o)–&gt;equals HashSet使用hashCode和equals方法 通过hashCode方法和equals方法来保证元素的唯一性，add()返回的是boolean类型 判断两个元素是否相同，先判断元素的hashCode值是否一致，如果相同才会去判断equals方法 TreeSet基于红黑树实现，不能重复存储元素，还能实现排序 有两种自定义排序规则的方法 让存入的元素自身具有比较性 实现Comparable接口，重写compareTo(Object o)方法 给TreeSet指定排序规则 定义一个类实现接口Comparator，重写compare(Object o1, Object o2）方法，该接口的子类实例对象作为参数传递给TreeMap集合的构造方法 如果Comparable和Comparator同时存在，优先Comparator LinkedHashSet会保存与元素插入的顺序 总结看到array，就要想到下标 看到link，就要想到first，last 看到hash，就要想到hashCode,equals 看到tree，就要想到两个接口。Comparable，Comparator]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法题解题思路]]></title>
    <url>%2F2018%2F11%2F26%2FAlgorithm%2F%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%E9%A2%98%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[算法题套路 如果是数据排序方面的题，那基本上是和二分查找有关系的。 如果是在一个无序数组上的搜索或者统计，基本上来说需要动用 O(1) 时间复杂度的 hash 数据结构。 在一堆无序的数据中找 top n 的算法，基本上来说，就是使用最大堆或是最小堆的数据结构。 如果是穷举答案相关的题（如八皇后、二叉树等），基本上来说，需要使用深度优先、广度优先或是回溯等递归的思路。 动态规划要重点准备一下，这样的题很多，如最大和子数组、买卖股票、背包问题、爬楼梯、改字符……这里有一个 Top 20 的动态规划题的列表 。 一些经典的数据结构算法也要看一下，比如，二叉树、链表和数组上的经典算法，LRU 算法，Tier 树，字符串子串匹配，回文等，这些常见的题都是经常会被考到的。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CAS]]></title>
    <url>%2F2018%2F11%2F26%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FCAS%2F</url>
    <content type="text"><![CDATA[CAS使用锁时，线程获取锁是一种悲观锁策略，即假设每一次执行临界区代码都会产生冲突，所以当前线程获取到锁的时候同时也会阻塞其他线程获取该锁。而CAS操作是一种乐观锁策略，它假设所有线程访问共享资源的时候不会出现冲突，既然不会出现冲突自然而然就不会阻塞其他线程的操作。如果出现冲突就重试当前操作直到没有冲突为止。 操作过程CAS比较交换的过程可以通俗的理解为CAS(V,O,N)，包含三个值分别为：V 内存地址存放的实际值；O 预期的值（旧值）；N 更新的新值。当V和O相同时，也就是说旧值和内存中实际的值相同表明该值没有被其他线程更改过，即该旧值O就是目前来说最新的值了，自然而然可以将新值N赋值给V。反之，V和O不相同，表明该值已经被其他线程改过了则该旧值O不是最新版本的值了，所以不能将新值N赋给V，返回V即可。当多个线程使用CAS操作一个变量是，只有一个线程会成功，并成功更新，其余会失败。失败的线程会重新尝试，当然也可以选择挂起线程 Synchronized VS CAS 1.5前的Synchronized在存在线程竞争的情况下会出现线程阻塞和唤醒锁带来的性能问题 CAS并不是武断的间线程挂起，当CAS操作失败后会进行一定的尝试，而非进行耗时的挂起唤醒的操作 CAS的应用场景在J.U.C包中利用CAS实现类有很多，可以说是支撑起整个concurrency包的实现，在Lock实现中会有CAS改变state变量，在atomic包中的实现类也几乎都是用CAS实现。 CAS与对象创建那就是在JVM创建对象的过程中。对象创建在虚拟机中是非常频繁的。即使是仅仅修改一个指针所指向的位置，在并发情况下也不是线程安全的，可能正在给对象A分配内存空间，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。解决这个问题的方案有两种，其中一种就是采用CAS配上失败重试的方式保证更新操作的原子性。 CAS的问题1. ABA问题因为CAS会检查旧值有没有变化，这里存在这样一个有意思的问题。比如一个旧值A变为了成B，然后再变成A，刚好在做CAS时检查发现旧值并没有变化依然为A，但是实际上的确发生了变化。解决方案可以沿袭数据库中常用的乐观锁方式，添加一个版本号可以解决。JDK1.5后的atomic包中提供了AtomicStampedReference来解决ABA问题。它通过包装[E,Integer]的元组来对对象标记版本戳stamp，从而避免ABA问题，例如下面的代码分别用AtomicInteger和AtomicStampedReference来对初始值为100的原子整型变量进行更新，AtomicInteger会成功执行CAS操作，而加上版本戳的AtomicStampedReference对于ABA问题会执行CAS失败。 2. 自旋时间过长 使用CAS时非阻塞同步，也就是说不会将线程挂起，会自旋（无非就是一个死循环）进行下一次尝试，如果这里自旋时间过长对性能是很大的消耗。 3. 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时CAS能保证其原子性，如果对多个共享变量进行操作,CAS就不能保证其原子性。有一个解决方案是利用对象整合多个共享变量，即一个类中的成员变量就是这几个共享变量。然后将这个对象做CAS操作就可以保证其原子性。atomic中提供了AtomicReference来保证引用对象之间的原子性。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>CAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL主从复制]]></title>
    <url>%2F2018%2F11%2F25%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FMySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[MySQL主从复制原理 主库将增删改操作写binlog日志，从库连接到主库之后有一个IO线程，将主库的binlog日志拷贝到本地，写入到一个中继日志中，接着从库中有一个SQL线程会从中继日志读取binlog日志，并执行binlog日志中的内容，也就是在本地再次执行一遍SQL，这样就可以保证自己跟主库的数据是一样的。这里有一个非常重要的点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。所以MySQL在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 半同步复制（semi-sync复制），指的就是主库写入binlog日志之后，就会立即将数据同步到从库，从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库的ack之后才会认为写操作完成了。 并行复制，指的是从库开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 MySQL主从同步延时 一般在读远远多于写的场景下来用这个MySQL主从同步，而且读的时候一般对数据时效性要求没那么高。主从同步延时问题，会导致一些线上的bug难以发现。可以通过show status，Seconds_Behind_Master，看到从库复制主库的数据落后了多少ms。 虽然可以用MySQL的并行复制，但是那是库级别的并行，时候作用不是很大。所以对于那种写了之后立马就要保证可以查到的场景，采用强制读主库的方式，这样就可以保证可以读到数据。也可以重写业务代码比如第二句SQL不依赖第一句SQL，就直接更新不查询，如果依赖就先读判断结果是否为空，如果为空就报错下次重试。 实现MySQL的读写分离基于主从复制架构，简单来说就是搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyBatis缓存]]></title>
    <url>%2F2018%2F11%2F25%2FMyBatis%2FMyBatis%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[前言MyBatis是常见的Java数据库访问层框架。在日常工作中，开发人员多数情况下是使用MyBatis的默认缓存配置，但是MyBatis缓存机制有一些不足之处，在使用中容易引起脏数据，形成一些潜在的隐患。个人在业务开发中也处理过一些由于MyBatis缓存引发的开发问题，带着个人的兴趣，希望从应用及源码的角度为读者梳理MyBatis缓存机制。本次分析中涉及到的代码和数据库表均放在GitHub上，地址： mybatis-cache-demo 。 目录本文按照以下顺序展开。 一级缓存介绍及相关配置。 一级缓存工作流程及源码分析。 一级缓存总结。 二级缓存介绍及相关配置。 二级缓存源码分析。 二级缓存总结。 全文总结。 一级缓存一级缓存介绍在应用运行过程中，我们有可能在一次数据库会话中，执行多次查询条件完全相同的SQL，MyBatis提供了一级缓存的方案优化这部分场景，如果是相同的SQL语句，会优先命中一级缓存，避免直接对数据库进行查询，提高性能。具体执行过程如下图所示。每个SqlSession中持有了Executor，每个Executor中有一个LocalCache。当用户发起查询时，MyBatis根据当前执行的语句生成MappedStatement，在Local Cache进行查询，如果缓存命中的话，直接返回结果给用户，如果缓存没有命中的话，查询数据库，结果写入Local Cache，最后返回结果给用户。具体实现类的类关系图如下图所示。 一级缓存配置我们来看看如何使用MyBatis一级缓存。开发者只需在MyBatis的配置文件中，添加如下语句，就可以使用一级缓存。共有两个选项，SESSION或者STATEMENT，默认是SESSION级别，即在一个MyBatis会话中执行的所有语句，都会共享这一个缓存。一种是STATEMENT级别，可以理解为缓存只对当前执行的这一个Statement有效。 1&lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt; 一级缓存实验接下来通过实验，了解MyBatis一级缓存的效果，每个单元测试后都请恢复被修改的数据。首先是创建示例表student，创建对应的POJO类和增改的方法，具体可以在entity包和mapper包中查看。 123456CREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(200) COLLATE utf8_bin DEFAULT NULL, `age` tinyint(3) unsigned DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 在以下实验中，id为1的学生名称是凯伦。 实验1开启一级缓存，范围为会话级别，调用三次getStudentById，代码如下所示： 1234567public void getStudentById() throws Exception &#123; SqlSession sqlSession = factory.openSession(true); // 自动提交事务 StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); System.out.println(studentMapper.getStudentById(1)); System.out.println(studentMapper.getStudentById(1)); System.out.println(studentMapper.getStudentById(1)); &#125; 执行结果：我们可以看到，只有第一次真正查询了数据库，后续的查询使用了一级缓存。 实验2增加了对数据库的修改操作，验证在一次数据库会话中，如果对数据库发生了修改操作，一级缓存是否会失效。 123456789@Testpublic void addStudent() throws Exception &#123; SqlSession sqlSession = factory.openSession(true); // 自动提交事务 StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); System.out.println(studentMapper.getStudentById(1)); System.out.println(&quot;增加了&quot; + studentMapper.addStudent(buildStudent()) + &quot;个学生&quot;); System.out.println(studentMapper.getStudentById(1)); sqlSession.close();&#125; 执行结果：我们可以看到，在修改操作后执行的相同查询，查询了数据库，一级缓存失效。 实验3开启两个SqlSession，在sqlSession1中查询数据，使一级缓存生效，在sqlSession2中更新数据库，验证一级缓存只在数据库会话内部共享。 1234567891011121314@Testpublic void testLocalCacheScope() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); System.out.println(&quot;studentMapper2更新了&quot; + studentMapper2.updateStudentName(&quot;小岑&quot;,1) + &quot;个学生的数据&quot;); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));&#125; sqlSession2更新了id为1的学生的姓名，从凯伦改为了小岑，但session1之后的查询中，id为1的学生的名字还是凯伦，出现了脏数据，也证明了之前的设想，一级缓存只在数据库会话内部共享。 一级缓存工作流程&amp;源码分析那么，一级缓存的工作流程是怎样的呢？我们从源码层面来学习一下。 工作流程一级缓存执行的时序图，如下图所示。 源码分析接下来将对MyBatis查询相关的核心类和一级缓存的源码进行走读。这对后面学习二级缓存也有帮助。SqlSession： 对外提供了用户和数据库之间交互需要的所有方法，隐藏了底层的细节。默认实现类是DefaultSqlSession。 Executor： SqlSession向用户提供操作数据库的方法，但和数据库操作有关的职责都会委托给Executor。 如下图所示，Executor有若干个实现类，为Executor赋予了不同的能力，大家可以根据类名，自行学习每个类的基本作用。 在一级缓存的源码分析中，主要学习BaseExecutor的内部实现。BaseExecutor： BaseExecutor是一个实现了Executor接口的抽象类，定义若干抽象方法，在执行的时候，把具体的操作委托给子类进行执行。 1234protected abstract int doUpdate(MappedStatement ms, Object parameter) throws SQLException;protected abstract List&lt;BatchResult&gt; doFlushStatements(boolean isRollback) throws SQLException;protected abstract &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException;protected abstract &lt;E&gt; Cursor&lt;E&gt; doQueryCursor(MappedStatement ms, Object parameter, RowBounds rowBounds, BoundSql boundSql) throws SQLException; 在一级缓存的介绍中提到对Local Cache的查询和写入是在Executor内部完成的。在阅读BaseExecutor的代码后发现Local Cache是BaseExecutor内部的一个成员变量，如下代码所示。 123public abstract class BaseExecutor implements Executor &#123;protected ConcurrentLinkedQueue&lt;DeferredLoad&gt; deferredLoads;protected PerpetualCache localCache; Cache： MyBatis中的Cache接口，提供了和缓存相关的最基本的操作，如下图所示。有若干个实现类，使用装饰器模式互相组装，提供丰富的操控缓存的能力，部分实现类如下图所示。BaseExecutor成员变量之一的PerpetualCache，是对Cache接口最基本的实现，其实现非常简单，内部持有HashMap，对一级缓存的操作实则是对HashMap的操作。如下代码所示。 123public class PerpetualCache implements Cache &#123; private String id; private Map&lt;Object, Object&gt; cache = new HashMap&lt;Object, Object&gt;(); 在阅读相关核心类代码后，从源代码层面对一级缓存工作中涉及到的相关代码，出于篇幅的考虑，对源码做适当删减，读者朋友可以结合本文，后续进行更详细的学习。为执行和数据库的交互，首先需要初始化SqlSession，通过DefaultSqlSessionFactory开启SqlSession： 12345private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; ............ final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit);&#125; 在初始化SqlSesion时，会使用Configuration类创建一个全新的Executor，作为DefaultSqlSession构造函数的参数，创建Executor代码如下所示： 123456789101112131415161718public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; // 尤其可以注意这里，如果二级缓存开关开启的话，是使用CahingExecutor装饰BaseExecutor的子类 if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; executor = (Executor) interceptorChain.pluginAll(executor); return executor;&#125; SqlSession创建完毕后，根据Statment的不同类型，会进入SqlSession的不同方法中，如果是Select语句的话，最后会执行到SqlSession的selectList，代码如下所示： 12345@Overridepublic &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);&#125; SqlSession把具体的查询职责委托给了Executor。如果只开启了一级缓存的话，首先会进入BaseExecutor的query方法。代码如下所示： 123456@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123; BoundSql boundSql = ms.getBoundSql(parameter); CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql); return query(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; 在上述代码中，会先根据传入的参数生成CacheKey，进入该方法查看CacheKey是如何生成的，代码如下所示： 1234567CacheKey cacheKey = new CacheKey();cacheKey.update(ms.getId());cacheKey.update(rowBounds.getOffset());cacheKey.update(rowBounds.getLimit());cacheKey.update(boundSql.getSql());//后面是update了sql中带的参数cacheKey.update(value); 在上述的代码中，将MappedStatement的Id、sql的offset、Sql的limit、Sql本身以及Sql中的参数传入了CacheKey这个类，最终构成CacheKey。以下是这个类的内部结构： 123456789101112131415private static final int DEFAULT_MULTIPLYER = 37;private static final int DEFAULT_HASHCODE = 17;private int multiplier;private int hashcode;private long checksum;private int count;private List&lt;Object&gt; updateList;public CacheKey() &#123; this.hashcode = DEFAULT_HASHCODE; this.multiplier = DEFAULT_MULTIPLYER; this.count = 0; this.updateList = new ArrayList&lt;Object&gt;();&#125; 首先是成员变量和构造函数，有一个初始的hachcode和乘数，同时维护了一个内部的updatelist。在CacheKey的update方法中，会进行一个hashcode和checksum的计算，同时把传入的参数添加进updatelist中。如下代码所示。 123456789public void update(Object object) &#123; int baseHashCode = object == null ? 1 : ArrayUtil.hashCode(object); count++; checksum += baseHashCode; baseHashCode *= count; hashcode = multiplier * hashcode + baseHashCode; updateList.add(object);&#125; 同时重写了CacheKey的equals方法，代码如下所示： 123456789101112@Overridepublic boolean equals(Object object) &#123; ............. for (int i = 0; i &lt; updateList.size(); i++) &#123; Object thisObject = updateList.get(i); Object thatObject = cacheKey.updateList.get(i); if (!ArrayUtil.equals(thisObject, thatObject)) &#123; return false; &#125; &#125; return true;&#125; 除去hashcode，checksum和count的比较外，只要updatelist中的元素一一对应相等，那么就可以认为是CacheKey相等。只要两条SQL的下列五个值相同，即可以认为是相同的SQL。 Statement Id + Offset + Limmit + Sql + Params BaseExecutor的query方法继续往下走，代码如下所示： 1234567list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null;if (list != null) &#123; // 这个主要是处理存储过程用的。 handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); &#125; else &#123; list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; 如果查不到的话，就从数据库查，在queryFromDatabase中，会对localcache进行写入。在query方法执行的最后，会判断一级缓存级别是否是STATEMENT级别，如果是的话，就清空缓存，这也就是STATEMENT级别的一级缓存无法共享localCache的原因。代码如下所示： 123if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123; clearLocalCache();&#125; 在源码分析的最后，我们确认一下，如果是insert/delete/update方法，缓存就会刷新的原因。SqlSession的insert方法和delete方法，都会统一走update的流程，代码如下所示： 12345678@Overridepublic int insert(String statement, Object parameter) &#123; return update(statement, parameter); &#125; @Override public int delete(String statement) &#123; return update(statement, null);&#125; update方法也是委托给了Executor执行。BaseExecutor的执行方法如下所示。 123456789@Overridepublic int update(MappedStatement ms, Object parameter) throws SQLException &#123; ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing an update&quot;).object(ms.getId()); if (closed) &#123; throw new ExecutorException(&quot;Executor was closed.&quot;); &#125; clearLocalCache(); return doUpdate(ms, parameter);&#125; 每次执行update前都会清空localCache。 至此，一级缓存的工作流程讲解以及源码分析完毕。 总结 MyBatis一级缓存的生命周期和SqlSession一致。 MyBatis一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。 MyBatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement。 二级缓存二级缓存介绍在上文中提到的一级缓存中，其最大的共享范围就是一个SqlSession内部，如果多个SqlSession之间需要共享缓存，则需要使用到二级缓存。开启二级缓存后，会使用CachingExecutor装饰Executor，进入一级缓存的查询流程前，先在CachingExecutor进行二级缓存的查询，具体的工作流程如下所示。 二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。当开启缓存后，数据的查询执行的流程就是 二级缓存 -&gt; 一级缓存 -&gt; 数据库。 二级缓存配置要正确的使用二级缓存，需完成如下配置的。 在MyBatis的配置文件中开启二级缓存。 1&lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; 在MyBatis的映射XML中配置cache或者 cache-ref 。 cache标签用于声明这个namespace使用二级缓存，并且可以自定义配置。 1&lt;cache/&gt; type：cache使用的类型，默认是PerpetualCache，这在一级缓存中提到过。 eviction： 定义回收的策略，常见的有FIFO，LRU。 flushInterval： 配置一定时间自动刷新缓存，单位是毫秒。 size： 最多缓存对象的个数。 readOnly： 是否只读，若配置可读写，则需要对应的实体类能够序列化。 blocking： 若缓存中找不到对应的key，是否会一直blocking，直到有对应的数据进入缓存。 cache-ref代表引用别的命名空间的Cache配置，两个命名空间的操作使用的是同一个Cache。 1&lt;cache-ref namespace=&quot;mapper.StudentMapper&quot;/&gt; 二级缓存实验接下来我们通过实验，了解MyBatis二级缓存在使用上的一些特点。在本实验中，id为1的学生名称初始化为点点。 实验1测试二级缓存效果，不提交事务，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。 1234567891011@Testpublic void testCacheWithoutCommitOrClose() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));&#125; 执行结果：我们可以看到，当sqlsession没有调用commit()方法时，二级缓存并没有起到作用。 实验2测试二级缓存效果，当提交事务时，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。 123456789101112@Testpublic void testCacheWithCommitOrClose() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); sqlSession1.commit(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));&#125; 从图上可知，sqlsession2的查询，使用了缓存，缓存的命中率是0.5。 实验3测试update操作是否会刷新该namespace下的二级缓存。 123456789101112131415161718@Testpublic void testCacheWithUpdate() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); SqlSession sqlSession3 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); StudentMapper studentMapper3 = sqlSession3.getMapper(StudentMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); sqlSession1.commit(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1)); studentMapper3.updateStudentName(&quot;方方&quot;,1); sqlSession3.commit(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));&#125; 我们可以看到，在sqlSession3更新数据库，并提交事务后，sqlsession2的StudentMapper namespace下的查询走了数据库，没有走Cache。 实验4验证MyBatis的二级缓存不适应用于映射文件中存在多表查询的情况。通常我们会为每个单表创建单独的映射文件，由于MyBatis的二级缓存是基于namespace的，多表查询语句所在的namspace无法感应到其他namespace中的语句对多表查询中涉及的表进行的修改，引发脏数据问题。 123456789101112131415161718@Testpublic void testCacheWithDiffererntNamespace() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); SqlSession sqlSession3 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); ClassMapper classMapper = sqlSession3.getMapper(ClassMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentByIdWithClassInfo(1)); sqlSession1.close(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentByIdWithClassInfo(1)); classMapper.updateClassName(&quot;特色一班&quot;,1); sqlSession3.commit(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentByIdWithClassInfo(1));&#125; 执行结果：在这个实验中，我们引入了两张新的表，一张class，一张classroom。class中保存了班级的id和班级名，classroom中保存了班级id和学生id。我们在StudentMapper中增加了一个查询方法getStudentByIdWithClassInfo，用于查询学生所在的班级，涉及到多表查询。在ClassMapper中添加了updateClassName，根据班级id更新班级名的操作。当sqlsession1的studentmapper查询数据后，二级缓存生效。保存在StudentMapper的namespace下的cache中。当sqlSession3的classMapper的updateClassName方法对class表进行更新时，updateClassName不属于StudentMapper的namespace，所以StudentMapper下的cache没有感应到变化，没有刷新缓存。当StudentMapper中同样的查询再次发起时，从缓存中读取了脏数据。 实验5为了解决实验4的问题呢，可以使用Cache ref，让ClassMapper引用StudenMapper命名空间，这样两个映射文件对应的Sql操作都使用的是同一块缓存了。执行结果：不过这样做的后果是，缓存的粒度变粗了，多个Mapper namespace下的所有操作都会对缓存使用造成影响。 二级缓存源码分析MyBatis二级缓存的工作流程和前文提到的一级缓存类似，只是在一级缓存处理前，用CachingExecutor装饰了BaseExecutor的子类，在委托具体职责给delegate之前，实现了二级缓存的查询和写入功能，具体类关系图如下图所示。 源码分析源码分析从CachingExecutor的query方法展开，源代码走读过程中涉及到的知识点较多，不能一一详细讲解，读者朋友可以自行查询相关资料来学习。CachingExecutor的query方法，首先会从MappedStatement中获得在配置初始化时赋予的Cache。 1Cache cache = ms.getCache(); 本质上是装饰器模式的使用，具体的装饰链是 SynchronizedCache -&gt; LoggingCache -&gt; SerializedCache -&gt; LruCache -&gt; PerpetualCache。 以下是具体这些Cache实现类的介绍，他们的组合为Cache赋予了不同的能力。 SynchronizedCache： 同步Cache，实现比较简单，直接使用synchronized修饰方法。 LoggingCache： 日志功能，装饰类，用于记录缓存的命中率，如果开启了DEBUG模式，则会输出命中率日志。 SerializedCache： 序列化功能，将值序列化后存到缓存中。该功能用于缓存返回一份实例的Copy，用于保存线程安全。 LruCache： 采用了Lru算法的Cache实现，移除最近最少使用的key/value。 PerpetualCache： 作为为最基础的缓存类，底层实现比较简单，直接使用了HashMap。 然后是判断是否需要刷新缓存，代码如下所示： 1flushCacheIfRequired(ms); 在默认的设置中SELECT语句不会刷新缓存，insert/update/delte会刷新缓存。进入该方法。代码如下所示： 123456private void flushCacheIfRequired(MappedStatement ms) &#123; Cache cache = ms.getCache(); if (cache != null &amp;&amp; ms.isFlushCacheRequired()) &#123; tcm.clear(cache); &#125;&#125; MyBatis的CachingExecutor持有了TransactionalCacheManager，即上述代码中的tcm。TransactionalCacheManager中持有了一个Map，代码如下所示： 1private Map&lt;Cache, TransactionalCache&gt; transactionalCaches = new HashMap&lt;Cache, TransactionalCache&gt;(); 这个Map保存了Cache和用TransactionalCache包装后的Cache的映射关系。TransactionalCache实现了Cache接口，CachingExecutor会默认使用他包装初始生成的Cache，作用是如果事务提交，对缓存的操作才会生效，如果事务回滚或者不提交事务，则不对缓存产生影响。在TransactionalCache的clear，有以下两句。清空了需要在提交时加入缓存的列表，同时设定提交时清空缓存，代码如下所示： 12345@Overridepublic void clear() &#123; clearOnCommit = true; entriesToAddOnCommit.clear();&#125; CachingExecutor继续往下走，ensureNoOutParams主要是用来处理存储过程的，暂时不用考虑。 12if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, parameterObject, boundSql); 之后会尝试从tcm中获取缓存的列表。 1List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key); 在getObject方法中，会把获取值的职责一路传递，最终到PerpetualCache。如果没有查到，会把key加入Miss集合，这个主要是为了统计命中率。 1234Object object = delegate.getObject(key);if (object == null) &#123; entriesMissedInCache.add(key);&#125; CachingExecutor继续往下走，如果查询到数据，则调用tcm.putObject方法，往缓存中放入值。 1234if (list == null) &#123; list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116&#125; tcm的put方法也不是直接操作缓存，只是在把这次的数据和key放入待提交的Map中。 1234@Overridepublic void putObject(Object key, Object object) &#123; entriesToAddOnCommit.put(key, object);&#125; 从以上的代码分析中，我们可以明白，如果不调用commit方法的话，由于TranscationalCache的作用，并不会对二级缓存造成直接的影响。因此我们看看Sqlsession的commit方法中做了什么。代码如下所示： 1234@Overridepublic void commit(boolean force) &#123; try &#123; executor.commit(isCommitOrRollbackRequired(force)); 因为我们使用了CachingExecutor，首先会进入CachingExecutor实现的commit方法。 12345@Overridepublic void commit(boolean required) throws SQLException &#123; delegate.commit(required); tcm.commit();&#125; 会把具体commit的职责委托给包装的Executor。主要是看下tcm.commit()，tcm最终又会调用到TrancationalCache。 1234567public void commit() &#123; if (clearOnCommit) &#123; delegate.clear(); &#125; flushPendingEntries(); reset();&#125; 看到这里的clearOnCommit就想起刚才TrancationalCache的clear方法设置的标志位，真正的清理Cache是放到这里来进行的。具体清理的职责委托给了包装的Cache类。之后进入flushPendingEntries方法。代码如下所示： 123456private void flushPendingEntries() &#123; for (Map.Entry&lt;Object, Object&gt; entry : entriesToAddOnCommit.entrySet()) &#123; delegate.putObject(entry.getKey(), entry.getValue()); &#125; ................&#125; 在flushPendingEntries中，将待提交的Map进行循环处理，委托给包装的Cache类，进行putObject的操作。后续的查询操作会重复执行这套流程。如果是insert|update|delete的话，会统一进入CachingExecutor的update方法，其中调用了这个函数，代码如下所示： 1private void flushCacheIfRequired(MappedStatement ms) 在二级缓存执行流程后就会进入一级缓存的执行流程，因此不再赘述。 总结 MyBatis的二级缓存相对于一级缓存来说，实现了SqlSession之间缓存数据的共享，同时粒度更加的细，能够到namespace级别，通过Cache接口实现类不同的组合，对Cache的可控性也更强。 MyBatis在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。 在分布式环境下，由于默认的MyBatis Cache实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将MyBatis的Cache接口实现，有一定的开发成本，直接使用Redis,Memcached等分布式缓存可能成本更低，安全性也更高。 全文总结本文对介绍了MyBatis一二级缓存的基本概念，并从应用及源码的角度对MyBatis的缓存机制进行了分析。最后对MyBatis缓存机制做了一定的总结，个人建议MyBatis缓存特性在生产环境中进行关闭，单纯作为一个ORM框架使用可能更为合适。]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[volatile实现原理]]></title>
    <url>%2F2018%2F11%2F25%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2Fvolatile%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[实现原理在对被volatile修饰的共享变量进行写操作时，会多出Lock前缀的指令。作用是： 将当前处理器缓存行的数据写回到系统内存； 使得其他CPU里缓存了该内存地址的数据无效； 当处理器发现本地缓存失效后，就会从内存中重新读取该变量数据。 通过这样的机制就使得每个线程都能获得该变量的最新值，从而避免出现数据脏读的现象。 为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，操作完成后不知道何时将数据写回到内存。如果对声明了volatile的变量进行写操作，虚拟机就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。在多处理器下为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议（每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期），当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。 volatile与重排序为了实现volatile的内存语义，JMM会通过插入内存屏障指令来限制特定类型的编译器和处理器重排序 在每个volatile读后面插入一个LoadLoad屏障：禁止下面所有的普通读操作和上面的volatile读重排序 在每个volatile读后面插入一个LoadStore屏障：禁止下面所有的普通写操作和上面的volatile读重排序 在每个volatile写前面插入一个StoreStore屏障：禁止上面的普通写和下面的volatile写重排序 在每个volatile写后面插入一个StoreLoad屏障：防止上面的volatile写与下面可能有的volatile读/写重排序]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap的工作原理及代码实现]]></title>
    <url>%2F2018%2F11%2F12%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FConcurrentHashMap%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1.8以后的锁的颗粒度，是加在链表头上的，这个是个思路上的突破。 为什么需要ConcurrentHashMapHashtable本身比较低效，因为它的实现基本就是将put、get、size等各种方法加上synchronized。这就导致了所有并发操作都要竞争同一把锁，大大降低了并发操作的效率。HashMap不是线程安全的，并发情况会导致类似CPU占用100%等一些问题。Collections提供的同步包装器只是利用输入Map构造了另一个同步版本，所有操作虽然不再声明成为synchronized方法，但是还是利用了“this”作为互斥的mutex，没有真正意义上的改进！所以Hashtable或者同步包装版本，都只是适合在非高度并发的场景下。 ConcurrentHashMap分析早期ConcurrentHashMap，其实现是基于分段锁，也就是将内部进行分段（Segment），里面则是HashEntry的数组，和HashMap类似，哈希相同的条目也是以链表形式存放。HashEntry内部使用volatile的value字段来保证可见性，也利用了不可变对象的机制以改进利用Unsafe提供的底层能力，比如volatile access，去直接完成部分操作，以最优化性能，毕竟Unsafe中的很多操作都是JVM intrinsic优化过的。核心是利用分段设计，在进行并发操作的时候，只需要锁定相应段，这样就有效避免了类似Hashtable整体同步的问题，大大提高了性能。对于put操作，首先是通过二次哈希避免哈希冲突，然后以Unsafe调用方式，直接获取相应的Segment，然后进行线程安全的put操作：所以，从上面的源码清晰的看出，在进行并发写操作时：ConcurrentHashMap会获取冲入锁，以保证数据一致性，Segment本身就是基于ReentrantLock的扩展实现，所以，在并发修改期间，相应Segment是被锁定的。如果不进行同步，简单的计算所有Segment的总值，可能会因为并发put，导致结果不准确，但是直接锁定所有Segment进行计算，就会变得非常昂贵。其实，分离锁也限制了Map的初始化等操作。所以，ConcurrentHashMap的实现是通过重试机制（RETRIES_BEFORE_LOCK，指定重试次数2），来试图获得可靠值。如果没有监控到发生变化（通过对比Segment.modCount），就直接返回，否则获取锁进行操作。下面我来对比一下，在Java 8中ConcurrentHashMap数据存储利用volatile来保证可见性。使用CAS等操作，在特定场景进行无锁并发操作。使用Unsafe、LongAdder之类底层手段，进行极端情况的优化。先看看现在的数据存储内部实现，我们可以发现Key是fnal的，因为在生命周期中，一个条目的Key发生变化是不可能的；与此同时val，则声明为volatile，以保证可见性。直接看并发的put是如何实现的。fnal V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException();int hash = spread(key.hashCode());int binCount = 0;for (Node[] tab = table;;) {Node f; int n, i, fh; K fk; V fv;if (tab == null || (n = tab.length) == 0)tab = initTable();else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) {// 利用CAS去进行无锁线程安全操作，如果bin是空的if (casTabAt(tab, i, null, new Node(hash, key, value)))break;}else if ((fh = f.hash) == MOVED)tab = helpTransfer(tab, f);else if (onlyIfAbsent // 不加锁，进行检查&amp;&amp; fh == hash&amp;&amp; ((fk = f.key) == key || (fk != null &amp;&amp; key.equals(fk)))&amp;&amp; (fv = f.val) != null)return fv;else {V oldVal = null;synchronized (f) {// 细粒度的同步修改操作…}}// Bin超过阈值，进行树化if (binCount != 0) {if (binCount &gt;= TREEIFY_THRESHOLD)treeifyBin(tab, i);if (oldVal != null)return oldVal;break;}}}addCount(1L, binCount);return null;}初始化操作实现在initTable里面，这是一个典型的CAS使用场景，利用volatile的sizeCtl作为互斥手段：如果发现竞争性的初始化，就spin在那里，等待条件恢复；否则利用CAS设置排他标志。如果成功则进行初始化；否则重试。请参考下面代码：private fnal Node[] initTable() {Node[] tab; int sc;while ((tab = table) == null || tab.length == 0) {// 如果发现冲突，进行spin等待if ((sc = sizeCtl) &lt; 0)Thread.yield();// CAS成功返回true，则进入真正的初始化逻辑else if (U.compareAndSetInt(this, SIZECTL, sc, -1)) {try {if ((tab = table) == null || tab.length == 0) {int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY;@SuppressWarnings(“unchecked”)Node[] nt = (Node[])new Node[n];table = tab = nt;sc = n - (n &gt;&gt;&gt; 2);}} fnally {sizeCtl = sc;}break;}}return tab;}当bin为空时，同样是没有必要锁定，也是以CAS操作去放置。你有没有注意到，在同步逻辑上，它使用的是synchronized，而不是通常建议的ReentrantLock之类，这是为什么呢？现代JDK中，synchronized已经被不断优化，可以不再过分极客时间担心性能差异，另外，相比于ReentrantLock，它可以减少内存消耗，这是个非常大的优势。与此同时，更多细节实现通过使用Unsafe进行了优化，例如tabAt就是直接利用getObjectAcquire，避免间接调用的开销。satic fnal Node tabAt(Node[] tab, int i) {return (Node)U.getObjectAcquire(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);}再看看，现在是如何实现size操作的。阅读代码你会发现，真正的逻辑是在sumCount方法中， 那么sumCount做了什么呢？fnal long sumCount() {CounterCell[] as = counterCells; CounterCell a;long sum = baseCount;if (as != null) {for (int i = 0; i &lt; as.length; ++i) {if ((a = as[i]) != null)sum += a.value;}}return sum;}我们发现，虽然思路仍然和以前类似，都是分而治之的进行计数，然后求和处理，但实现却基于一个奇怪的CounterCell。 难道它的数值，就更加准确吗？数据一致性是怎么保证的？satic fnal class CounterCell {volatile long value;CounterCell(long x) { value = x; }}其实，对于CounterCell的操作，是基于java.util.concurrent.atomic.LongAdder进行的，是一种JVM利用空间换取更高效率的方法，利用了Striped64内部的复杂逻辑。这个东西非常小众，大多数情况下，建议还是使用AtomicLong，足以满足绝大部分应用的性能需求。今天我从线程安全问题开始，概念性的总结了基本容器工具，分析了早期同步容器的问题，进而分析了Java 7和Java 8中ConcurrentHashMap是如何设计实现的，希望ConcurrentHashMap的并发技巧对你在日常开发可以有所帮助。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>ConcurrentHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker与Java]]></title>
    <url>%2F2018%2F11%2F12%2FDocker%2FDocker%E4%B8%8EJava%2F</url>
    <content type="text"><![CDATA[对于Java来说，Docker毕竟是一个较新的环境，例如，其内存、CPU等资源限制是通过CGroup（Control Group）实现的，早期的JDK版本（8u131之前）并不能识别这些限制，进而会导致一些基础问题：如果未配置合适的JVM堆和元数据区、直接内存等参数，Java就有可能试图使用超过容器限制的内存，最终被容器OOM kill，或者自身发生OOM。错误判断了可获取的CPU资源，例如，Docker限制了CPU的核数，JVM就可能设置不合适的GC并行线程数等。从应用打包、发布等角度出发，JDK自身就比较大，生成的镜像就更为臃肿，当我们的镜像非常多的时候，镜像的存储等开销就比较明显了。如果考虑到微服务、Serverless等新的架构和场景，Java自身的大小、内存占用、启动速度，都存在一定局限性，因为Java早期的优化大多是针对长时间运行的大型服务器端应用。Java在容器环境的局限性来源，Docker到底有什么特别？虽然看起来Docker之类容器和虚拟机非常相似，例如，它也有自己的shell，能独立安装软件包，运行时与其他容器互不干扰。但是，如果深入分析你会发现，Docker并不是一种完全的虚拟化技术，而更是一种轻量级的隔离技术。 从技术角度，基于namespace，Docker为每个容器提供了单独的命名空间，对网络、PID、用户、IPC通信、文件系统挂载点等实现了隔离。对于CPU、内存、磁盘IO等计算资源，则是通过CGroup进行管理。Docker仅在类似Linux内核之上实现了有限的隔离和虚拟化，并不是像传统虚拟化软件那样，独立运行一个新的操作系统。如果是虚拟化的操作系统，不管是Java还是其他程序，只要调用的是同一个系统API，都可以透明地获取所需的信息，基本不需要额外的兼容性改变。容器虽然省略了虚拟操作系统的开销，实现了轻量级的目标，但也带来了额外复杂性，它限制对于应用不是透明的，需要用户理解Docker的新行为。第一，容器环境对于计算资源的管理方式是全新的，CGroup作为相对比较新的技术，历史版本的Java显然并不能自然地理解相应的资源限制。第二，namespace对于容器内的应用细节增加了一些微妙的差异，比如jcmd、jstack等工具会依赖于“/proc//”下面提供的部分信息，但是Docker的设计改变了这部分信息的原有结构，我们需要对原有工具进行修改以适应这种变化。从JVM运行机制的角度，为什么这些“沟通障碍”会导致OOM等问题呢？你可以思考一下，这个问题实际是反映了JVM如何根据系统资源（内存、CPU等）情况，在启动时设置默认参数。这就是所谓的Ergonomics机制，例如：JVM会大概根据检测到的内存大小，设置最初启动时的堆大小为系统内存的1/64；并将堆最大值，设置为系统内存的1/4。而JVM检测到系统的CPU核数，则直接影响到了Parallel GC的并行线程数目和JIT complier线程数目，甚至是我们应用中ForkJoinPool等机制的并行等级。这些默认参数，是根据通用场景选择的初始值。但是由于容器环境的差异，Java的判断很可能是基于错误信息而做出的。这就类似，我以为我住的是整栋别墅，实际上却只有一个房间是给我住的。更加严重的是，JVM的一些原有诊断或备用机制也会受到影响。为保证服务的可用性，一种常见的选择是依赖“-XX:OnOutOfMemoryError”功能，通过调用处理脚本的形式来做一些补救措施，比如自动重启服务等。但是，这种机制是基于fork实现的，当Java进程已经过度提交内存时，fork新的进程往往已经不可能正常运行了。根据前面的总结，似乎问题非常棘手，那我们在实践中，如何解决这些问题呢？首先，如果你能够升级到最新的JDK版本，这个问题就迎刃而解了。针对这种情况，JDK 9中引入了一些实验性的参数，以方便Docker和Java“沟通”，例如针对内存限制，可以使用下面的参数设置：-XX:+UnlockExperimentalVMOptions-XX:+UseCGroupMemoryLimitForHeap注意，这两个参数是顺序敏感的，并且只支持Linux环境。而对于CPU核心数限定，Java已经被修正为可以正确理解“–cpuset-cpus”等设置，无需单独设置参数。如果你可以切换到JDK 10或者更新的版本，问题就更加简单了。Java对容器（Docker）的支持已经比较完善，默认就会自适应各种资源限制和实现差异。前面提到的实验性参数“UseCGroupMemoryLimitForHeap”已经被标记为废弃。与此同时，新增了参数用以明确指定CPU核心的数目。-XX:ActiveProcessorCount=N如果实践中发现有问题，也可以使用“-XX:-UseContainerSupport”，关闭Java的容器支持特性，这可以作为一种防御性机制，避免新特性破坏原有基础功能。当然，也欢迎你向OpenJDK社区反馈问题。幸运的是，JDK 9中的实验性改进已经被移植到Oracle JDK 8u131之中，你可以直接下载相应镜像，并配置“UseCGroupMemoryLimitForHeap”，后续很有可能还会进一步将JDK 10中相关的增强，应用到JDK 8最新的更新中。但是，如果我暂时只能使用老版本的JDK怎么办？我这里有几个建议：明确设置堆、元数据区等内存区域大小，保证Java进程的总大小可控。例如，我们可能在环境中，这样限制容器内存：$ docker run -it –rm –name yourcontainer -p 8080:8080 -m 800M repo/your-java-container:openjdk极客时间那么，就可以额外配置下面的环境变量，直接指定JVM堆大小。-e JAVA_OPTIONS=’-Xmx300m’明确配置GC和JIT并行线程数目，以避免二者占用过多计算资源。-XX:ParallelGCThreads-XX:CICompilerCount除了我前面介绍的OOM等问题，在很多场景中还发现Java在Docker环境中，似乎会意外使用Swap。具体原因待查，但很有可能也是因为Ergonomics机制失效导致的，我建议配置下面参数，明确告知JVM系统内存限额。-XX:MaxRAM=cat /sys/fs/cgroup/memory/memory.limit_in_bytes也可以指定Docker运行参数，例如：–memory-swappiness=0这是受操作系统Swappiness机制影响，当内存消耗达到一定门限，操作系统会试图将不活跃的进程换出（Swap out），上面的参数有显式关闭Swap的作用。所以可以看到，Java在Docker中的使用，从操作系统、内核到JVM自身机制，需要综合运用我们所掌握的知识。回顾我在专栏第25讲JVM内存区域的介绍，JVM内存消耗远不止包括堆，很多时候仅仅设置Xmx是不够的，MaxRAM也有助于JVM合理分配其他内存区域。如果应用需要设置更多Java启动参数，但又不确定什么数值合理，可以试试一些社区提供的工具，但要注意通用工具的局限性。更进一步来说，对于容器镜像大小的问题，如果你使用的是JDK 9以后的版本，完全可以使用jlink工具定制最小依赖的Java运行环境，将JDK裁剪为几十M的大小，这样运行起来并不困难。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式ID]]></title>
    <url>%2F2018%2F11%2F12%2F%E5%88%86%E5%B8%83%E5%BC%8F%2F%E5%88%86%E5%B8%83%E5%BC%8FID%2F</url>
    <content type="text"><![CDATA[雪花算法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Snowflake &#123; public static final int NODE_SHIFT = 10; public static final int SEQ_SHIFT = 12; public static final short MAX_NODE = 1024; public static final short MAX_SEQUENCE = 4096; private short sequence; private long referenceTime; private int node; /** * A snowflake is designed to operate as a singleton instance within the context of a node. * If you deploy different nodes, supplying a unique node id will guarantee the uniqueness * of ids generated concurrently on different nodes. * * @param node This is an id you use to differentiate different nodes. */ public Snowflake(int node) &#123; if (node &lt; 0 || node &gt; MAX_NODE) &#123; throw new IllegalArgumentException(String.format(&quot;node must be between %s and %s&quot;, 0, MAX_NODE)); &#125; this.node = node; &#125; /** * Generates a k-ordered unique 64-bit integer. Subsequent invocations of this method will produce * increasing integer values. * * @return The next 64-bit integer. */ public long next() &#123; long currentTime = System.currentTimeMillis(); long counter; synchronized(this) &#123; if (currentTime &lt; referenceTime) &#123; throw new RuntimeException(String.format(&quot;Last referenceTime %s is after reference time %s&quot;, referenceTime, currentTime)); &#125; else if (currentTime &gt; referenceTime) &#123; this.sequence = 0; &#125; else &#123; if (this.sequence &lt; Snowflake.MAX_SEQUENCE) &#123; this.sequence++; &#125; else &#123; throw new RuntimeException(&quot;Sequence exhausted at &quot; + this.sequence); &#125; &#125; counter = this.sequence; referenceTime = currentTime; &#125; return currentTime &lt;&lt; NODE_SHIFT &lt;&lt; SEQ_SHIFT | node &lt;&lt; SEQ_SHIFT | counter; &#125;&#125; 123int node = 1;Snowflake s = new Snowflake(node);long id = s.next();]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式ID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES基本用法]]></title>
    <url>%2F2018%2F11%2F12%2FElasticsearch%2FES%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[document 数据 需要有唯一key index 表 具有相同字段的docment组成 node es实例 cluster 集群 _source 原始json数据 restful api 倒排索引 单词词典 记录所有单词 倒排列表 在文档中的位置 偏移量offset 分词器character filters 去p标签 字符替换 tokenizer 切分 成单词 token filters 转小写 删除the of 的 这 那 这种没实际意义的词 123456POST _analyze&#123; "analyzer":"standard", "filter": ["lowercase"], "text":"HeLLo wORld!"&#125; result 123456789101112131415161718&#123; "tokens": [ &#123; "token": "hello", "start_offset": 0, "end_offset": 5, "type": "&lt;ALPHANUM&gt;", "position": 0 &#125;, &#123; "token": "world", "start_offset": 6, "end_offset": 11, "type": "&lt;ALPHANUM&gt;", "position": 1 &#125; ]&#125; 自带分词器 standard 默认分词器 simple 非字母字符切分（空格 -之类的） 小写处理 whitespace 空格切分 stop 删除语气词 the of keyword 不做分词 pattern 正则表达式自定义 默认\w+,非字符做分割 language 不同语言的分词 中文分词 IK jieba 基于自然语言 模型 算法 hanpl thulac 自定义分词123456789101112131415161718192021PUT test_index&#123; "settings": &#123; "analysis": &#123; "analyzer": &#123; "my_analyzer":&#123; "type":"custom", "tokenizer":"standard", "char_filter": ["html_strip"], "filter":["lowercase"] &#125; &#125; &#125; &#125;&#125;POST /test_index/_analyze&#123; "analyzer":"my_analyzer", "text":"HeLLo wORld!"&#125; 自定义mapping 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556PUT my_index&#123; "mappings": &#123; "doc":&#123; "dynamic":false, "properties": &#123; "title":&#123; "type": "text" &#125;, "name":&#123; "type": "keyword" &#125;, "age":&#123; "type": "integer" &#125; &#125; &#125; &#125;&#125;PUT my_index/doc/1&#123; "name":"mason", "age":1, "title":"mason good", "money":3123&#125;_all 所有字段匹配GET my_index/_search&#123; "query": &#123; "match": &#123; "_all": "mason" &#125; &#125;&#125;"index": false 不会被查询PUT my_index&#123; "mappings": &#123; "doc":&#123; "dynamic":false, "properties": &#123; "title":&#123; "type": "text", "index": false &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ribbon]]></title>
    <url>%2F2018%2F11%2F12%2FSpringCloud%2FRibbon%2F</url>
    <content type="text"><![CDATA[ribbon是一个基于http和tcp的客户端负载均衡工具 eureka强调了cap中的ap zookeeper强调 cp]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hystrix]]></title>
    <url>%2F2018%2F11%2F12%2FSpringCloud%2FHystrix%2F</url>
    <content type="text"><![CDATA[因为网络问题或者服务自身问题出现调用故障或延迟，会直接导致调用方的堆外服务也出现延迟，请求不断鞥家，调用方等待响应形成任务挤压，导致自身服务瘫痪。为了解决连锁故障，要使用断路器服务保护机制]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Eureka]]></title>
    <url>%2F2018%2F11%2F12%2FSpringCloud%2FEureka%2F</url>
    <content type="text"><![CDATA[当ribbon与eureka联合使用时ribbon的服务配置清单ribbonserverlist会被discoveryEnabledNIWSServerlist重写，扩展成从eureka注册中心获取服务端列表 服务治理机制 注册中心互相注册组成高可用集群 失效剔除 eureka会创建一个定时任务，默认每隔一段时间当清单中超时的没有预约的服务剔除 自我保护 服务再注册是会维护一个心跳连接，eureka会将实例注册信息保存起来,让这些实例不过期，客户端必须要有容错机制，如请求重试，断路器等等 服务提供者 启动的时候会发送rest请求将自己注册到eureka上，同时带上自身服务的一些元数据，eureka接受到rest请求后，将元数据信息存储再一个双层map，第一层key是服务名，第二层key是具体服务的实例名 服务同步 服务提供者注册到不同的服务中心，提供者注册时会将请求装发到集群中其他的注册中心，实现同步 服务续约 注册完服务后，服务提供者会维护一个心跳，防止eureka将服务实例从服务列表中剔除 服务获取 启动消费者时，发送一个rest请求给注册中心，来获取上面注册的服务清单，eureka每隔一段时间返回给客户端 服务调用 消费端获取清单后，通过服务名可以获取具体提供服务的实例名和元数据信息，ribbon会默认采用轮询的方式进行调用]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Scala集合]]></title>
    <url>%2F2018%2F11%2F11%2FScala%2FScala%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[数组new Array Array() 12345678910111213var arr=new Array[String](5)arr(1)=&quot;hello&quot;println(arr.length)var arr2=Array(&quot;hello&quot;,&quot;world&quot;)println(arr2.mkString(&quot;-&quot;))var d=scala.collection.mutable.ArrayBuffer[String]() // 可变的d += &quot;a&quot;d += (&quot;1&quot;,&quot;2&quot;,&quot;3&quot;)d ++= arr // 加其他数组d.insert(5,&quot;mason&quot;) // 插入指定位置println(d)removetrimend listNil=空的list 12345678var l=List(1,23,4,5,67)println(l.head) //第一个元素println(l.tail) //除了第一个var l2=1::Nil //1是头 Nil是尾var lb=ListBuffer[Int]() // 可变listlb+=(1,23,5,4)println(lb) set map option some none tuple]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对比HashMap、Hashtable、LinkedHashMap、TreeMap]]></title>
    <url>%2F2018%2F11%2F11%2FJava%2F%E5%AF%B9%E6%AF%94HashMap%E3%80%81Hashtable%E3%80%81LinkedHashMap%E3%80%81TreeMap%2F</url>
    <content type="text"><![CDATA[HashTable中的key、value都不能为null；HashMap中的key、value可以为null，很显然只能有一个key为null的键值对，但是允许有多个值为null的键值对；TreeMap中当未实现Comparator 接口时，key 不可以为null；当实现 Comparator 接口时，若未对null情况进行判断，则key不可以为null，反之亦然。TreeMap是利用红黑树来实现的（树中的每个节点的值，都会大于或等于它的左子树种的所有节点的值，并且小于或等于它的右子树中的所有节点的值），实现了SortMap接口，能够对保存的记录根据键进行排序。所以一般需要排序的情况下是选择TreeMap来进行，默认为升序排序方式（深度优先搜索），可自定义实现Comparator接口实现排序方式。 HashMap基于哈希思想实现对数据的读写。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。当两个不同的键对象的hashcode相同时，它们会储存在同一个bucket位置的链表中，可通过键对象的equals()方法用来找到键值对。如果链表大小超过阈值（TREEIFY_THRESHOLD, 8），链表就会被改造为树形结构 扩容时：Hashtable将容量变为原来的2倍加1；HashMap扩容将容量变为原来的2倍newCap = oldCap &lt;&lt; 1 HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致，在并发环境可能出现无限循环占用CPU、size不准确等诡异的问题。可以使用如下方法进行同步 可以用 Collections的synchronizedMap方法 使用ConcurrentHashMap类，相较于HashTable锁住的是对象整体， ConcurrentHashMap基于lock实现锁分段技术。首先将Map存放的数据分成一段一段的存储方式，然后给每一段数据分配一把锁，当一个线程占用锁访问其中一个段的数据时，其他段的数据也能被其他线程访问。ConcurrentHashMap不仅保证了多线程运行环境下的数据访问安全性，而且性能上有长足的提升。 LinkedHashMap通常提供的是遍历顺序符合插入顺序，它的实现是通过为条目（键值对）维护一个双向链表。注意，通过特定构造函数，我们可以创建反映访问顺序的实例，所谓的put、get、compute等，都算作“访问”。这种行为适用于一些特定应用场景，例如，我们构建一个空间占用敏感的资源池，希望可以自动将最不常被访问的对象释放掉，这就可以利用LinkedHashMap提供的机制来实现 1234567891011121314151617181920212223242526272829303132import java.util.LinkedHashMap;import java.util.Map;public class LinkedHashMapSample &#123; public satic void main(String[] args) &#123; LinkedHashMap&lt;String, String&gt; accessOrderedMap = new LinkedHashMap&lt;&gt;(16, 0.75F, true)&#123; @Override protected boolean removeEldesEntry(Map.Entry&lt;String, String&gt; eldes) &#123; // 实现自定义删除策略，否则行为就和普遍Map没有区别 return size() &gt; 3; &#125; &#125;; accessOrderedMap.put(&quot;Project1&quot;, &quot;Valhalla&quot;); accessOrderedMap.put(&quot;Project2&quot;, &quot;Panama&quot;); accessOrderedMap.put(&quot;Project3&quot;, &quot;Loom&quot;); accessOrderedMap.forEach( (k,v) -&gt; &#123; Sysem.out.println(k +&quot;:&quot; + v); &#125;); // 模拟访问 accessOrderedMap.get(&quot;Project2&quot;); accessOrderedMap.get(&quot;Project2&quot;); accessOrderedMap.get(&quot;Project3&quot;); Sysem.out.println(&quot;Iterate over should be not afected:&quot;); accessOrderedMap.forEach( (k,v) -&gt; &#123; Sysem.out.println(k +&quot;:&quot; + v); &#125;); // 触发删除 accessOrderedMap.put(&quot;Project4&quot;, &quot;Mission Control&quot;); Sysem.out.println(&quot;Oldes entry should be removed:&quot;); accessOrderedMap.forEach( (k,v) -&gt; &#123;// 遍历顺序不变 Sysem.out.println(k +&quot;:&quot; + v); &#125;); &#125;&#125; 依据resize源码，不考虑极端情况（容量理论最大极限由MAXIMUM_CAPACITY指定，数值为 1&lt;&lt;30，也就是2的30次方），我们可以归纳为：门限值等于（负载因子）x（容量），如果构建HashMap的时候没有指定它们，那么就是依据相应的默认常量值。门限通常是以倍数进行调整 （newThr = oldThr &lt;&lt; 1），我前面提到，根据putVal中的逻辑，当元素个数超过门限大小时，则调整Map大小。扩容后，需要将老的数组中的元素重新放置到新的数组，这是扩容的一个主要开销来源。3.容量、负载因子和树化前面我们快速梳理了一下HashMap从创建到放入键值对的相关逻辑，现在思考一下，为什么我们需要在乎容量和负载因子呢？这是因为容量和负载系数决定了可用的桶的数量，空桶太多会浪费空间，如果使用的太满则会严重影响操作的性能。极端情况下，假设只有一个桶，那么它就退化成了链表，完全不能提供所谓常数时间存的性能。既然容量和负载因子这么重要，我们在实践中应该如何选择呢？如果能够知道HashMap要存取的键值对数量，可以考虑预先设置合适的容量大小。具体数值我们可以根据扩容发生的条件来做简单预估，根据前面的代码分析，我们知道它需要符合计算条件：极客时间负载因子 * 容量 &gt; 元素数量所以，预先设置的容量需要满足，大于“预估元素数量/负载因子”，同时它是2的幂数，结论已经非常清晰了。而对于负载因子，我建议：如果没有特别需求，不要轻易进行更改，因为JDK自身的默认负载因子是非常符合通用场景的需求的。如果确实需要调整，建议不要设置超过0.75的数值，因为会显著增加冲突，降低HashMap的性能。如果使用太小的负载因子，按照上面的公式，预设容量值也进行调整，否则可能会导致更加频繁的扩容，增加无谓的开销，本身访问性能也会受影响。我们前面提到了树化改造，对应逻辑主要在putVal和treeifyBin中。fnal void treeifyBin(Node[] tab, int hash) {int n, index; Node e;if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY)resize();else if ((e = tab[index = (n - 1) &amp; hash]) != null) {//树化改造逻辑}}上面是精简过的treeifyBin示意，综合这两个方法，树化改造的逻辑就非常清晰了，可以理解为，当bin的数量大于TREEIFY_THRESHOLD时：如果容量小于MIN_TREEIFY_CAPACITY，只会进行简单的扩容。如果容量大于MIN_TREEIFY_CAPACITY ，则会进行树化改造。那么，为什么HashMap要树化呢？本质上这是个安全问题。因为在元素放置过程中，如果一个对象哈希冲突，都被放置到同一个桶里，则会形成一个链表，我们知道链表查询是线性的，会严重影响存取的性能。而在现实世界，构造哈希冲突的数据并不是非常复杂的事情，恶意代码就可以利用这些数据大量与服务器端交互，导致服务器端CPU大量占用，这就构成了哈希碰撞拒绝服务攻击，国内一线互联网公司就发生过类似攻击事件]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对比Vector、ArrayList、LinkedList有何区别]]></title>
    <url>%2F2018%2F11%2F11%2FJava%2F%E5%AF%B9%E6%AF%94Vector%E3%80%81ArrayList%E3%80%81LinkedList%E6%9C%89%E4%BD%95%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[ArrayListArrayList 是基于数组实现的，实现了 List、RandomAccess 接口。可以插入空数据，也支持随机访问。 数组的默认大小为 10。 1private static final int DEFAULT_CAPACITY = 10; ArrayList相当于动态数据，其中最重要的两个属性分别是:elementData 数组，以及 size 大小。在调用 add() 方法的时候： 12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 首先进行扩容校验。 将插入的值放到尾部，并将 size + 1 。 如果是调用 add(index,e) 在指定位置添加的话： 12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 也是首先扩容校验。 接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。 其实扩容最终调用的代码: 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，也就是旧容量的 1.5 倍。扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，减少扩容操作的次数。更要减少在指定位置插入数据的操作。 删除元素时需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看出 ArrayList 删除元素的代价是非常高的。 12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 序列化由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。 1transient Object[] elementData; 因此 ArrayList 自定义了序列化与反序列化： 123456789101112131415161718192021222324252627282930313233343536373839404142 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125;// modCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125; &#125; 当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。 VectorVector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。 以下是 add() 方法： 123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 以及指定位置插入数据: 1234567891011121314public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt; " + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125; LinkedList基于双向链表实现，使用 Node 存储链表节点信息。 12345private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;&#125; 每个链表存储了 first 和 last 指针： 12transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; 总结 Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。 ArrayList 基于动态数组实现，LinkedList 基于双向链表实现； ArrayList 支持随机访问，LinkedList 不支持； LinkedList 在任意位置添加删除元素更快。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>ArrayList</tag>
        <tag>Vector</tag>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对比synchronized、ReentrantLock]]></title>
    <url>%2F2018%2F11%2F11%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2F%E5%AF%B9%E6%AF%94synchronized%E3%80%81ReentrantLock%2F</url>
    <content type="text"><![CDATA[ReentrantLock使用起来比较灵活，但是必须手动获取与释放锁，而synchronized不需要手动释放和开启锁ReentrantLock只适用于代码块锁，而synchronized可用于修饰方法、代码块ReentrantLock的优势体现在：具备尝试非阻塞地获取锁的特性：当前线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取并持有锁能被中断地获取锁的特性：与synchronized不同，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放超时获取锁的特性：在指定的时间范围内获取锁；如果截止时间到了仍然无法获取锁，则返回3 注意事项在使用ReentrantLock类的时，一定要注意三点：在fnally中释放锁，目的是保证在获取锁之后，最终能够被释放不要将获取锁的过程写在try块内，因为如果在获取锁时发生了异常，异常抛出的同时，也会导致锁无故被释放。ReentrantLock提供了一个newCondition的方法，以便用户在同一锁的情况下可以根据不同的情况执行等待或唤醒的动作 锁降级确实是会发生的，当JVM进入安全点（SafePoint）的时候，会检查是否有闲置的Monitor，然后试图进行降级。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>ReentrantLock</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK中的设计模式]]></title>
    <url>%2F2018%2F11%2F06%2FJava%2FJDK%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[结构型模式： 适配器：用来把一个接口转化成另一个接口，如 java.util.Arrays#asList()。java.io.InputStreamReader(InputStream)java.io.OutputStreamWriter(OutputStream) 桥接模式：这个模式将抽象和抽象操作的实现进行了解耦，这样使得抽象和实现可以独立地变化，如JDBC； 组合模式：使得客户端看来单个对象和对象的组合是同等的。换句话说，某个类型的方法同时也接受自身类型作为参数，如 Map.putAll，List.addAll、Set.addAll。 装饰者模式：动态的给一个对象附加额外的功能，这也是子类的一种替代方式，如 java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap。java.io.BufferedInputStream(InputStream)java.io.DataInputStream(InputStream)java.io.BufferedOutputStream(OutputStream) 享元模式：使用缓存来加速大量小对象的访问时间，如 valueOf(int)。 代理模式：代理模式是用一个简单的对象来代替一个复杂的或者创建耗时的对象，如 java.lang.reflect.Proxy 门面模式：给一组组件，接口，抽象，或者子系统提供一个简单的接口。 java.lang.Class javax.faces.webapp.FacesServlet 创建模式: 抽象工厂模式：抽象工厂模式提供了一个协议来生成一系列的相关或者独立的对象，而不用指定具体对象的类型，如 java.util.Calendar#getInstance()。java.util.Arrays#asList()java.util.ResourceBundle#getBundle()java.sql.DriverManager#getConnection()java.sql.Connection#createStatement()java.sql.Statement#executeQuery() 建造模式(Builder)：定义了一个新的类来构建另一个类的实例，以简化复杂对象的创建，如：java.lang.StringBuilder#append()。 工厂方法：就是 一个返* 回具体对象的方法，而不是多个，如 java.lang.Object#toString()、java.lang.Class#newInstance()。java.lang.Class#newInstance()java.lang.Class#forName() 原型模式：使得类的实例能够生成自身的拷贝、如：java.lang.Object#clone()。java.lang.Object#clone()java.lang.Cloneable 单例模式用来确保类只有一个实例。 java.lang.Runtime#getRuntime() 行为模式： 责任链模式：通过把请求从一个对象传递到链条中下一个对象的方式，直到请求被处理完毕，以实现对象间的解耦。如 javax.servlet.Filter#doFilter()。 java.util.logging.Logger#log() 命令模式：将操作封装到对象内，以便存储，传递和返回，如：java.lang.Runnable。 解释器模式：定义了一个语言的语法，然后解析相应语法的语句，如，java.text.Format，java.text.Normalizer。 迭代器模式：提供一个一致的方法来顺序访问集合中的对象，如 java.util.Iterator。 中介者模式：通过使用一个中间对象来进行消息分发以及减少类之间的直接依赖，java.lang.reflect.Method#invoke()。java.util.Timerjava.util.concurrent.Executor#execute()java.util.concurrent.ExecutorService#submit()java.lang.reflect.Method#invoke() 空对象模式：如 java.util.Collections#emptyList()。 观察者模式：它使得一个对象可以灵活的将消息发送给感兴趣的对象，如 java.util.EventListener。javax.servlet.http.HttpSessionBindingListenerjavax.servlet.http.HttpSessionAttributeListenerjavax.faces.event.PhaseListener 模板方法模式：让子类可以重写方法的一部分，而不是整个重写，如 java.util.Collections#sort()。 备忘录模式生成对象状态的一个快照，以便对象可以恢复原始状态而不用暴露自身的内容。Date对象通过自身内部的一个long值来实现备忘录模式。 java.util.Date java.io.Serializable 状态模式通过改变对象内部的状态，使得你可以在运行时动态改变一个对象的行为。 java.util.Iterator javax.faces.lifecycle.LifeCycle#execute() 策略模式使用这个模式来将一组算法封装成一系列对象。通过传递这些对象可以灵活的改变程序的功能。 java.util.Comparator#compare() javax.servlet.http.HttpServlet javax.servlet.Filter#doFilter() 模板方法模式让子类可以重写方法的一部分，而不是整个重写，你可以控制子类需要重写那些操作。 java.util.Collections#sort() java.io.InputStream#skip() java.io.InputStream#read() java.util.AbstractList#indexOf() 访问者模式提供一个方便的可维护的方式来操作一组对象。它使得你在不改变操作的对象前提下，可以修改或者扩展对象的行为。 javax.lang.model.element.Element and javax.lang.model.element.ElementVisitor javax.lang.model.type.TypeMirror and javax.lang.model.type.TypeVisitor]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mybatis中的设计模式]]></title>
    <url>%2F2018%2F11%2F06%2FMyBatis%2FMybatis%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、装饰模式 最明显的就是cache包下面的实现 Cahe、LoggingCache、LruCache、TransactionalCahe…等 以LoggingCache为例，UML图 ​ Cache cache = new LoggingCache(new PerpetualCache(“cacheid”));一层层包装就使得默认cache实现PerpetualCache具有附加的功能，比如上面的log功能。二、建造者模式 BaseBuilder、XMLMapperBuilder 三、工厂方法 SqlSessionFactory 四、适配器模式 Log、LogFactory 五、模板方法 BaseExecutor、SimpleExecutor 六、动态代理 Plugin 见7图 7、责任链模式 Interceptor、InterceptorChain]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring中的设计模式]]></title>
    <url>%2F2018%2F11%2F06%2FSpring%2FSpring%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1. 简单工厂 又叫做静态工厂方法（StaticFactory Method）模式，但不属于23种GOF设计模式之一。 简单工厂模式的实质是由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。 Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得Bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。 2. 工厂方法（Factory Method） 定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使一个类的实例化延迟到其子类。 Spring中的FactoryBean就是典型的工厂方法模式。如下图： 3. 单例（Singleton） 保证一个类仅有一个实例，并提供一个访问它的全局访问点。 Spring中的单例模式完成了后半句话，即提供了全局的访问点BeanFactory。但没有从构造器级别去控制单例，这是因为Spring管理的是是任意的Java对象。 4. 适配器（Adapter） 将一个类的接口转换成客户希望的另外一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。 Spring中在对于AOP的处理中有Adapter模式的例子，见如下图： 由于Advisor链需要的是MethodInterceptor（拦截器）对象，所以每一个Advisor中的Advice都要适配成对应的MethodInterceptor对象。 5.包装器（Decorator） 动态地给一个对象添加一些额外的职责。就增加功能来说，Decorator模式相比生成子类更为灵活。 Spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。基本上都是动态地给一个对象添加一些额外的职责。 6. 代理（Proxy） 为其他对象提供一种代理以控制对这个对象的访问。 从结构上来看和Decorator模式类似，但Proxy是控制，更像是一种对功能的限制，而Decorator是增加职责。 Spring的Proxy模式在aop中有体现，比如JdkDynamicAopProxy和Cglib2AopProxy。 7.观察者（Observer） 定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。 Spring中Observer模式常用的地方是listener的实现。如ApplicationListener。 8. 策略（Strategy） 定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。 Spring中在实例化对象的时候用到Strategy模式，见如下图： 在SimpleInstantiationStrategy中有如下代码说明了策略模式的使用情况： 9.模板方法（Template Method） 定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。Template Method使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 Template Method模式一般是需要继承的。这里想要探讨另一种对Template Method的理解。Spring中的JdbcTemplate，在用这个类时并不想去继承这个类，因为这个类的方法太多，但是我们还是想用到JdbcTemplate已有的稳定的、公用的数据库连接，那么我们怎么办呢？我们可以把变化的东西抽出来作为一个参数传入JdbcTemplate的方法中。但是变化的东西是一段代码，而且这段代码会用到JdbcTemplate中的变量。怎么办？那我们就用回调对象吧。在这个回调对象中定义一个操纵JdbcTemplate中变量的方法，我们去实现这个方法，就把变化的东西集中到这里了。然后我们再传入这个回调对象到JdbcTemplate，从而完成了调用。这可能是Template Method不需要继承的另一种实现方式吧。 以下是一个具体的例子： JdbcTemplate中的execute方法： JdbcTemplate执行execute方法： 知识只有共享才能传播，才能推崇出新的知识，才能学到更多，这里写的每一篇文字/博客，基本都是从网上查询了一下资料然后记录下来，也有些是原滋原味搬了过来，也有时加了一些自己的想法]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Nio]]></title>
    <url>%2F2018%2F11%2F06%2FNetty%2FNIO%2F</url>
    <content type="text"><![CDATA[初识NIO： ​ 在 JDK 1. 4 中 新 加入 了 NIO( New Input/ Output) 类, 引入了一种基于通道和缓冲区的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆的 DirectByteBuffer 对象作为这块内存的引用进行操作，避免了在 Java 堆和 Native 堆中来回复制数据。 ​ NIO 是一种同步非阻塞的 IO 模型。同步是指线程不断轮询 IO 事件是否就绪，非阻塞是指线程在等待 IO 的时候，可以同时做其他任务。同步的核心就是 Selector，Selector 代替了线程本身轮询 IO 事件，避免了阻塞同时减少了不必要的线程消耗；非阻塞的核心就是通道和缓冲区，当 IO 事件就绪时，可以通过写道缓冲区，保证 IO 的成功，而无需线程阻塞式地等待。 Buffer： ​ 为什么说NIO是基于缓冲区的IO方式呢？因为，当一个链接建立完成后，IO的数据未必会马上到达，为了当数据到达时能够正确完成IO操作，在BIO（阻塞IO）中，等待IO的线程必须被阻塞，以全天候地执行IO操作。为了解决这种IO方式低效的问题，引入了缓冲区的概念，当数据到达时，可以预先被写入缓冲区，再由缓冲区交给线程，因此线程无需阻塞地等待IO。 通道： ​ 当执行：SocketChannel.write(Buffer)，便将一个 buffer 写到了一个通道中。如果说缓冲区还好理解，通道相对来说就更加抽象。网上博客难免有写不严谨的地方，容易使初学者感到难以理解。 ​ 引用 Java NIO 中权威的说法：通道是 I/O 传输发生时通过的入口，而缓冲区是这些数 据传输的来源或目标。对于离开缓冲区的传输，您想传递出去的数据被置于一个缓冲区，被传送到通道。对于传回缓冲区的传输，一个通道将数据放置在您所提供的缓冲区中。 ​ 例如 有一个服务器通道 ServerSocketChannel serverChannel，一个客户端通道 SocketChannel clientChannel；服务器缓冲区：serverBuffer，客户端缓冲区：clientBuffer。 ​ 当服务器想向客户端发送数据时，需要调用：clientChannel.write(serverBuffer)。当客户端要读时，调用 clientChannel.read(clientBuffer) ​ 当客户端想向服务器发送数据时，需要调用：serverChannel.write(clientBuffer)。当服务器要读时，调用 serverChannel.read(serverBuffer) ​ 这样，通道和缓冲区的关系似乎更好理解了。在实践中，未必会出现这种双向连接的蠢事（然而这确实存在的，后面的内容还会涉及），但是可以理解为在NIO中：如果想将Data发到目标端，则需要将存储该Data的Buffer，写入到目标端的Channel中，然后再从Channel中读取数据到目标端的Buffer中。 Selector： ​ 通道和缓冲区的机制，使得线程无需阻塞地等待IO事件的就绪，但是总是要有人来监管这些IO事件。这个工作就交给了selector来完成，这就是所谓的同步。 ​ Selector允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用Selector就会很方便。 ​ 要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪，这就是所说的轮询。一旦这个方法返回，线程就可以处理这些事件。 ​ Selector中注册的感兴趣事件有： OP_ACCEPT OP_CONNECT OP_READ OP_WRITE 优化： ​ 一种优化方式是：将Selector进一步分解为Reactor，将不同的感兴趣事件分开，每一个Reactor只负责一种感兴趣的事件。这样做的好处是：1、分离阻塞级别，减少了轮询的时间；2、线程无需遍历set以找到自己感兴趣的事件，因为得到的set中仅包含自己感兴趣的事件。 NIO和epoll： ​ epoll是Linux内核的IO模型。我想一定有人想问，AIO听起来比NIO更加高大上，为什么不使用AIO？AIO其实也有应用，但是有一个问题就是，Linux是不支持AIO的，因此基于AIO的程序运行在Linux上的效率相比NIO反而更低。而Linux是最主要的服务器OS，因此相比AIO，目前NIO的应用更加广泛。 ​ 说到这里，可能你已经明白了，epoll一定和NIO有着很深的因缘。没错，如果仔细研究epoll的技术内幕，你会发现它确实和NIO非常相似，都是基于“通道”和缓冲区的，也有selector，只是在epoll中，通道实际上是操作系统的“管道”。和NIO不同的是，NIO中，解放了线程，但是需要由selector阻塞式地轮询IO事件的就绪；而epoll中，IO事件就绪后，会自动发送消息，通知selector：“我已经就绪了。”可以认为，Linux的epoll是一种效率更高的NIO。 NIO轶事： ​ 一篇有意思的博客，讲的 Java selector.open() 的时候，会创建一个自己和自己的链接（windows上是tcp，linux上是通道） ​ 这么做的原因：可以从 Apache Mina 中窥探。在 Mina 中，有如下机制： Mina框架会创建一个Work对象的线程。 Work对象的线程的run()方法会从一个队列中拿出一堆Channel，然后使用Selector.select()方法来侦听是否有数据可以读/写。 最关键的是，在select的时候，如果队列有新的Channel加入，那么，Selector.select()会被唤醒，然后重新select最新的Channel集合。 要唤醒select方法，只需要调用Selector的wakeup()方法。 ​ 而一个阻塞在select上的线程有以下三种方式可以被唤醒： 有数据可读/写，或出现异常。 阻塞时间到，即time out。 收到一个non-block的信号。可由kill或pthread_kill发出。 ​ 首先 2 可以排除，而第三种方式，只在linux中存在。因此，Java NIO为什么要创建一个自己和自己的链接：就是如果想要唤醒select，只需要朝着自己的这个loopback连接发点数据过去，于是，就可以唤醒阻塞在select上的线程了。 《Java NIO编写Socket服务器的一个例子》 1.Java NIO概览首先，熟悉一下NIO的主要组成部分：Bufer，高效的数据容器，除了布尔类型，所有原始数据类型都有相应的Bufer实现。Channel，类似在Linux之类操作系统上看到的文件描述符，是NIO中被用来支持批量式IO操作的一种抽象。File或者Socket，通常被认为是比较高层次的抽象，而Channel则是更加操作系统底层的一种抽象，这也使得NIO得以充分利用现代操作系统底层机制，获得特定场景的性能优化，例如，DMA（Direct Memory Access）等。不同层次的抽象是相互关联的，我们可以通过Socket获取Channel，反之亦然。Selector，是NIO实现多路复用的基础，它提供了一种高效的机制，可以检测到注册在Selector上的多个Channel中，是否有Channel处于就绪状态，进而实现了单线程对多Channel的高效管理。Selector同样是基于底层操作系统机制，不同模式、不同版本都存在区别，例如，在最新的代码库里，相关实现如下：Linux上依赖于epoll（http://hg.openjdk.java.net/jdk/jdk/fle/d8327f838b88/src/java.base/linux/classes/sun/nio/ch/EPollSelectorImpl.java）。Windows上NIO2（AIO）模式则是依赖于iocp（http://hg.openjdk.java.net/jdk/jdk/fle/d8327f838b88/src/java.base/windows/classes/sun/nio/ch/Iocp.java）。Chartset，提供Unicode字符串定义，NIO也提供了相应的编解码器等，例如，通过下面的方式进行字符串到ByteBufer的转换：Charset.defaultCharset().encode(“Hello world!”));2.NIO能解决什么问题？下面我通过一个典型场景，来分析为什么需要NIO，为什么需要多路复用。设想，我们需要实现一个服务器应用，只简单要求能够同时服务多个客户端请求即可。使用java.io和java.net中的同步、阻塞式API，可以简单实现。public class DemoServer extends Thread {private ServerSocket serverSocket;public int getPort() {return serverSocket.getLocalPort();}public void run() {try {极客时间serverSocket = new ServerSocket(0);while (true) {Socket socket = serverSocket.accept();RequesHandler requesHandler = new RequesHandler(socket);requesHandler.sart();}} catch (IOException e) {e.printStackTrace();} fnally {if (serverSocket != null) {try {serverSocket.close();} catch (IOException e) {e.printStackTrace();};}}}public satic void main(String[] args) throws IOException {DemoServer server = new DemoServer();server.sart();try (Socket client = new Socket(InetAddress.getLocalHos(), server.getPort())) {BuferedReader buferedReader = new BuferedReader(new InputStreamReader(client.getInputStream()));buferedReader.lines().forEach(s -&gt; Sysem.out.println(s));}}}// 简化实现，不做读取，直接发送字符串class RequesHandler extends Thread {private Socket socket;RequesHandler(Socket socket) {this.socket = socket;}@Overridepublic void run() {try (PrintWriter out = new PrintWriter(socket.getOutputStream());) {out.println(“Hello world!”);out.fush();} catch (Exception e) {e.printStackTrace();}}}其实现要点是：服务器端启动ServerSocket，端口0表示自动绑定一个空闲端口。调用accept方法，阻塞等待客户端连接。利用Socket模拟了一个简单的客户端，只进行连接、读取、打印。当连接建立后，启动一个单独线程负责回复客户端请求。这样，一个简单的Socket服务器就被实现出来了。思考一下，这个解决方案在扩展性方面，可能存在什么潜在问题呢？大家知道Java语言目前的线程实现是比较重量级的，启动或者销毁一个线程是有明显开销的，每个线程都有单独的线程栈等结构，需要占用非常明显的内存，所以，每一个Client启动一个线程似乎都有些浪费。那么，稍微修正一下这个问题，我们引入线程池机制来避免浪费。serverSocket = new ServerSocket(0);executor = Executors.newFixedThreadPool(8);while (true) {Socket socket = serverSocket.accept();RequesHandler requesHandler = new RequesHandler(socket);executor.execute(requesHandler);}这样做似乎好了很多，通过一个固定大小的线程池，来负责管理工作线程，避免频繁创建、销毁线程的开销，这是我们构建并发服务的典型方式。这种工作方式，可以参考下图来理解。极客时间如果连接数并不是非常多，只有最多几百个连接的普通应用，这种模式往往可以工作的很好。但是，如果连接数量急剧上升，这种实现方式就无法很好地工作了，因为线程上下文切换开销会在高并发时变得很明显，这是同步阻塞方式的低扩展性劣势。NIO引入的多路复用机制，提供了另外一种思路，请参考我下面提供的新的版本。public class NIOServer extends Thread {public void run() {try (Selector selector = Selector.open();ServerSocketChannel serverSocket = ServerSocketChannel.open();) {// 创建Selector和ChannelserverSocket.bind(new InetSocketAddress(InetAddress.getLocalHos(), 8888));serverSocket.confgureBlocking(false);// 注册到Selector，并说明关注点serverSocket.regiser(selector, SelectionKey.OP_ACCEPT);while (true) {selector.select();// 阻塞等待就绪的Channel，这是关键点之一Set selectedKeys = selector.selectedKeys();Iterator iter = selectedKeys.iterator();while (iter.hasNext()) {SelectionKey key = iter.next();// 生产系统中一般会额外进行就绪状态检查sayHelloWorld((ServerSocketChannel) key.channel());iter.remove();}}} catch (IOException e) {e.printStackTrace();}}private void sayHelloWorld(ServerSocketChannel server) throws IOException {try (SocketChannel client = server.accept();) { client.write(Charset.defaultCharset().encode(“Hello world!”));}}// 省略了与前面类似的main}这个非常精简的样例掀开了NIO多路复用的面纱，我们可以分析下主要步骤和元素：首先，通过Selector.open()创建一个Selector，作为类似调度员的角色。然后，创建一个ServerSocketChannel，并且向Selector注册，通过指定SelectionKey.OP_ACCEPT，告诉调度员，它关注的是新的连接请求。注意，为什么我们要明确配置非阻塞模式呢？这是因为阻塞模式下，注册操作是不允许的，会抛出IllegalBlockingModeException异常。Selector阻塞在select操作，当有Channel发生接入请求，就会被唤醒。在sayHelloWorld方法中，通过SocketChannel和Bufer进行数据操作，在本例中是发送了一段字符串。可以看到，在前面两个样例中，IO都是同步阻塞模式，所以需要多线程以实现多任务处理。而NIO则是利用了单线程轮询事件的机制，通过高效地定位就绪的Channel，来决定做什么，仅仅select阶段是阻塞的，可以有效避免大量客户端连接时，频繁线程切换带来的问题，应用的扩展能力有了非常大的提高。下面这张图对这种实现思路进行了形象地说明。极客时间在Java 7引入的NIO 2中，又增添了一种额外的异步IO模式，利用事件和回调，处理Accept、Read等操作。 AIO实现看起来是类似这样子：AsynchronousServerSocketChannel serverSock = AsynchronousServerSocketChannel.open().bind(sockAddr);serverSock.accept(serverSock, new CompletionHandler&lt;&gt;() { //为异步操作指定CompletionHandler回调函数@Overridepublic void completed(AsynchronousSocketChannel sockChannel, AsynchronousServerSocketChannel serverSock) {serverSock.accept(serverSock, this);// 另外一个 write（sock，CompletionHandler{}）sayHelloWorld(sockChannel, Charset.defaultCharset().encode(“Hello World!”));}// 省略其他路径处理方法…});鉴于其编程要素（如Future、CompletionHandler等），我们还没有进行准备工作，为避免理解困难，我会在专栏后面相关概念补充后的再进行介绍，尤其是Reactor、Proactor模式等方面将在Netty主题一起分析，这里我先进行概念性的对比：基本抽象很相似，AsynchronousServerSocketChannel对应于上面例子中的ServerSocketChannel；AsynchronousSocketChannel则对应SocketChannel。业务逻辑的关键在于，通过指定CompletionHandler回调接口，在accept/read/write等关键节点，通过事件机制调用，这是非常不同的一种编程思路。今天我初步对Java提供的IO机制进行了介绍，概要地分析了传统同步IO和NIO的主要组成，并根据典型场景，通过不同的IO模式进行了实现与拆解。专栏下一讲，我还将继续分析Java IO的主题。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zero-Copy]]></title>
    <url>%2F2018%2F11%2F06%2FNetty%2FZero-Copy%2F</url>
    <content type="text"><![CDATA[多个物理分离的buffer，通过逻辑上合并成为一个，从而避免了数据在内存之间的拷贝。 所谓的 Zero-copy, 就是在操作数据时, 不需要将数据 buffer 从一个内存区域拷贝到另一个内存区域. 因为少了一次内存的拷贝, 因此 CPU 的效率就得到的提升. 在 OS 层面上的 Zero-copy 通常指避免在 用户态(User-space) 与 内核态(Kernel-space) 之间来回拷贝数据. 例如 Linux 提供的 mmap 系统调用, 它可以将一段用户空间内存映射到内核空间, 当映射成功后, 用户对这段内存区域的修改可以直接反映到内核空间; 同样地, 内核空间对这段区域的修改也直接反映用户空间. 正因为有这样的映射关系, 我们就不需要在 用户态(User-space) 与 内核态(Kernel-space) 之间拷贝数据, 提高了数据传输的效率. 而需要注意的是, Netty 中的 Zero-copy 与上面我们所提到到 OS 层面上的 Zero-copy 不太一样, Netty的 Zero-coyp完全是在用户态(Java 层面)的, 它的 Zero-copy 的更多的是偏向于 优化数据操作 这样的概念. Netty 的 Zero-copy 体现在如下几个个方面: Netty 提供了 CompositeByteBuf 类, 它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf, 避免了各个 ByteBuf 之间的拷贝. 通过 wrap 操作, 我们可以将 byte[] 数组、ByteBuf、ByteBuffer等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作. ByteBuf 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf, 避免了内存的拷贝. 通过 FileRegion 包装的FileChannel.tranferTo 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel, 避免了传统通过循环 write 方式导致的内存拷贝问题. 下面我们就来简单了解一下这几种常见的零拷贝操作. 通过 CompositeByteBuf 实现零拷贝假设我们有一份协议数据, 它由头部和消息体组成, 而头部和消息体是分别存放在两个 ByteBuf 中的, 即: 12ByteBuf header = ...ByteBuf body = ... 我们在代码处理中, 通常希望将 header 和 body 合并为一个 ByteBuf, 方便处理, 那么通常的做法是: 123ByteBuf allBuf = Unpooled.buffer(header.readableBytes() + body.readableBytes());allBuf.writeBytes(header);allBuf.writeBytes(body); 可以看到, 我们将 header 和 body 都拷贝到了新的 allBuf 中了, 这无形中增加了两次额外的数据拷贝操作了. 那么有没有更加高效优雅的方式实现相同的目的呢? 我们来看一下 CompositeByteBuf 是如何实现这样的需求的吧. 12345ByteBuf header = ...ByteBuf body = ...CompositeByteBuf compositeByteBuf = Unpooled.compositeBuffer();compositeByteBuf.addComponents(true, header, body); 上面代码中, 我们定义了一个 CompositeByteBuf 对象, 然后调用 123public CompositeByteBuf addComponents(boolean increaseWriterIndex, ByteBuf... buffers) &#123;...&#125; 方法将 header 与 body 合并为一个逻辑上的 ByteBuf, 即: 1不过需要注意的是, 虽然看起来 CompositeByteBuf 是由两个 ByteBuf 组合而成的, 不过在 CompositeByteBuf 内部, 这两个 ByteBuf 都是单独存在的, CompositeByteBuf 只是逻辑上是一个整体. 上面 CompositeByteBuf 代码还以一个地方值得注意的是, 我们调用 addComponents(boolean increaseWriterIndex, ByteBuf... buffers) 来添加两个 ByteBuf, 其中第一个参数是 true, 表示当添加新的 ByteBuf 时, 自动递增 CompositeByteBuf 的 writeIndex.如果我们调用的是 1compositeByteBuf.addComponents(header, body); 那么其实 compositeByteBuf 的 writeIndex 仍然是0, 因此此时我们就不可能从 compositeByteBuf 中读取到数据, 这一点希望大家要特别注意. 除了上面直接使用 CompositeByteBuf 类外, 我们还可以使用 Unpooled.wrappedBuffer 方法, 它底层封装了 CompositeByteBuf 操作, 因此使用起来更加方便: 1234ByteBuf header = ...ByteBuf body = ...ByteBuf allByteBuf = Unpooled.wrappedBuffer(header, body); 通过 wrap 操作实现零拷贝例如我们有一个 byte 数组, 我们希望将它转换为一个 ByteBuf 对象, 以便于后续的操作, 那么传统的做法是将此 byte 数组拷贝到 ByteBuf 中, 即: 123byte[] bytes = ...ByteBuf byteBuf = Unpooled.buffer();byteBuf.writeBytes(bytes); 显然这样的方式也是有一个额外的拷贝操作的, 我们可以使用 Unpooled 的相关方法, 包装这个 byte 数组, 生成一个新的 ByteBuf 实例, 而不需要进行拷贝操作. 上面的代码可以改为: 12byte[] bytes = ...ByteBuf byteBuf = Unpooled.wrappedBuffer(bytes); 可以看到, 我们通过 Unpooled.wrappedBuffer 方法来将 bytes 包装成为一个 UnpooledHeapByteBuf 对象, 而在包装的过程中, 是不会有拷贝操作的. 即最后我们生成的生成的 ByteBuf 对象是和 bytes 数组共用了同一个存储空间, 对 bytes 的修改也会反映到 ByteBuf 对象中. Unpooled 工具类还提供了很多重载的 wrappedBuffer 方法: 12345678910111213public static ByteBuf wrappedBuffer(byte[] array)public static ByteBuf wrappedBuffer(byte[] array, int offset, int length)public static ByteBuf wrappedBuffer(ByteBuffer buffer)public static ByteBuf wrappedBuffer(ByteBuf buffer)public static ByteBuf wrappedBuffer(byte[]... arrays)public static ByteBuf wrappedBuffer(ByteBuf... buffers)public static ByteBuf wrappedBuffer(ByteBuffer... buffers)public static ByteBuf wrappedBuffer(int maxNumComponents, byte[]... arrays)public static ByteBuf wrappedBuffer(int maxNumComponents, ByteBuf... buffers)public static ByteBuf wrappedBuffer(int maxNumComponents, ByteBuffer... buffers) 这些方法可以将一个或多个 buffer 包装为一个 ByteBuf 对象, 从而避免了拷贝操作. 通过 slice 操作实现零拷贝slice 操作和 wrap 操作刚好相反, Unpooled.wrappedBuffer 可以将多个 ByteBuf 合并为一个, 而 slice 操作可以将一个 ByteBuf 切片 为多个共享一个存储区域的 ByteBuf 对象.ByteBuf 提供了两个 slice 操作方法: 12public ByteBuf slice();public ByteBuf slice(int index, int length); 不带参数的 slice 方法等同于 buf.slice(buf.readerIndex(), buf.readableBytes()) 调用, 即返回 buf 中可读部分的切片. 而 slice(int index, int length) 方法相对就比较灵活了, 我们可以设置不同的参数来获取到 buf 的不同区域的切片. 下面的例子展示了 ByteBuf.slice 方法的简单用法: 123ByteBuf byteBuf = ...ByteBuf header = byteBuf.slice(0, 5);ByteBuf body = byteBuf.slice(5, 10); 用 slice 方法产生 header 和 body 的过程是没有拷贝操作的, header 和 body 对象在内部其实是共享了 byteBuf 存储空间的不同部分而已. 即: 通过 FileRegion 实现零拷贝Netty 中使用 FileRegion 实现文件传输的零拷贝, 不过在底层 FileRegion 是依赖于 Java NIO FileChannel.transfer 的零拷贝功能. 首先我们从最基础的 Java IO 开始吧. 假设我们希望实现一个文件拷贝的功能, 那么使用传统的方式, 我们有如下实现: 123456789101112public static void copyFile(String srcFile, String destFile) throws Exception &#123; byte[] temp = new byte[1024]; FileInputStream in = new FileInputStream(srcFile); FileOutputStream out = new FileOutputStream(destFile); int length; while ((length = in.read(temp)) != -1) &#123; out.write(temp, 0, length); &#125; in.close(); out.close();&#125; 上面是一个典型的读写二进制文件的代码实现了. 不用我说, 大家肯定都知道, 上面的代码中不断中源文件中读取定长数据到 temp 数组中, 然后再将 temp 中的内容写入目的文件, 这样的拷贝操作对于小文件倒是没有太大的影响, 但是如果我们需要拷贝大文件时, 频繁的内存拷贝操作就消耗大量的系统资源了.下面我们来看一下使用 Java NIO 的 FileChannel 是如何实现零拷贝的: 123456789101112public static void copyFileWithFileChannel(String srcFileName, String destFileName) throws Exception &#123; RandomAccessFile srcFile = new RandomAccessFile(srcFileName, &quot;r&quot;); FileChannel srcFileChannel = srcFile.getChannel(); RandomAccessFile destFile = new RandomAccessFile(destFileName, &quot;rw&quot;); FileChannel destFileChannel = destFile.getChannel(); long position = 0; long count = srcFileChannel.size(); srcFileChannel.transferTo(position, count, destFileChannel);&#125; 可以看到, 使用了 FileChannel 后, 我们就可以直接将源文件的内容直接拷贝(transferTo) 到目的文件中, 而不需要额外借助一个临时 buffer, 避免了不必要的内存操作. 有了上面的一些理论知识, 我们来看一下在 Netty 中是怎么使用 FileRegion 来实现零拷贝传输一个文件的: 1234567891011121314151617181920212223242526272829@Overridepublic void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; RandomAccessFile raf = null; long length = -1; try &#123; // 1. 通过 RandomAccessFile 打开一个文件. raf = new RandomAccessFile(msg, &quot;r&quot;); length = raf.length(); &#125; catch (Exception e) &#123; ctx.writeAndFlush(&quot;ERR: &quot; + e.getClass().getSimpleName() + &quot;: &quot; + e.getMessage() + &apos;\n&apos;); return; &#125; finally &#123; if (length &lt; 0 &amp;&amp; raf != null) &#123; raf.close(); &#125; &#125; ctx.write(&quot;OK: &quot; + raf.length() + &apos;\n&apos;); if (ctx.pipeline().get(SslHandler.class) == null) &#123; // SSL not enabled - can use zero-copy file transfer. // 2. 调用 raf.getChannel() 获取一个 FileChannel. // 3. 将 FileChannel 封装成一个 DefaultFileRegion ctx.write(new DefaultFileRegion(raf.getChannel(), 0, length)); &#125; else &#123; // SSL enabled - cannot use zero-copy file transfer. ctx.write(new ChunkedFile(raf)); &#125; ctx.writeAndFlush(&quot;\n&quot;);&#125; 上面的代码是 Netty 的一个例子, 其源码在 netty/example/src/main/java/io/netty/example/file/FileServerHandler.java可以看到, 第一步是通过 RandomAccessFile 打开一个文件, 然后 Netty 使用了 DefaultFileRegion 来封装一个 FileChannel 即: 1new DefaultFileRegion(raf.getChannel(), 0, length) 当有了 FileRegion 后, 我们就可以直接通过它将文件的内容直接写入 Channel 中, 而不需要像传统的做法: 拷贝文件内容到临时 buffer, 然后再将 buffer 写入 Channel. 通过这样的零拷贝操作, 无疑对传输大文件很有帮助.]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch入门]]></title>
    <url>%2F2018%2F11%2F05%2FElasticsearch%2FElasticsearch%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Elasticsearch是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单 elasticsearch与solr比较solr优点1、Solr有一个更大、更成熟的用户、开发和贡献者社区。2、支持添加多种格式的索引，如：HTML、PDF、微软 Office 系列软件格式以及 JSON、XML、CSV 等纯文本格式。3、Solr比较成熟、稳定。4、不考虑建索引的同时进行搜索，速度更快。缺点建立索引时，搜索效率下降，实时索引搜索效率不高。 Elasticsearch优点1、Elasticsearch是分布式的。不需要其他组件，分发是实时的，被叫做”Push replication”。2、Elasticsearch 完全支持 Apache Lucene 的接近实时的搜索。3、处理多租户（multitenancy）不需要特殊配置，而Solr则需要更多的高级设置。4、Elasticsearch 采用 Gateway 的概念，使得完备份更加简单。5、各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。缺点1、还不够自动，不适合当前新的Index Warmup API (参考：http://zhaoyanblog.com/archives/764.html) 总结：1、当单纯的对已有数据进行搜索时，Solr更快。2、当实时建立索引时, Solr会产生io阻塞，查询性能较差, Elasticsearch具有明显的优势。3、随着数据量的增加，Solr的搜索效率会变得更低，而Elasticsearch却没有明显的变化。4、Solr的架构不适合实时搜索的应用。5、Solr 支持更多格式的数据，而 Elasticsearch 仅支持json文件格式6、Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch7、Solr 是传统搜索应用的有力解决方案，但 Elasticsearch 更适用于新兴的实时搜索应用 资料 《Elasticsearch学习，请先看这一篇！》 《Elasticsearch索引原理》 死磕es es elasticsearch资源汇总 awesome-elasticsearch 插件汇总 ES权威指南中文 ES权威指南英文 ES安全search-guard elasticsearch相关介绍 solr和elasticsearch对比 将 ELASTICSEARCH 写入速度优化到极限 剖析Elasticsearch集群系列 英文原文 剖析Elasticsearch集群系列第一篇 存储模型和读写操作 剖析Elasticsearch集群系列第二篇 分布式的三个C、translog和Lucene段 剖析Elasticsearch集群系列第三篇 近实时搜索、深层分页问题和搜索相关性权衡之道 nested 和parent使用介绍 企业使用案例 使用Akka、Kafka和ElasticSearch等构建分析引擎 用Elasticsearch构建电商搜索平台，一个极有代表性的基础技术架构和算法实践案例 用Elasticsearch+Redis构建投诉监控系统，看Airbnb如何保证用户持续增长 基于Elasticsearch构建千亿流量日志搜索平台实战 Elasticsearch作为时间序列数据库 索引原理 ES原理 docvalues 介绍 ElasticSearch存储文件解析 问题解决 如何防止elasticsearch的脑裂问题 jar conflic 通过maven-shade-plugin 解决Elasticsearch与hbase的jar包冲突问题]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty入门]]></title>
    <url>%2F2018%2F11%2F05%2FNetty%2FNetty%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Netty《Netty原理剖析》 Reactor 模式介绍。 Netty 是 Reactor 模式的一种实现。 为什么选择 Netty 说说业务中，Netty 的使用场景 原生的 NIO 在 JDK 1.7 版本存在 epoll bug 什么是TCP 粘包/拆包 TCP粘包/拆包的解决办法 Netty 线程模型 说说 Netty 的零拷贝 Netty 内部执行流程 Netty 重连实现 深入一些的话，就会问NIO的原理、NIO属于哪种IO模型、NIO的三大组成等等，这有些难，当时我也是研究了很久才搞懂NIO。提一句，NIO并不是严格意义上的非阻塞IO而应该属于多路复用IO，面试回答的时候要注意这个细节，讲到NIO会阻塞在Selector的select方法上会增加面试官对你的好感。 如果用过Netty，可能会问一些Netty的东西，毕竟这个框架基本属于当前最好的NIO框架了（Mina其实也不错，不过总体来说还是比不上Netty的），大多数互联网公司也都在用Netty。 nio netty 监听端口 NioEventLoop 新连接socket Channel 接收数据io byte ByteBuf 业务逻辑 ChannelHandler nioeventloop run()相当于while(true) 单独从性能角度，Netty在基础的NIO等类库之上进行了很多改进，例如：更加优雅的Reactor模式实现、灵活的线程模型、利用EventLoop等创新性的机制，可以非常高效地管理成百上千的Channel。充分利用了Java的Zero-Copy机制，并且从多种角度，“斤斤计较”般的降低内存分配和回收的开销。例如，使用池化的Direct Bufer等技术，在提高IO性能的同时，减少了对象的创建和销毁；利用反射等技术直接操纵SelectionKey，使用数组而不是Java容器等。使用更多本地代码。例如，直接利用JNI调用Open SSL等方式，获得比Java内建SSL引擎更好的性能。在通信协议、序列化等其他角度的优化。总的来说，Netty并没有Java核心类库那些强烈的通用性、跨平台等各种负担，针对性能等特定目标以及Linux等特定环境，采取了一些极致的优化手段。考点分析这是一个比较开放的问题，我给出的回答是个概要性的举例说明。面试官很可能利用这种开放问题作为引子，针对你回答的一个或者多个点，深入探讨你在不同层次上的理解程度。在面试准备中，兼顾整体性的同时，不要忘记选定个别重点进行深入理解掌握，最好是进行源码层面的深入阅读和实验。如果你希望了解更多从性能角度Netty在编码层面的手段，可以参考Norman在Devoxx上的分享，其中的很多技巧对于实现极致性能的API有一定借鉴意义，但在一般的业务开发中要谨慎采用。虽然提到Netty，人们会自然地想到高性能，但是Netty本身的优势不仅仅只有这一个方面，下面我会侧重两个方面：对Netty进行整体介绍，帮你了解其基本组成。从一个简单的例子开始，对比在第11讲中基于IO、NIO等标准API的实例，分析它的技术要点，给你提供一个进一步深入学习的思路。知识扩展首先，我们从整体了解一下Netty。按照官方定义，它是一个异步的、基于事件Client/Server的网络框架，目标是提供一种简单、快速构建网络应用的方式，同时保证高吞吐量、低延时、高可靠性。从设计思路和目的上，Netty与Java自身的NIO框架相比有哪些不同呢？我们知道Java的标准类库，由于其基础性、通用性的定位，往往过于关注技术模型上的抽象，而不是从一线应用开发者的角度去思考。我曾提到过，引入并发包的一个重要原因就是，应用开发者使用Thread API比较痛苦，需要操心的不仅仅是业务逻辑，而且还要自己负责将其映射到Thread模型上。Java NIO的设计也有类似的特点，开发者需要深入掌握线程、IO、网络等相关概念，学习路径很长，很容易导致代码复杂、晦涩，即使是有经验的工程师，也难以快速地写出高可靠性的实现。Netty的设计强调了 “Separation Of Concerns”，通过精巧设计的事件机制，将业务逻辑和无关技术逻辑进行隔离，并通过各种方便的抽象，一定程度上填补了了基础平台和业务开发之间的鸿沟，更有利于在应用开发中普及业界的最佳实践。另外，Netty &gt; java.nio + java. net！从API能力范围来看，Netty完全是Java NIO框架的一个大大的超集，你可以参考Netty官方的模块划分。第38讲 | 对比Java标准NIO类库，你知道Netty是如何实现更高性能的吗？杨晓峰 00:18 / 09:27极客时间除了核心的事件机制等，Netty还额外提供了很多功能，例如：从网络协议的角度，Netty除了支持传输层的UDP、TCP、SCTP协议，也支持HTTP(s)、WebSocket等多种应用层协议，它并不是单一协议的API。在应用中，需要将数据从Java对象转换成为各种应用协议的数据格式，或者进行反向的转换，Netty为此提供了一系列扩展的编解码框架，与应用开发场景无缝衔接，并且性能良好。它扩展了Java NIO Bufer，提供了自己的ByteBuf实现，并且深度支持Direct Bufer等技术，甚至hack了Java内部对Direct Bufer的分配和销毁等。同时，Netty也提供了更加完善的Scatter/Gather机制实现。可以看到，Netty的能力范围大大超过了Java核心类库中的NIO等API，可以说它是一个从应用视角出发的产物。当然，对于基础API设计，Netty也有自己独到的见解，未来Java NIO API也可能据此进行一定的改进，如果你有兴趣可以参考JDK-8187540。接下来，我们一起来看一个入门的代码实例，看看Netty应用到底是什么样子。与第11讲类似，同样是以简化的Echo Server为例，下图是Netty官方提供的Server部分，完整用例请点击链接。上面的例子，虽然代码很短，但已经足够体现出Netty的几个核心概念，请注意我用红框标记出的部分：ServerBootstrap，服务器端程序的入口，这是Netty为简化网络程序配置和关闭等生命周期管理，所引入的Bootstrapping机制。我们通常要做的创建Channel、绑定端口、注册Handler等，都可以通过这个统一的入口，以Fluent API等形式完成，相对简化了API使用。与之相对应， Bootstrap则是Client端的通常入口。Channel，作为一个基于NIO的扩展框架，Channel和Selector等概念仍然是Netty的基础组件，但是针对应用开发具体需求，提供了相对易用的抽象。EventLoop，这是Netty处理事件的核心机制。例子中使用了EventLoopGroup。我们在NIO中通常要做的几件事情，如注册感兴趣的事件、调度相应的Handler等，都是EventLoop负责。ChannelFuture，这是Netty实现异步IO的基础之一，保证了同一个Channel操作的调用顺序。Netty扩展了Java标准的Future，提供了针对自己场景的特有Future定义。ChannelHandler，这是应用开发者放置业务逻辑的主要地方，也是我上面提到的“Separation Of Concerns”原则的体现。ChannelPipeline，它是ChannelHandler链条的容器，每个Channel在创建后，自动被分配一个ChannelPipeline。在上面的示例中，我们通过ServerBootstrap注册了ChannelInitializer，并且实现了initChannel方法，而在该方法中则承担了向ChannelPipleline安装其他Handler的任务。你可以参考下面的简化示意图，忽略Inbound/OutBound Handler的细节，理解这几个基本单元之间的操作流程和对应关系。极客时间对比Java标准NIO的代码，Netty提供的相对高层次的封装，减少了对Selector等细节的操纵，而EventLoop、Pipeline等机制则简化了编程模型，开发者不用担心并发等问题，在一定程度上简化了应用代码的开发。最难能可贵的是，这一切并没有以可靠性、可扩展性为代价，反而将其大幅度提高。我在专栏周末福利中已经推荐了Norman Maurer等编写的《Netty实战》（Netty In Action），如果你想系统学习Netty，它会是个很好的入门参考。针对Netty的一些实现原理，很可能成为面试中的考点，例如：Reactor模式和Netty线程模型。Pipelining、EventLoop等部分的设计实现细节。Netty的内存管理机制、引用计数等特别手段。有的时候面试官也喜欢对比Java标准NIO API，例如，你是否知道Java NIO早期版本中的Epoll空转问题，以及Netty的解决方式等。对于这些知识点，公开的深入解读已经有很多了，在学习时希望你不要一开始就被复杂的细节弄晕，可以结合实例，逐步、有针对性的进行学习。我的一个建议是，可以试着画出相应的示意图，非常有助于理解并能清晰阐述自己的看法。今天，从Netty性能的问题开始，我概要地介绍了Netty框架，并且以Echo Server为例，对比了Netty和Java NIO在设计上的不同。但这些都仅仅是冰山的一角，全面掌握还需要下非常多的功夫。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos常用软件安装]]></title>
    <url>%2F2018%2F11%2F05%2FLinux%2F%E9%85%8D%E7%BD%AEIP%2F</url>
    <content type="text"><![CDATA[配置IP1vi /etc/sysconfig/network-scripts/ifcfg-ens33 12345BOOTPROTO=&quot;static&quot;IPADDR=&quot;192.168.10.110&quot;NETMASK=&quot;255.255.255.0&quot;GATEWAY=&quot;192.168.10.1&quot;DNS1=&quot;8.8.8.8&quot; 1234567891011service network restartvi /etc/sysconfig/networkNETWORKING=yesHOSTNAME=hadoop1vi /etc/hosts192.168.10.110 hadoop1192.168.10.111 hadoop2192.168.10.112 hadoop3192.168.10.113 hadoop4192.168.10.114 hadoop5192.168.10.115 hadoop6 安装JDK123tar -zxvf jdk-8u181-linux-x64.tar.gzmv jdk1.8.0_181 jdkvi /etc/profile 123export JAVA_HOME=/usr/local/jdkexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tool.jar 1source /etc/profile 安装Perl12345yum install -y gcctar -zxvf perl-5.16.1.tar.gzmv perl-5.16.1 perl./Configure -des -Dprefix=/usr/local/perlmake &amp;&amp; make install 配置ssh免密码通信各自主机 123ssh-keygen -t rsacd /root/.sshcp id_rsa.pub authorized_keys IP:192.168.10.51的主机 1ssh-copy-id -i 192.168.10.52 IP:192.168.10.52的主机 1ssh-copy-id -i 192.168.10.51 安装Nginx12345678910111213141516171819202122mkdir -p /usr/servers/distribution_nginxcd /usr/servers/distribution_nginxyum install -y readline-devel pcre-devel openssl-devel gcctar -xzvf ngx_openresty-1.7.7.2.tar.gzcd ngx_openresty-1.7.7.2cd bundle/LuaJIT-2.1-20150120make clean &amp;&amp; make &amp;&amp; make installln -sf luajit-2.1.0-alpha /usr/local/bin/luajitcd ..wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gztar -xvf 2.3.tar.gzwget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gztar -xvf v0.3.0.tar.gzcd .../configure --prefix=/usr/servers/distribution_nginx --with-http_realip_module --with-pcre --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2make &amp;&amp; make installcd ..cd nginx/sbin/./nginxcd ..cd confvi nginx.conf http{}内添加 123lua_package_path &quot;/usr/servers/distribution_nginx/lualib/?.lua;;&quot;;lua_package_cpath &quot;/usr/servers/distribution_nginx/lualib/?.so;;&quot;;include lua.conf; /usr/servers/distribution_nginx/nginx/conf 路径下 vi lua.conf 12345678server &#123; listen 80; server_name _; location /lua &#123; default_type &apos;text/html&apos;; content_by_lua &apos;ngx.say(&quot;hello world&quot;)&apos;; &#125; &#125; /usr/servers/distribution_nginx/nginx/sbin目录下 1./nginx -s reload iptables -I INPUT -p tcp –dport 80 -j ACCEPT 开放端口 访问 http://192.168.10.51/lua 安装tcl1234tar -xzvf tcl8.6.1-src.tar.gzcd unix./configuremake &amp;&amp; make install 安装Redis123456789tar -zxvf redis-3.2.12.tar.gzcd redis-3.2.12make &amp;&amp; make installcd utilscp redis_init_script /etc/init.dcp redis.conf /etc/redismv redis.conf 6379.confvi 6379.confmv redis_init_script redis_6379 123daemonize yesbind 192.168.10.51dir /var/redis/6379 12345chmod 777 redis_6379./redis_6379 startiptables -I INPUT -p tcp --dport 6379 -j ACCEPTps -ef|grep redisvi redis_6379 开机启动 123# chkconfig: 2345 90 10# description: Redis is a persistent key-value databasechkconfig redis_6379 on slave节点的conf 1slaveof 192.168.10.51 6379 启动 12redis-cli 192.168.10.51info replication 停止firewall 并 禁止firewall开机启动 12systemctl stop firewalld.servicesystemctl disable firewalld.service 安装MySQL12345678wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum install mysql-serverchkconfig mysqld onservice mysqld startmysql -u rootset password for root@localhost=password(&apos;root&apos;);mysql -uroot -proot vsftpd123456789101112131415161718192021222324252627282930yum -y install vsftpdvi /etc/vsftpd/chroot_list 添加ftpuser，保存关闭vi /etc/selinux/config修改为SELINUX=disabled，保存关闭// 关闭防火墙systemctl stop firewalld.servicesystemctl disable firewalld.service// 创建目录cd /cd /ftpfile// 创建一个没登陆权限的用户useradd ftpuser -d /ftpfile/ -s /sbin/nologin//设置权限chown -R ftpuser.ftpuser /ftpfile/// 设置密码passwd ftpuservi /etc/vsftpd/vsftpd.conf新增local_root=/ftpfileanon_root=/ftpfileuser_localtime=yespasv_min_port=61001pasv_max_port=62000修改anonymous_enable=yes // 打开匿名访问打开chroot_local_user=YES和chroot_list_file=/etc/vsftpd/chroot_list的注释 nginx安装12345678yum install -y gccyum -y install pcre-develyum install -y zlib-develtar -zxvf nginx-1.14.0.tar.gz cd nginx-1.14.0./configuremakemake install 平滑重启 kill -HUP pid tail -f file 用于监视File文件增长]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2018%2F11%2F05%2FJava%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[+ 匹配一个或多个 * 匹配0个或多个 ？ 匹配0个或1个 {m} 匹配m个 {m，n} 匹配m-n个 { m，} 匹配m个及以上 . 可以匹配任意单个字符，不能匹配换行符 如果要匹配 . 需要转义 \. [] 匹配一组字符 0-9、a-z 、A-Z定义了一个字符区间，区间使用 ASCII 码来确定 ^ 在 [ ] 中是取非操作 | 或 \b 匹配单词边界，结果不包含 \B 匹配不是单词边界的 \d 等价于 [0-9] \D 等价于 [^0-9] \s 任意空白字符 等价 [\f\n\r\t\v] \S 任意非空白字符 \w 等价于 [a-zA-Z0-9_] \W 等价于 [^a-zA-Z0-9_] （） 子表达式 分组 \r\n windows 文本行结束 \n linux文本行结束 ？非贪婪匹配 ^ 匹配字符串开头 $ 字符串结尾 规则 正则表达式语法 一个或多个汉字 ^[\u0391-\uFFE5]+$ 邮政编码 ^[1-9]\d{5}$ QQ号码 ^[1-9]\d{4,10}$ 邮箱 ^[a-zA-Z_]{1,}[0-9]{0,}@(([a-zA-z0-9]-*){1,}.){1,3}[a-zA-z-]{1,}$ 用户名（字母开头 + 数字/字母/下划线） ^[A-Za-z][A-Za-z1-9_-]+$ 手机号码 ^1[3\ 4\ 5\ 8][0-9]\d{8}$ URL ^((http\ https)://)?([\w-]+.)+[\w-]+(/[\w-./?%&amp;=]*)?$ 18位身份证号 ^(\d{6})(18\ 19\ 20)?(\d{2})([01]\d)([0123]\d)(\d{3})(\d\ X\ x)?$]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[跨域问题如何解决]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%88%86%E5%B8%83%E5%BC%8F%2F%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[跨域是什么跨域问题，是由于JavaScript出于安全方面的考虑，不允许跨域调用其他页面的对象。换句话说，只有JavaScript存在跨域问题。在现在前后端分离，微服务化之后，往往我们就存在许多不同的域名，这种情况下，就存在非常普遍的跨域问题。因此，跨域问题，在日常开发过程中，是一个非常熟悉的名词。 什么情况下会出现跨域 不同的二级域名，存在跨域问题。 不同的协议，存在跨域问题 不同的端口号，存在跨域问题。 解决之道我们是如何去解决跨域问题呢？来吧，我们进入正题。 JSONP（废弃）很早很早之前，我有个项目曾经使用过JSONP处理跨域问题。简单的理解，jsonp是带有回调函数callback的json，它是一个很棒的方案，可用于解决主流浏览器的跨域数据访问的问题。但是，JSONP方案的局限性在于，JSONP只能实现GET请求。随着现在RESTful的兴起，JSONP显得力不从心了。因为，RESTful不仅有GET，还存在POST、PUT、PATCH、DELETE。 CORS1234&lt;dependency&gt; &lt;groupId&gt;com.thetransactioncompany&lt;/groupId&gt; &lt;artifactId&gt;cors-filter&lt;/artifactId&gt;&lt;/dependency&gt; CORS 全称为 Cross Origin Resource Sharing（跨域资源共享）。整个CORS通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的AJAX通信没有差别，代码完全一样。浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，但用户不会有感觉。因此，实现CORS通信的关键是服务端。服务端只需添加相关响应头信息，即可实现客户端发出 AJAX 跨域请求。 值得注意的是，浏览器必须先以 OPTIONS 请求方式发送一个预请求，从而获知服务器端对跨源请求所支持 HTTP 方法。在确认服务器允许该跨源请求的情况下，以实际的 HTTP 请求方法发送那个真正的请求。注意CORS不支持IE8、IE9， 搭建中间转发层跨域问题的核心是什么？不同源访问。是啊，如果我们转换成同源请求，就不存在这个问题啦。通过搭建中间层，当然可以是java，也可以是node.js，通过将服务端的请求进行转发，换句话说，就是dispatcher了一层，那么前端请求的地址，就被转发了，所以很好的解决跨域问题。当然，如果对性能有考量的产品，就需要慎重选择这个方案咯，因为多了一层中间转发，不管是网络开销，还是性能负载都是有一定的影响。 Nginx反向代理首先，产品需要搭建一个中转nginx服务器，用于转发请求。当然，我们都是基于Nginx作为反向代理，所以当然是水到渠成。 那么，Nginx的思路，就是通过Nginx解析URL地址的时候进行判断，将请求转发的具体的服务器上。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot配置]]></title>
    <url>%2F2018%2F11%2F04%2FSpringBoot%2FSpringBoot%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Spring方式@Configuration 声明一个配置类相当于xml @Bean 将方法的返回值加入Bean容器，代理bean标签 @Value属性注入 @PropertySource指定外部属性文件 123456789101112131415161718192021222324@Configuration@PropertySource(&quot;classpath:jdbc.properties&quot;)public class DruidConfig &#123; @Value(&quot;$&#123;jdbc.url&#125;&quot;) String url; @Value(&quot;$&#123;jdbc.driver-class-name&#125;&quot;) String driverClassName; @Value(&quot;$&#123;jdbc.username&#125;&quot;) String username; @Value(&quot;$&#123;jdbc.password&#125;&quot;) String password; @Bean public DataSource dataSource()&#123; DruidDataSource dataSource=new DruidDataSource(); dataSource.setUsername(username); dataSource.setPassword(password); dataSource.setUrl(url); dataSource.setDriverClassName(driverClassName); return dataSource; &#125;&#125; SpringBoot 方式适合多个使用 12345678@ConfigurationProperties(prefix = &quot;jdbc&quot;)@Datapublic class ConfigProperties &#123; String url; String driverclassname; String username; String password;&#125; 123456789101112131415@Configuration@EnableConfigurationProperties(ConfigProperties.class)public class DruidConfig &#123; @Bean public DataSource dataSource(ConfigProperties configProperties)&#123; DruidDataSource dataSource=new DruidDataSource(); dataSource.setUsername(configProperties.getUsername()); dataSource.setPassword(configProperties.getPassword()); dataSource.setUrl(configProperties.getUrl()); dataSource.setDriverClassName(configProperties.getDriverclassname()); return dataSource; &#125;&#125; 适合单个使用 123456789@Configurationpublic class DruidConfig &#123; @Bean @ConfigurationProperties(prefix = &quot;jdbc&quot;) public DataSource dataSource()&#123; return new DruidDataSource(); &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐]]></title>
    <url>%2F2018%2F11%2F04%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[数据收集： etl 提取 转换 加载 数据存储 ： sql nosql 资源管理 批处理 交互式分析 流处理 数据挖掘 ： 数据仓库 olap 商务智能 数据可视化 Grafana mahout flume kafka 推荐角度： 社交推荐 朋友推荐 流行度推荐 排行榜 内容推荐 搜的内容 协同过滤推荐 买这个的其他人还买什么 稀疏矩阵的问题 冷启动的问题 相似度算法： Jaccard coefficient 交集/并集 余弦相似度 cos(a,b) Perarson Correlation 协方差/标准差 0.8-1.0 0.6-0.8 KNN算法 分类算法 找出和目标用户兴趣相似的目标集合 ​ 相似度 得到K个用户 找到集合中用户喜欢的 ​ s(u,k) 使用单一行为隐反馈数据 UserCF算法 冷启动问题 用户冷启动 商品冷启动 系统冷启动 方法： 非个性化推荐 最流行 排名最高 进去强制选几个分类 注册信息 用户画像是高维向量 消费层级 年龄 性别 爱好 购买的商品等 明确问题 数据预处理 特征工程 模型算法 产出]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[知识点]]></title>
    <url>%2F2018%2F11%2F04%2FJava%2F%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[精度问题：2.0-1.1打印结果0.89999 因为二进制无法准确描述1/10 &lt;9位用int ； &lt;18可以用long ；&gt;18必须使用bigdecimal。BigDecimal：add() 加； subtract() 减； multiply() 乘； divide() 除；构造是传String，否则还有精度问题 finally带return会覆盖返回值，否则finally里的赋值改变不了返回值 接口中的变量都是public static final，方法都是public 在泛型代码内部，无法获得泛型参数类型的信息，Java泛型是使用擦除来实现的，List\和List\在运行时实际上是相同的类型 public 所有类可见； protected 本包和子类可见； 默认 本包可见； private 本类可见 switch可以传 int short byte char string for循环顺序：int i=0 { 判断i&lt;10 打印 i++}，int i=0执行一次，{}内重复 123for (int i = 0; i &lt; 10; i++) &#123; System.out.println(i);&#125; int 4字节； short 2字节； long 8字节； byte 1字节； float 4字节； double 8字节； char 2字节 HashMap中如果有一个key对应的value不再使用，无法从映射表中删除，可以使用WeakHashMap使用弱引用保存键，将引用存放到另一个对象中，如果对象只能由WeakReference引用，垃圾回收机制能启动作用 覆盖equals时总要覆盖hashcode String.join(“–”,”a”,”b”);连接字符串 Runtime.getRuntime().availableProcessors(); 得到处理器数量 获得当前时间下一天 123Calendar c=Calendar.getInstance();c.add(Calendar.DAY_OF_MONTH,3);System.out.println(c.getTime()); 类初始化顺序：父类静态变量、静态语句块 → 子类静态变量、静态语句块 → 父类实例变量、普通语句块 → 父类构造方法 → 子类实例变量、普通语句块 → 子类构造方法 SpringCloud调试时经常出现服务非正常关闭导致端口被占用的情况，使用cmd使用taskkill /f /t /im java.exe杀进程 如果父类没有实现序列化，而子类实现列序列化。那么父类中的成员没办法做序列化操作]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线程异常处理]]></title>
    <url>%2F2018%2F11%2F04%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F%E7%BA%BF%E7%A8%8B%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[Thread.DefaultUncaughtExceptionHandler设置该线程由于未捕获到异常而突然终止时调用的处理程序。通过明确设置未捕获到的异常处理程序，线程可以完全控制它对未捕获到的异常作出响应的方式。如果没有设置这样的处理程序，则该线程的 ThreadGroup 对象将充当其处理程序。 Thread.UncaughtExceptionHandler设置当线程由于未捕获到异常而突然终止，并且没有为该线程定义其他处理程序时所调用的默认处理程序。未捕获到的异常处理首先由线程控制，然后由线程的 ThreadGroup 对象控制，最后由未捕获到的默认异常处理程序控制。如果线程不设置明确的未捕获到的异常处理程序，并且该线程的线程组（包括父线程组）未特别指定其 uncaughtException 方法，则将调用默认处理程序的 uncaughtException 方法。通过设置未捕获到的默认异常处理程序，应用程序可以为那些已经接受系统提供的任何“默认”行为的线程改变未捕获到的异常处理方式请注意，未捕获到的默认异常处理程序通常不应顺从该线程的 ThreadGroup 对象，因为这可能导致无限递归。 1234567891011121314151617181920212223242526272829303132333435363738public class Test &#123; public static void main(String[] args) &#123; set(); test(); &#125; private static void test()&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("子线程异常"); System.out.println(1/0); &#125; &#125;).start(); System.out.println("当前线程异常"); System.out.println(1/0); &#125; private static void set()&#123; Thread.UncaughtExceptionHandler uncaughtExceptionHandler = new Thread.UncaughtExceptionHandler() &#123; @Override public void uncaughtException(Thread t, Throwable e) &#123; System.out.println("current"+t.toString()+":"+e.getMessage()); &#125; &#125;; Thread.UncaughtExceptionHandler defaultHandler = new Thread.UncaughtExceptionHandler() &#123; @Override public void uncaughtException(Thread t, Throwable e) &#123; System.out.println("default"+t.toString()+":"+e.getMessage()); &#125; &#125;; Thread.currentThread().setUncaughtExceptionHandler(uncaughtExceptionHandler); Thread.setDefaultUncaughtExceptionHandler(defaultHandler); &#125;&#125;]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyCAT入门]]></title>
    <url>%2F2018%2F11%2F04%2FMyCAT%2FMyCAT%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[MyCAT是一个数据库中间层，屏蔽分库分表 读写分离的细节 实现负载均衡 控制连接数量 分片表 是 水平切分的表 wget http://dl.mycat.io/1.6.5/Mycat-server-1.6.5-release-20180122220033-linux.tar.gz https://www.cnblogs.com/ivictor/p/5111495.html adduser mycat chown mycat:mycat -R mycat/ 逻辑库schema 逻辑表 table 分片表 水平切分的表 非分片表 er表 全局表 分片节点 datanode 每个表分片所在的数据库就是分片节点 节点主机 一个或多个分片节点所在的机器就是节点主机 原理拦截用户发送的sql,进行分析，然后发往真实数据库,将返回的结果做适当处理，最终再返回给用户]]></content>
      <categories>
        <category>MyCAT</category>
      </categories>
      <tags>
        <tag>MyCAT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java.time包]]></title>
    <url>%2F2018%2F11%2F04%2FJava%2Fjava.time%E5%8C%85%2F</url>
    <content type="text"><![CDATA[java8新加的java.time包 123456LocalDate localDate=LocalDate.of(2018,10,29);System.out.println(localDate.getYear());System.out.println(localDate.getMonth());System.out.println(localDate.getDayOfMonth());System.out.println(localDate.getDayOfWeek());System.out.println(localDate.isLeapYear()); 1234LocalTime localTime= LocalTime.of(19,01,45);System.out.println(localTime.getHour());System.out.println(localTime.getMinute());System.out.println(localTime.getSecond()); 12LocalDate localDate=LocalDate.parse(&quot;2018-07-21&quot;);LocalTime localTime=LocalTime.parse(&quot;19:03:44&quot;);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运算符]]></title>
    <url>%2F2018%2F11%2F04%2FJava%2F%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[&amp;&amp; 短路 &amp;&amp; || 优先&amp;&amp; +=属于右结合运算符 位运算按位与：&amp; a,b都为1,结果位才是1，否则0 按位或：| a,b都为0,结果位才是0，否则1 按位取反：~ 1改成0，0改成1 按位异或：^ 相同为0，否则为1 移位操作：&lt;&lt; &gt;&gt;右移 最高位是0补0 是1补1 &gt;&gt;&gt; 左侧被移空高位都填入0 y&lt;&lt;2 = y*4 y&gt;&gt;1 = y/2]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[类加载机制]]></title>
    <url>%2F2018%2F11%2F04%2FJVM%2F%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[类的加载机制就是将class文件加载到内存，对数据进行校验，转换，解析，初始化，变成可以使用的java类型 加载步骤加载：根据全限定名获取二进制流，静态存储结构转化为运行时数据结构，生成class对象作为各种数据的访问入口 验证：文件格式验证，元数据验证，字节码验证，符号引用验证 。确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全 准备：分配内存，设置初始值 解析：符号引号替换为直接引用，其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定 初始化：通过程序制定的主观计划去初始化类变量和其它资源 必须对类进行初始化的情况 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。 使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类； 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化； 被动引用不会初始化 通过子类引用父类的静态变量，不会导致子类初始化 父类会初始化 通过数组定义来引用类，不会触发此类的初始化。该过程对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化 static final 不会初始化 static会初始化 任何一个类 ，都需要有加载它的类加载器和这个类本身确立在jvm中的唯一性，来源于同一个class，但加载它们的类加载器不同，那么这两个类必定不相等 ‘ 类加载器分类 启动类加载器（Bootstrap ClassLoader）此类加载器负责将存放在 &lt;JRE_HOME&gt;\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可。 扩展类加载器（Extension ClassLoader）这个类加载器是由 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将 &lt;JAVA_HOME&gt;/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）这个类加载器是由 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此一般称为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 双亲委派机制一个类加载器首先将类加载请求传送到父类加载器，只有当父类加载器无法完成类加载请求时才尝试加载，使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而使得基础类得到统一 双亲委派的好处 : 由于每个类加载都会经过最顶层的启动类加载器，比如 java.lang.Object这样的类在各个类加载器下都是同一个类(只有当两个类是由同一个类加载器加载的才有意义，这两个类才相等。) 如果没有双亲委派模型，由各个类加载器自行加载的话。当用户自己编写了一个 java.lang.Object类，那样系统中就会出现多个 Object，这样 Java 程序中最基本的行为都无法保证，程序会变的非常混乱。 如何破坏双亲委派机制？ 可以自定义一个ClassLoader，重写loadClass,findClass方法]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>ClassLoader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Array和ArrayBuffer]]></title>
    <url>%2F2018%2F11%2F04%2FScala%2FArray%E5%92%8CArrayBuffer%2F</url>
    <content type="text"><![CDATA[Array固定长度 ArrayBuff可变 1234567891011121314151617181920212223242526272829303132333435363738object Demo &#123; def main(args: Array[String]): Unit = &#123; // 初始化数组 val a=new Array[Int](100) var b=Array("hello","world") // 初始化ArrayBuff可变数组 相当于arraylist var c=ArrayBuffer[Int]() // 添加元素 c+=1 // 添加多个元素 c+=(2,3,4,5) // 添加其他集合的元素 c++=Array(6,7,8,9) // 尾部去2个 c.trimEnd(2) // 在某个索引位置 插入元素 c.insert(0,10,11) // 删除某个位置的元素 c.remove(0) // 索引位置 个数 c.remove(0,3) // 遍历打印 c.foreach(println) // array arraybuffer 可以互相转换 c.toArray b.toBuffer // 求和 a.sum // 最大值 a.max // 排序 Sorting.quickSort(a) // 获取数据所有元素内容 val str=a.mkString("&lt;",",","&gt;"); println(str) &#125;&#125;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala函数]]></title>
    <url>%2F2018%2F11%2F03%2FScala%2FScala%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[变长参数1234567891011121314object Demo &#123; def main(args: Array[String]): Unit = &#123; val s=sum(1,2,3,4,5) println(s) &#125; def sum(nums: Int*): Int =&#123; var result=0; for (num&lt;- nums)&#123; result+=num &#125; result &#125;&#125; 也可以使用val s=sum(1 to 5: _*)使用序列调用变长参数 异常1234567891011object Demo &#123; def main(args: Array[String]): Unit = &#123; try &#123; throw new IllegalArgumentException("exception") &#125;catch &#123; case _:IllegalArgumentException=&gt;println("catch") &#125;finally &#123; println("finally") &#125; &#125;&#125; to [x,y] range [x,y） 还有step步长 until _占位符 12//指定范围private [this] val gender=&quot;male&quot; 123456789class Animal(val name:String,val age:Int)&#123; var sch=&quot;jit&quot;; println(name+&quot;age:&quot;+age+&quot;school&quot;+sch) // 附属构造器 def this( name:String, age:Int,sch:String)&#123; this(name,age) this.sch=sch &#125;&#125; 继承+重写1234567891011121314class Tiger(name:String,age:Int,height:Int) extends Animal(name, age )&#123; override def toString: String = &#123; &quot;重写了&quot; &#125;&#125;class Animal( name:String, age:Int)&#123; var sch=&quot;jit&quot;; println(name+&quot;age:&quot;+age+&quot;school&quot;+sch) // 附属构造器 def this( name:String, age:Int,sch:String)&#123; this(name,age) this.sch=sch &#125;&#125; apply方法单例对象与类同名时，这个单例对象被称为这个类的伴生对象，而这个类被称为这个单例对象的伴生类。伴生类和伴生对象要在同一个源文件中定义，伴生对象和伴生类可以互相访问其私有成员。不与伴生类同名的单例对象称为孤立对象 String打印1234567891011121314object ArrayApp &#123; def main(args: Array[String]): Unit = &#123; val s=&quot;hello:pk&quot; var name=&quot;mason&quot; println(s&quot;hello:$name&quot;) var b= &quot;&quot;&quot; | &quot;&quot;&quot;.stripMargin println(b) &#125;&#125; 匿名函数和科里化123456789101112object ArrayApp &#123; def main(args: Array[String]): Unit = &#123; var a=(x:Int)=&gt; x+1 println(a(10)) // curry 科里化 println(sum(2)(1)) &#125; def sum(a:Int)(b:Int)=a+b&#125; map filter flatmap reduce]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ安装]]></title>
    <url>%2F2018%2F11%2F03%2FRabbitMQ%2FRabbitMQ%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Docker安装RabbitMQ123docker search rabbitmq:managementdocker pull rabbitmq:managementdocker run -d --name rabbitmq --publish 5671:5671 --publish 5672:5672 --publish 4369:4369 --publish 25672:25672 --publish 15671:15671 --publish 15672:15672 rabbitmq:management 12http://192.168.10.51:15672/guest guest 普通安装安装编译工具123yum install -y ncurses ncurses-base ncurses-devel ncurses-libs ncurses-static ncurses-term ocaml-curses ocaml-curses-develyum install -y openssl-devel zlib-develyum install -y make ncurses-devel gcc gcc-c++ unixODBC unixODBC-devel openssl openssl-devel 安装erlang下载erlang：http://erlang.org/download/otp_src_20.0.tar.gz 12345678910tar -zxvf otp_src_20.0.tar.gzcd otp_src_20.0./configure --prefix=/usr/local/erlang --with-ssl -enable-threads -enable-smmp-support -enable-kernel-poll --enable-hipe --without-javacmake &amp;&amp; make installln -s /usr/local/erlang/bin/erl /usr/local/bin/erlvi ~/.bashrcERLANG_HOME=/usr/local/erlangPATH=$ERLANG_HOME/bin:$PATHsource ~/.bashrcerl 安装rabbitmqhttp://www.rabbitmq.com/releases/rabbitmq-server/v3.6.12/rabbitmq-server-generic-unix-3.6.12.tar.xz 12345678910111213141516yum install -y xzxz -d rabbitmq-server-generic-unix-3.6.12.tar.xztar -xvf rabbitmq-server-generic-unix-3.6.12.tarmv rabbitmq_server-3.6.1 rabbitmq-3.6.12# 开启管理页面的插件cd rabbitmq-3.6.1/sbin/./rabbitmq-plugins enable rabbitmq_management# 后台启动rabbitmq server./rabbitmq-server -detached# 关闭rabbitmq server./rabbitmqctl stop# 添加管理员账号./rabbitmqctl add_user rabbitadmin 123456./rabbitmqctl set_user_tags rabbitadmin administrator进入管理页面15672端口号，输入用户名和密码]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Collections.sort()解读]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2FCollections.sort()%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[Collections.sort方法底层就是调用的Array.sort 123456789101112131415161718192021222324252627282930313233343536373839404142434445static &lt;T&gt; void sort(T[] a, int lo, int hi, Comparator&lt;? super T&gt; c, T[] work, int workBase, int workLen) &#123; assert c != null &amp;&amp; a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; int nRemaining = hi - lo; if (nRemaining &lt; 2) return; // array的大小为0或者1就不用排了 // 当数组大小小于MIN_MERGE(32)的时候，就用一个"mini-TimSort"的方法排序 if (nRemaining &lt; MIN_MERGE) &#123; // 将最长的递减序列，找出来，然后倒过来 int initRunLen = countRunAndMakeAscending(a, lo, hi, c); // 长度小于32的时候，是使用binarySort的 binarySort(a, lo, hi, lo + initRunLen, c); return; &#125; // 先扫描一次array，找到已经排好的序列，然后再用刚才的mini-TimSort，然后合并 TimSort&lt;T&gt; ts = new TimSort&lt;&gt;(a, c, work, workBase, workLen); int minRun = minRunLength(nRemaining); do &#123; // Identify next run int runLen = countRunAndMakeAscending(a, lo, hi, c); // If run is short, extend to min(minRun, nRemaining) if (runLen &lt; minRun) &#123; int force = nRemaining &lt;= minRun ? nRemaining : minRun; binarySort(a, lo, lo + force, lo + runLen, c); runLen = force; &#125; // Push run onto pending-run stack, and maybe merge ts.pushRun(lo, runLen); ts.mergeCollapse(); // Advance to find next run lo += runLen; nRemaining -= runLen; &#125; while (nRemaining != 0); // Merge all remaining runs to complete sort assert lo == hi; ts.mergeForceCollapse(); assert ts.stackSize == 1; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Collections</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ApplicationContext]]></title>
    <url>%2F2018%2F11%2F03%2FSpring%2FApplicationContext%2F</url>
    <content type="text"><![CDATA[applicationcontext和beanfactory都是加载bean的 applicationcontext包含beanfactory的所有功能，是对beanfactory的扩展。 添加了@Qualifier @Autowired等功能 beanfactory适合内存小的 实例化bean比较复杂，FactoryBean是一个工厂类接口，可以通过改接口实例化bean的逻辑 spring中的循环依赖分三种 构造器循环依赖 无法解决 只能跑出beancurrentlyincreateionexception setter注入 通过提前暴露单例工厂方法addSingletionFactory protootype,spring无法完成依赖注入]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>ApplicationContext</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[001两数之和]]></title>
    <url>%2F2018%2F11%2F03%2FLeetCode%2F001%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C%20%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122public class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; for (int j = i+1; j &lt; nums.length; j++) &#123; if(nums[j]==target-nums[i]) &#123; return new int[] &#123;i,j&#125;; &#125; &#125; &#125; throw new IllegalArgumentException("No two sum solution"); &#125; public static void main(String[] args) &#123; Solution solution=new Solution(); int[] nums= &#123;2, 7, 11, 15&#125;; int target=9; int[] data= solution.twoSum(nums, target); System.out.println(Arrays.toString(data)); &#125;&#125; 1234567891011121314151617181920212223242526public class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map=new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; map.put(nums[i], i); &#125; for (int i = 0; i &lt; nums.length; i++) &#123; int key=target-nums[i]; if(map.containsKey(key)&amp;&amp;map.get(key)!=i) &#123; return new int[] &#123;i,map.get(key)&#125;; &#125; &#125; throw new IllegalArgumentException("No two sum solution"); &#125; public static void main(String[] args) &#123; Solution solution=new Solution(); int[] nums= &#123;2, 7, 11, 15&#125;; int target=9; int[] data= solution.twoSum(nums, target); System.out.println(Arrays.toString(data)); &#125;&#125; 1234567891011121314151617181920212223public class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map=new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; map.put(nums[i], i); int key=target-nums[i]; if(map.containsKey(key)&amp;&amp;map.get(key)!=i) &#123; return new int[] &#123;map.get(key),i&#125;; &#125; &#125; throw new IllegalArgumentException(&quot;No two sum solution&quot;); &#125; public static void main(String[] args) &#123; Solution solution=new Solution(); int[] nums= &#123;2, 7, 11, 15&#125;; int target=9; int[] data= solution.twoSum(nums, target); System.out.println(Arrays.toString(data)); &#125;&#125; 总结：检查数组中是否存在目标元素，返回下标，保持数组中的每个元素与其索引相互对应的最好方法是哈希表]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Queue]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2FQueue%2F</url>
    <content type="text"><![CDATA[Queue是一个FIFO的数据结构，Queue接口与List、Set一样都继承了Collection接口 Deque双向队列，队列两端的元素既能入队也能出队，LinkedList实现了Deque接口 非阻塞队列 PriorityQueue实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的 java.util.Comparator 实现来定位 ConcurrentLinkedQueue是基于链接节点的、线程安全的无界队列。并发访问不需要同步，需要遍历队列。 因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，ConcurrentLinkedQueue 对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。采用CAS机制（compareAndSwapObject原子操作）。不支持阻塞去取元素 阻塞队列 LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。LinkedBlockingQueue的容量是没有上限的（说的不准确，在不指定时容量为Integer.MAX_VALUE，不要然的话在put时怎么会受阻呢），但是也可以选择指定其最大容量，它是基于链表的队列，此队列按 FIFO（先进先出）排序元素。支持阻塞的take()方法 。使用 ReentrantLock 锁,添加元素为原子操作的队列 ArrayBlockingQueue ：一个由数组支持的有界队列。ArrayBlockingQueue在构造时需要指定容量， 并可以选择是否需要公平性，如果公平参数被设置true，等待时间最长的线程会优先得到处理（其实就是通过将ReentrantLock设置为true来 达到这种公平性的：即等待时间最长的线程会先操作）。通常，公平性会使你在性能上付出代价，只有在的确非常需要的时候再使用它。它是基于数组的阻塞循环队 列，此队列按 FIFO（先进先出）原则对元素进行排序。 PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列，而不是先进先出队列。元素按优先级顺序被移除，该队列也没有上限（看了一下源码，PriorityBlockingQueue是对 PriorityQueue的再次包装，是基于堆数据结构的，而PriorityQueue是没有容量限制的，与ArrayList一样，所以在优先阻塞 队列上put时是不会受阻的。虽然此队列逻辑上是无界的，但是由于资源被耗尽，所以试图执行添加操作可能会导致 OutOfMemoryError），但是如果队列为空，那么取元素的操作take就会阻塞，所以它的检索操作take是受阻的。另外，往入该队列中的元 素要具有比较能力。 DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。DelayQueue（基于PriorityQueue来实现的）是一个存放Delayed 元素的无界阻塞队列，只有在延迟期满时才能从中提取元素。该队列的头部是延迟期满后保存时间最长的 Delayed 元素。如果延迟都还没有期满，则队列没有头部，并且poll将返回null。当一个元素的 getDelay(TimeUnit.NANOSECONDS) 方法返回一个小于或等于零的值时，则出现期满，poll就以移除这个元素了。此队列不允许使用 null 元素。 SynchronousQueue （并发同步阻塞队列）一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。 method description add 添加元素 如果队列满 抛出IIIegaISlabEepeplian异常 offer 元素插入到队尾 如果队列满，返回false put 添加元素 队列满则阻塞 peek 不移除元素返回队头，队列为空返回null element 不移除元素返回队头，队列为空抛NoSuchElementException异常 poll 移除元素返回队头，队列为空返回null remove 移除元素返回队头，队列为空抛NoSuchElementException异常 take 移除并返回队头，队列空则阻塞 生产者-消费者阻塞队列支持设计模式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.yao;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class BlockingQueueTest &#123; /** 定义装苹果的篮子 */ public static class Basket&#123; // 篮子，能够容纳3个苹果 BlockingQueue&lt;String&gt; basket = new ArrayBlockingQueue&lt;String&gt;(3); // 生产苹果，放入篮子 public void produce() throws InterruptedException&#123; // put方法放入一个苹果，若basket满了，等到basket有位置 basket.put("An apple"); &#125; // 消费苹果，从篮子中取走 public String consume() throws InterruptedException&#123; // get方法取出一个苹果，若basket为空，等到basket有苹果为止 String apple = basket.take(); return apple; &#125; public int getAppleNumber()&#123; return basket.size(); &#125; &#125; // 测试方法 public static void testBasket() &#123; // 建立一个装苹果的篮子 final Basket basket = new Basket(); // 定义苹果生产者 class Producer implements Runnable &#123; public void run() &#123; try &#123; while (true) &#123; // 生产苹果 System.out.println("生产者准备生产苹果：" + System.currentTimeMillis()); basket.produce(); System.out.println("生产者生产苹果完毕：" + System.currentTimeMillis()); System.out.println("生产完后有苹果："+basket.getAppleNumber()+"个"); // 休眠300ms Thread.sleep(300); &#125; &#125; catch (InterruptedException ex) &#123; &#125; &#125; &#125; // 定义苹果消费者 class Consumer implements Runnable &#123; public void run() &#123; try &#123; while (true) &#123; // 消费苹果 System.out.println("消费者准备消费苹果：" + System.currentTimeMillis()); basket.consume(); System.out.println("消费者消费苹果完毕：" + System.currentTimeMillis()); System.out.println("消费完后有苹果："+basket.getAppleNumber()+"个"); // 休眠1000ms Thread.sleep(1000); &#125; &#125; catch (InterruptedException ex) &#123; &#125; &#125; &#125; ExecutorService service = Executors.newCachedThreadPool(); Producer producer = new Producer(); Consumer consumer = new Consumer(); service.submit(producer); service.submit(consumer); // 程序运行10s后，所有任务停止 try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; &#125; service.shutdownNow(); &#125; public static void main(String[] args) &#123; BlockingQueueTest.testBasket(); &#125;&#125; 123451.LinkedBlockingQueue是使用锁机制，ConcurrentLinkedQueue是使用CAS算法，虽然LinkedBlockingQueue的底层获取锁也是使用的CAS算法2.关于取元素，ConcurrentLinkedQueue不支持阻塞去取元素，LinkedBlockingQueue支持阻塞的take()方法，如若大家需要ConcurrentLinkedQueue的消费者产生阻塞效果，需要自行实现3.关于插入元素的性能，从字面上和代码简单的分析来看ConcurrentLinkedQueue肯定是最快的，但是这个也要看具体的测试场景，我做了两个简单的demo做测试，测试的结果如下，两个的性能差不多，但在实际的使用过程中，尤其在多cpu的服务器上，有锁和无锁的差距便体现出来了，ConcurrentLinkedQueue会比LinkedBlockingQueue快很多： linkedlist线程不安全]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2F%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[数组初始化123int[] arr1= new int[5];int[] arr2= &#123;1,2,3,4,5&#125;;int[] arr3= new int[] &#123;1,2,3,4,5&#125;; Arrays method description Arrays.fill(arr, val); 替换数组的值 Arrays.fill(arr, fromIndex, toIndex, val); 替换数组下标[fromIndex,toIndex)的值 Arrays.copyOf(arr, newLength); 拷贝数组，先数组长度为newLength Arrays.copyOfRange(arr, from, to); 拷贝数组下标[from,to)的值 Arrays.toString(arr); 打印数组 Arrays.binarySearch(arr, val); 二分查找，返回下标 System.arraycopy(src, srcPos, dest, destPos, length); src原数组，srcPos起始index，dest目标数组，despos起始位置，length拷贝长度，属于浅拷贝 Arrays.sort(arr); 排序 Arrays.equals(arr1, arr2); 比较两个数据是否相等]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[简单的JDBC]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2F%E7%AE%80%E5%8D%95%E7%9A%84JDBC%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536public class Conn &#123; static Connection connection; static Statement stat; static ResultSet rs; public Connection getConnection() &#123; try &#123; Class.forName(&quot;com.mysql.jdbc.Driver&quot;); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; Connection conn=null; try &#123; conn=DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/mall&quot;, &quot;root&quot;, &quot;Gepoint&quot;); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return conn; &#125; public static void main(String[] args) &#123; Conn c=new Conn(); connection=c.getConnection(); try &#123; stat=connection.createStatement(); rs=stat.executeQuery(&quot;select * from spbrands limit 10&quot;); while(rs.next()) &#123; String brand=rs.getString(&quot;Brand&quot;); System.out.println(&quot;品牌:&quot;+brand); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap实现一个LRU本地缓存]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2FLinkedHashMap%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AALRU%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[基于访问的最近最少使用算法 构造函数accessOrder=true时,当get（Object key）时，将最新访问的元素放到双向链表的第一位 123456789101112131415161718192021222324252627public class Demo &#123; public static void main(String[] args) &#123; Map&lt;String,String&gt; map=new LinkedHashMap&lt;&gt;(16,0.75f,true); map.put(&quot;brand1&quot;, &quot;12&quot;); map.put(&quot;brand2&quot;, &quot;13&quot;); map.put(&quot;brand3&quot;, &quot;14&quot;); map.put(&quot;brand4&quot;, &quot;15&quot;); map.put(&quot;brand5&quot;, &quot;16&quot;); map.put(&quot;brand6&quot;, &quot;17&quot;); print(map); map.get(&quot;brand2&quot;); print(map); map.get(&quot;brand5&quot;); print(map); &#125; private static void print(Map&lt;String,String&gt; map) &#123; Set&lt;Entry&lt;String,String&gt;&gt; set=map.entrySet(); Iterator&lt;Entry&lt;String,String&gt;&gt; i=set.iterator(); while(i.hasNext()) &#123; Entry&lt;String,String&gt; entry=i.next(); System.out.println(entry.getValue()+entry.getKey()); &#125; System.out.println(&quot;---&quot;); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>LinkedHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String与StrinBuilder与StringBuffer]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2FString%E4%B8%8EStrinBuilder%E4%B8%8EStringBuffer%2F</url>
    <content type="text"><![CDATA[StringString的创建机理由于String在Java世界中使用过于频繁，Java为了避免在一个系统中产生大量的String对象，引入了字符串常量池。其运行机制是： 创建一个字符串时，首先检查池中是否有值相同的字符串对象，如果有则不需要创建直接从池中刚查找到的对象引用；如果没有则新建字符串对象，返回对象引用，并且将新创建的对象放入池中。但是，通过new方法创建的String对象是不检查字符串池的，而是直接在堆区或栈区创建一个新的对象，也不会把对象放入池中。上述原则只适用于通过直接量给String对象引用赋值的情况。 举例： 12String str1 = "123"; //通过直接量赋值方式，放入字符串常量池String str2 = new String(“123”);//通过new方式赋值方式，不放入字符串常量池 注意：String提供了intern()方法。调用该方法时，如果常量池中包括了一个等于此String对象的字符串（由equals方法确定），则返回池中的字符串。否则，将此String对象添加到池中，并且返回此池中对象的引用。 Intern()是一种显式地排重机制，但是它也有一定的副作用，因为需要开发者写代码时明确调用，一是不方便，每一个都显式调用是非常麻烦的；另外就是我们很难保证效率，应用开发阶段很难清楚地预计字符串的重复情况，有人认为这是一种污染代码的实践。幸好在Oracle JDK 8u20之后，推出了一个新的特性，也就是G1 GC下的字符串排重。它是通过将相同数据的字符串指向同一份数据来做到的，是JVM底层的改变，并不需要Java类库做什么修改。注意这个功能目前是默认关闭的，你需要使用下面参数开启，并且记得指定使用G1 GC：-XX:+UseStringDeduplication String的特性 不可变。是指String对象一旦生成，则不能再对它进行改变。不可变的主要作用在于当一个对象需要被多线程共享，并且访问频繁时，可以省略同步和锁等待的时间，从而大幅度提高系统性能。不可变模式是一个可以提高多线程程序的性能，降低多线程程序复杂度的设计模式。 针对常量池的优化。当2个String对象拥有相同的值时，他们只引用常量池中的同一个拷贝。当同一个字符串反复出现时，这个技术可以大幅度节省内存空间。 StringBufer/StringBuilderStringBuffer和StringBuilder都实现了AbstractStringBuilder抽象类，拥有几乎一致对外提供的调用接口；其底层在内存中的存储方式与String相同，都是以一个有序的字符序列（char类型的数组）进行存储，不同点是StringBufer/StringBuilder对象的值是可以改变的，并且值改变以后，对象引用不会发生改变;两者对象在构造过程中，首先按照默认大小申请一个字符数组，由于会不断加入新数据，当超过默认大小后，会创建一个更大的数组，并将原先的数组内容复制过来，再丢弃旧的数组。因此，对于较大对象的扩容会涉及大量的内存复制操作，如果能够预先评估大小，可提升性能。需要注意的是：StringBuffer是线程安全的，但是StringBuilder是线程不安全的，StringBuffer类中方法定义前面都会有synchronized关键字。为此，StringBuffer的性能要远低StringBuilder 应用场景 在字符串内容不经常发生变化的业务场景优先使用String类。例如：常量声明、少量的字符串拼接操作等。如果有大量的字符串内容拼接，避免使用String与String之间的“+”操作，因为这样会产生大量无用的中间对象，耗费空间且执行效率低下（新建对象、回收对象花费大量时间）。 在频繁进行字符串的运算（如拼接、替换、删除等），并且运行在多线程环境下，建议使用StringBufer，例如XML解析、HTTP参数解析与封装。 在频繁进行字符串的运算（如拼接、替换、删除等），并且运行在单线程环境下，建议使用StringBuilder，例如SQL语句拼装、JSON封装等。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并行数组]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2F%E5%B9%B6%E8%A1%8C%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[并行数组Java8版本新增了很多新的方法，用于支持并行数组处理。最重要的方法是parallelSort()，可以显著加快多核机器上的数组排序。下面的例子论证了parallexXxx系列的方法： 123456789101112131415161718192021package com.javacodegeeks.java8.parallel.arrays;import java.util.Arrays;import java.util.concurrent.ThreadLocalRandom;public class ParallelArrays &#123; public static void main( String[] args ) &#123; long[] arrayOfLong = new long [ 20000 ]; Arrays.parallelSetAll( arrayOfLong, index -&gt; ThreadLocalRandom.current().nextInt( 1000000 ) ); Arrays.stream( arrayOfLong ).limit( 10 ).forEach( i -&gt; System.out.print( i + " " ) ); System.out.println(); Arrays.parallelSort( arrayOfLong ); Arrays.stream( arrayOfLong ).limit( 10 ).forEach( i -&gt; System.out.print( i + " " ) ); System.out.println(); &#125;&#125; 上述这些代码使用parallelSetAll()方法生成20000个随机数，然后使用parallelSort()方法进行排序。这个程序会输出乱序数组和排序数组的前10个元素。上述例子的代码输出的结果是： 12Unsorted: 591217 891976 443951 424479 766825 351964 242997 642839 119108 552378 Sorted: 39 220 263 268 325 607 655 678 723 793]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[接口的默认方法和静态方法]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2F%E6%8E%A5%E5%8F%A3%E7%9A%84%E9%BB%98%E8%AE%A4%E6%96%B9%E6%B3%95%E5%92%8C%E9%9D%99%E6%80%81%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[接口的默认方法和静态方法Java 8使用两个新概念扩展了接口的含义：默认方法和静态方法。 默认方法默认方法使得开发者可以在 不破坏二进制兼容性的前提下，往现存接口中添加新的方法，即不强制那些实现了该接口的类也同时实现这个新加的方法。 默认方法和抽象方法之间的区别在于抽象方法需要实现，而默认方法不需要。接口提供的默认方法会被接口的实现类继承或者覆写，例子代码如下： 1234567891011121314151617private interface Defaulable &#123; // Interfaces now allow default methods, the implementer may or // may not implement (override) them. default String notRequired() &#123; return "Default implementation"; &#125; &#125;private static class DefaultableImpl implements Defaulable &#123;&#125;private static class OverridableImpl implements Defaulable &#123; @Override public String notRequired() &#123; return "Overridden implementation"; &#125;&#125; Defaulable接口使用关键字default定义了一个默认方法notRequired()。DefaultableImpl类实现了这个接口，同时默认继承了这个接口中的默认方法；OverridableImpl类也实现了这个接口，但覆写了该接口的默认方法，并提供了一个不同的实现。 静态方法Java 8带来的另一个有趣的特性是在接口中可以定义静态方法，我们可以直接用接口调用这些静态方法。例子代码如下： 123456private interface DefaulableFactory &#123; // Interfaces now allow static methods static Defaulable create( Supplier&lt; Defaulable &gt; supplier ) &#123; return supplier.get(); &#125;&#125; 下面的代码片段整合了默认方法和静态方法的使用场景： 12345678public static void main( String[] args ) &#123; // 调用接口的静态方法，并且传递DefaultableImpl的构造函数引用来构建对象 Defaulable defaulable = DefaulableFactory.create( DefaultableImpl::new ); System.out.println( defaulable.notRequired() ); // 调用接口的静态方法，并且传递OverridableImpl的构造函数引用来构建对象 defaulable = DefaulableFactory.create( OverridableImpl::new ); System.out.println( defaulable.notRequired() );&#125; 这段代码的输出结果如下： 12Default implementationOverridden implementation 由于JVM上的默认方法的实现在字节码层面提供了支持，因此效率非常高。默认方法允许在不打破现有继承体系的基础上改进接口。该特性在官方库中的应用是：给java.util.Collection接口添加新方法，如stream()、parallelStream()、forEach()和removeIf()等等。 尽管默认方法有这么多好处，但在实际开发中应该谨慎使用：在复杂的继承体系中，默认方法可能引起歧义和编译错误。如果你想了解更多细节，可以参考官方文档。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Optional]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2FOptional%2F</url>
    <content type="text"><![CDATA[OptionalJava应用中最常见的bug就是空指针异常 Optional仅仅是一个容器，可以存放T类型的值或者null。它提供了一些有用的接口来避免显式的null检查，可以参考Java 8官方文档了解更多细节。 method description isPresent 有值返回true 否则返回false get() 有值时返回值 没有抛出异常 orElse 有值时返回值 没有返回默认值 orElseGet 有值时返回值 没有返回一个supplier接口生成的值 map 只存在就执行mapping函数调用,以将现有Optional实例的值转换成新的值 创建Optional1234567Apple apple = new Apple();// 空的optionalOptional&lt;Apple&gt; optApple=Optional.empty(); // 值为nul抛出异常Optional&lt;Apple&gt; optApple1=Optional.of(apple); // 值为null 返回空的optionalOptional&lt;Apple&gt; optApple2=Optional.ofNullable(apple);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Optional</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM故障引起的问题]]></title>
    <url>%2F2018%2F11%2F03%2FJVM%2FJVM%E6%95%85%E9%9A%9C%E5%BC%95%E8%B5%B7%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[nio操作用到堆外内存，堆外内存只能等老年代满了以后fullgc 顺便回收一下，否则会一直等到oom 异步处理 等待的线程太多 积压了很多socket，jvm直接崩溃，所有改成生产者消费者的模式]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM性能监控和故障处理工具]]></title>
    <url>%2F2018%2F11%2F03%2FJVM%2FJVM%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%92%8C%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[jps查看虚拟机进程状态 -l 主类的全名 -v 数据jvm参数 jstat看虚拟机状态 jstat -gc 6424 jstat -gcutil 6424 jinfo 看jvm的参数 jinfo -flag CMSInitiatingOccupancyFraction 5192 jmap -dump:format=b,file=D:\DUMP.bin 5192 生成堆转储快照 jmap -heap 5192 堆详情信息 jhat D:\DUMP.bin 分析dump jstack 5192 打印堆栈信息 jconsole、visualVM可视化工具]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[引用类型]]></title>
    <url>%2F2018%2F11%2F03%2FJVM%2F%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[判定对象是否可被回收都与引用有关，在Java语言中，除了基本数据类型外，其他的都是指向各类对象的对象引用；Java中根据其生命周期的长短，将引用分为4类。 引用类型强引用1Object obj = new Object(); 通过关键字new创建的对象所关联的引用就是强引用。 当JVM内存空间不足，JVM宁愿抛出OutOfMemoryError运行时错误（OOM）使程序异常终止，也不会随意回收具有强引用的“存活”对象来解决内存不足的问题。对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为 null，就是可以被垃圾收集的了，具体回收时机还是要看垃圾收集策略。 软引用123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; 软引用通过SoftReference类实现。 软引用的生命周期比强引用短一些。只有当 JVM 认为内存不足时，才会去试图回收软引用指向的对象：即JVM 会确保在抛出 OutOfMemoryError之前，清理软引用指向的对象。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。后续，我们可以调用ReferenceQueue的poll()方法来检查是否有它所关心的对象被回收。如果队列为空，将返回一个null,否则该方法返回队列中前面的一个Reference对象。应用场景：软引用通常用来实现内存敏感的缓存。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 android图片缓存框架中，“内存缓存”中的图片是以这种引用来保存，使得JVM在发生OOM之前，可以回收这部分缓存 弱引用123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; 弱引用通过WeakReference类实现。 弱引用的生命周期比软引用短。被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。由于垃圾回收器是一个优先级很低的线程，因此不一定会很快回收弱引用的对象。弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。应用场景：弱应用同样可用于内存敏感的缓存。 在静态内部类中，经常会使用虚引用。例如，一个类发送网络请求，承担callback的静态内部类，则常以虚引用的方式来保存外部类(宿主类)的引用，当外部类需要被JVM回收时，不会因为网络请求没有及时回来，导致外部类不能被回收，引起内存泄漏 虚引用123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj);obj = null; 特点：虚引用也叫幻象引用，通过PhantomReference类来实现。无法通过虚引用访问对象的任何属性或函数。幻象引用仅仅是提供了一种确保对象被 fnalize 以后，做某些事情的机制。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。ReferenceQueue queue = new ReferenceQueue ();PhantomReference pr = new PhantomReference (object, queue);程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取一些程序行动。应用场景：可用来跟踪对象被垃圾回收器回收的活动，当一个虚引用关联的对象被垃圾收集器回收之前会收到一条系统通知。 这种引用的get()方法返回总是null，所以，可以想象，在平常的项目开发肯定用的少。但是根据这种引用的特点，我想可以通过监控这类引用，来进行一些垃圾清理的动作 引用队列引用队列（ReferenceQueue）使用谈到各种引用的编程，就必然要提到引用队列。我们在创建各种引用并关联到响应对象时，可以选择是否需要关联引用队列，JVM会在特定时机将引用enqueue到队列里，我们可以从队列里获取引用（remove方法在这里实际是有获取的意思）进行相关后续逻辑。尤其是幻象引用，get方法只返回null，如果再不指定引用队列，基本就没有意义了。看看下面的示例代码。利用引用队列，我们可以在对象处于相应状态时（对于幻象引用，就是前面说的被fnalize了，处于幻象可达状态），执行后期处理逻辑。 1234567891011121314Object counter = new Object();ReferenceQueue refQueue = new ReferenceQueue&lt;&gt;();PhantomReference&lt;Object&gt; p = new PhantomReference&lt;&gt;(counter, refQueue);counter = null;Sysem.gc();try &#123; // Remove是一个阻塞方法，可以指定timeout，或者选择一直阻塞 Reference&lt;Object&gt; ref = refQueue.remove(1000L); if (ref != null) &#123; // do something &#125;&#125; catch (InterruptedException e) &#123; // Handle it&#125;]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Reference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM参数]]></title>
    <url>%2F2018%2F11%2F03%2FJVM%2FJVM%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Java HotSpot VM Options文档官网的这份文档有点老了不知道新的在哪儿，可以做参考 内存和GC-XX:MaxDirectMemorySize 堆外内存的最大值 -XX:MetaspaceSize=128m Metaspace初始大小 第一次扩张会造成JVM停顿，spring aop后类比较多 -XX:MaxMetaspaceSize=512m Metaspace最大大小 设一个更大的Max值以求保险，防止将内存用光 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly 内存达到75%主动CMS GC -XX:+ExplicitGCInvokesConcurrent 让full gc时使用CMS算法，不是全程停顿 性能 -XX:AutoBoxCacheMax=20000 加大Integer Cache 自动装箱时缓冲中有的直接从缓存拿 运维-Xloggc:/dev/shm/gc-myapp.log -XX:+PrintGCDateStamps -XX:+PrintGCDetails 打印gc日志 -XX:+PrintCommandLineFlags 将每次启动的参数输出到stdout -XX:-OmitStackTraceInFastThrow 输出完整栈的日志 -XX:ErrorFile =${LOGDIR}/hs_err_%p.log JVM crash时，hotspot 会生成一个error文件，提供JVM状态信息的细节。如前所述，将其输出到固定目录，避免到时会到处找这文件。文件名中的%p会被自动替换为应用的PID -XX:+HeapDumpOnOutOfMemoryError 在OOM时，输出一个dump.core文件，记录当时的堆内存快照 对内存快照目录设置-XX:HeapDumpPath=${LOGDIR}/ -XX:+PrintGCApplicationStoppedTime 打印清晰的完整的GC停顿时间外，还可以打印其他的JVM停顿时间，比如取消偏向锁，class 被agent redefine，code deoptimization等等 -XX:+PrintPromotionFailure 多大的新生代对象晋升到老生代失败从而引发Full GC的 -XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1 -XX:+UnlockDiagnosticVMOptions -XX:-DisplayVMOutput -XX:+LogVMOutput -XX:LogFile=/dev/shm/vm-myapp.log 开启安全点日志 Option and Default Value Description -Xms 初始堆大小 -Xmx 最大堆大小 -Xss 线程占用栈空间大小，默认1m（以前是256k），可以适当调小，节约空间开启更多的线程 -Xmn 新生代的大小 JDK默认新生代占堆大小的1/3，调大可以让对象尽量在新生代被回收掉，不进入老年代 Behavioral Options Option and Default Value Description -XX:-AllowUserSignalHandlers Do not complain if the application installs signal handlers. (Relevant to Solaris and Linux only.) -XX:AltStackSize=16384 Alternate signal stack size (in Kbytes). (Relevant to Solaris only, removed from 5.0.) -XX:-DisableExplicitGC By default calls to System.gc() are enabled (-XX:-DisableExplicitGC). Use -XX:+DisableExplicitGC to disable calls to System.gc(). Note that the JVM still performs garbage collection when necessary. -XX:+FailOverToOldVerifier Fail over to old verifier when the new type checker fails. (Introduced in 6.) -XX:+HandlePromotionFailure The youngest generation collection does not require a guarantee of full promotion of all live objects. (Introduced in 1.4.2 update 11) [5.0 and earlier: false.] -XX:+MaxFDLimit Bump the number of file descriptors to max. (Relevant to Solaris only.) -XX:PreBlockSpin=10 Spin count variable for use with -XX:+UseSpinning. Controls the maximum spin iterations allowed before entering operating system thread synchronization code. (Introduced in 1.4.2.) -XX:-RelaxAccessControlCheck Relax the access control checks in the verifier. (Introduced in 6.) -XX:+ScavengeBeforeFullGC Do young generation GC prior to a full GC. (Introduced in 1.4.1.) -XX:+UseAltSigs Use alternate signals instead of SIGUSR1 and SIGUSR2 for VM internal signals. (Introduced in 1.3.1 update 9, 1.4.1. Relevant to Solaris only.) -XX:+UseBoundThreads Bind user level threads to kernel threads. (Relevant to Solaris only.) -XX:-UseConcMarkSweepGC Use concurrent mark-sweep collection for the old generation. (Introduced in 1.4.1) -XX:+UseGCOverheadLimit Use a policy that limits the proportion of the VM’s time that is spent in GC before an OutOfMemory error is thrown. (Introduced in 6.) -XX:+UseLWPSynchronization Use LWP-based instead of thread based synchronization. (Introduced in 1.4.0. Relevant to Solaris only.) -XX:-UseParallelGC Use parallel garbage collection for scavenges. (Introduced in 1.4.1) -XX:-UseParallelOldGC Use parallel garbage collection for the full collections. Enabling this option automatically sets -XX:+UseParallelGC. (Introduced in 5.0 update 6.) -XX:-UseSerialGC Use serial garbage collection. (Introduced in 5.0.) -XX:-UseSpinning Enable naive spinning on Java monitor before entering operating system thread synchronizaton code. (Relevant to 1.4.2 and 5.0 only.) [1.4.2, multi-processor Windows platforms: true] -XX:+UseTLAB Use thread-local object allocation (Introduced in 1.4.0, known as UseTLE prior to that.) [1.4.2 and earlier, x86 or with -client: false] -XX:+UseSplitVerifier Use the new type checker with StackMapTable attributes. (Introduced in 5.0.)[5.0: false] -XX:+UseThreadPriorities Use native thread priorities. -XX:+UseVMInterruptibleIO Thread interrupt before or with EINTR for I/O operations results in OS_INTRPT. (Introduced in 6. Relevant to Solaris only.) Garbage First (G1) Garbage Collection Options Option and Default Value Description -XX:+UseG1GC Use the Garbage First (G1) Collector -XX:MaxGCPauseMillis=n Sets a target for the maximum GC pause time. This is a soft goal, and the JVM will make its best effort to achieve it. -XX:InitiatingHeapOccupancyPercent=n Percentage of the (entire) heap occupancy to start a concurrent GC cycle. It is used by GCs that trigger a concurrent GC cycle based on the occupancy of the entire heap, not just one of the generations (e.g., G1). A value of 0 denotes ‘do constant GC cycles’. The default value is 45. -XX:NewRatio=n 老年代/新生代. 默认2. -XX:SurvivorRatio=n eden/survivor值. 默认8. -XX:MaxTenuringThreshold=n 对象在Survivor区最多熬过多少次Young GC后晋升到年老代，调大让对象在新生代多存活几次，默认15 -XX:ParallelGCThreads=n Sets the number of threads used during parallel phases of the garbage collectors. The default value varies with the platform on which the JVM is running. -XX:ConcGCThreads=n Number of threads concurrent garbage collectors will use. The default value varies with the platform on which the JVM is running. -XX:G1ReservePercent=n Sets the amount of heap that is reserved as a false ceiling to reduce the possibility of promotion failure. The default value is 10. -XX:G1HeapRegionSize=n With G1 the Java heap is subdivided into uniformly sized regions. This sets the size of the individual sub-divisions. The default value of this parameter is determined ergonomically based upon heap size. The minimum value is 1Mb and the maximum value is 32Mb. Performance Options Option and Default Value Description -XX:+AggressiveOpts Turn on point performance compiler optimizations that are expected to be default in upcoming releases. (Introduced in 5.0 update 6.) -XX:CompileThreshold=10000 Number of method invocations/branches before compiling [-client: 1,500] -XX:LargePageSizeInBytes=4m Sets the large page size used for the Java heap. (Introduced in 1.4.0 update 1.) [amd64: 2m.] -XX:MaxHeapFreeRatio=70 GC后，如果发现空闲堆内存大于70%时，则收缩堆内存的最大值 -XX:MaxNewSize=size Maximum size of new generation (in bytes). Since 1.4, MaxNewSize is computed as a function of NewRatio. [1.3.1 Sparc: 32m; 1.3.1 x86: 2.5m.] -XX:MinHeapFreeRatio=40 GC后，如果发现空闲堆内存小于40%时，则放大堆内存的最大值，但不超过固定最大值 -XX:NewRatio=2 Ratio of old/new generation sizes. [Sparc -client: 8; x86 -server: 8; x86 -client: 12.]-client: 4 (1.3) 8 (1.3.1+), x86: 12] -XX:NewSize=2m Default size of new generation (in bytes) [5.0 and newer: 64 bit VMs are scaled 30% larger; x86: 1m; x86, 5.0 and older: 640k] -XX:ReservedCodeCacheSize=32m Reserved code cache size (in bytes) - maximum code cache size. [Solaris 64-bit, amd64, and -server x86: 2048m; in 1.5.0_06 and earlier, Solaris 64-bit and amd64: 1024m.] -XX:SurvivorRatio=8 Ratio of eden/survivor space size [Solaris amd64: 6; Sparc in 1.3.1: 25; other Solaris platforms in 5.0 and earlier: 32] -XX:TargetSurvivorRatio=50 Desired percentage of survivor space used after scavenge. -XX:ThreadStackSize=512 Thread Stack Size (in Kbytes). (0 means use default stack size) [Sparc: 512; Solaris x86: 320 (was 256 prior in 5.0 and earlier); Sparc 64 bit: 1024; Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.] -XX:-UseBiasedLocking 取消偏向锁，大量多线程并发，锁会从偏向所升级，取消反而有性能提升 -XX:+UseFastAccessorMethods Use optimized versions of GetField. -XX:-UseISM Use Intimate Shared Memory. [Not accepted for non-Solaris platforms.] For details, see Intimate Shared Memory. -XX:+UseLargePages Use large page memory. (Introduced in 5.0 update 5.) For details, see Java Support for Large Memory Pages. -XX:+UseMPSS Use Multiple Page Size Support w/4mb pages for the heap. Do not use with ISM as this replaces the need for ISM. (Introduced in 1.4.0 update 1, Relevant to Solaris 9 and newer.) [1.4.1 and earlier: false] -XX:+UseStringCache Enables caching of commonly allocated strings. -XX:AllocatePrefetchLines=1 Number of cache lines to load after the last object allocation using prefetch instructions generated in JIT compiled code. Default values are 1 if the last allocated object was an instance and 3 if it was an array. -XX:AllocatePrefetchStyle=1 Generated code style for prefetch instructions. 0 - no prefetch instructions are generated, 1 - execute prefetch instructions after each allocation, 2 - use TLAB allocation watermark pointer to gate when prefetch instructions are executed. -XX:+UseCompressedStrings Use a byte[] for Strings which can be represented as pure ASCII. (Introduced in Java 6 Update 21 Performance Release) -XX:+OptimizeStringConcat Optimize String concatenation operations where possible. (Introduced in Java 6 Update 20) Debugging Options Option and Default Value Description -XX:-CITime Prints time spent in JIT Compiler. (Introduced in 1.4.0.) -XX:ErrorFile=./hs_err_pid.log If an error occurs, save the error data to this file. (Introduced in 6.) -XX:-ExtendedDTraceProbes Enable performance-impacting dtrace probes. (Introduced in 6. Relevant to Solaris only.) -XX:HeapDumpPath=./java_pid.hprof 设置内存快照目录 -XX:+HeapDumpOnOutOfMemoryError 在OOM时，输出一个dump.core文件，记录当时堆内存快照 -XX:OnError=”;“ Run user-defined commands on fatal error. (Introduced in 1.4.2 update 9.) -XX:OnOutOfMemoryError=”; “ Run user-defined commands when an OutOfMemoryError is first thrown. (Introduced in 1.4.2 update 12, 6) -XX:-PrintClassHistogram Print a histogram of class instances on Ctrl-Break. Manageable. (Introduced in 1.4.2.) The jmap -histocommand provides equivalent functionality. -XX:-PrintConcurrentLocks Print java.util.concurrent locks in Ctrl-Break thread dump. Manageable. (Introduced in 6.) The jstack -lcommand provides equivalent functionality. -XX:-PrintCommandLineFlags Print flags that appeared on the command line. (Introduced in 5.0.) -XX:-PrintCompilation Print message when a method is compiled. -XX:-PrintGC Print messages at garbage collection. Manageable. -XX:-PrintGCDetails Print more details at garbage collection. Manageable. (Introduced in 1.4.0.) -XX:-PrintGCTimeStamps Print timestamps at garbage collection. Manageable(Introduced in 1.4.0.) -XX:-PrintTenuringDistribution 查看survivor区对象大部分多少次进老年代 -XX:-PrintAdaptiveSizePolicy Enables printing of information about adaptive generation sizing. -XX:-TraceClassLoading Trace loading of classes. -XX:-TraceClassLoadingPreorder Trace all classes loaded in order referenced (not loaded). (Introduced in 1.4.2.) -XX:-TraceClassResolution Trace constant pool resolutions. (Introduced in 1.4.2.) -XX:-TraceClassUnloading Trace unloading of classes. -XX:-TraceLoaderConstraints Trace recording of loader constraints. (Introduced in 6.) -XX:+PerfDataSaveToFile Saves jvmstat binary data on exit. -XX:ParallelGCThreads=n Sets the number of garbage collection threads in the young and old parallel garbage collectors. The default value varies with the platform on which the JVM is running. -XX:+UseCompressedOops Enables the use of compressed pointers (object references represented as 32 bit offsets instead of 64-bit pointers) for optimized 64-bit performance with Java heap sizes less than 32gb. -XX:+AlwaysPreTouch 为了避免多次内存分配的开销，让HotSpot VM在commit内存时跑个循环来强制保证申请的内存真的commit了 -XX:AllocatePrefetchDistance=n Sets the prefetch distance for object allocation. Memory about to be written with the value of new objects is prefetched into cache at this distance (in bytes) beyond the address of the last allocated object. Each Java thread has its own allocation point. The default value varies with the platform on which the JVM is running. -XX:InlineSmallCode=n Inline a previously compiled method only if its generated native code size is less than this. The default value varies with the platform on which the JVM is running. -XX:MaxInlineSize=35 Maximum bytecode size of a method to be inlined. -XX:FreqInlineSize=n Maximum bytecode size of a frequently executed method to be inlined. The default value varies with the platform on which the JVM is running. -XX:LoopUnrollLimit=n Unroll loop bodies with server compiler intermediate representation node count less than this value. The limit used by the server compiler is a function of this value, not the actual value. The default value varies with the platform on which the JVM is running. -XX:InitialTenuringThreshold=7 Sets the initial tenuring threshold for use in adaptive GC sizing in the parallel young collector. The tenuring threshold is the number of times an object survives a young collection before being promoted to the old, or tenured, generation. -XX:MaxTenuringThreshold=n Sets the maximum tenuring threshold for use in adaptive GC sizing. The current largest value is 15. The default value is 15 for the parallel collector and is 4 for CMS. -Xloggc: Log GC verbose output to specified file. The verbose output is controlled by the normal verbose GC flags. -XX:-UseGCLogFileRotation Enabled GC log rotation, requires -Xloggc. -XX:NumberOfGClogFiles=1 Set the number of files to use when rotating logs, must be &gt;= 1. The rotated log files will use the following naming scheme, .0, .1, …, .n-1. -XX:GCLogFileSize=8K The size of the log file at which point the log will be rotated, must be &gt;= 8K.]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC日志]]></title>
    <url>%2F2018%2F11%2F03%2FJVM%2FGC%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[垃圾收集器长时间停顿，表现在 Web 页面上可能是页面响应码 500 之类的服务器错误问题，如果是个支付过程可能会导致支付失败，将造成公司的直接经济损失。 MetaSpace内存溢出JDK8 使用 MetaSpace 来保存类加载之后的类信息，字符串常量池也被移动到 Java 堆 JDK 8 中将类信息移到到了本地堆内存(Native Heap)中，将原有的永久代移动到了本地堆中成为 MetaSpace ,如果不指定该区域的大小，JVM 将会动态的调整。 可以使用 -XX:MaxMetaspaceSize=10M 来限制最大元数据。这样当不停的创建类时将会占满该区域并出现 OOM 动态代理对象太多也会 oom动态代理生成的对象在Jvm中指向的不是同一个地址，它只是与源对象有相同的hashcode值而已 CMS (concurrent mode failure) 老年代碎片化严重，无法容纳新生代提升上来的大对象 新生代来不及回收，老年代被用完 发送这种情况，应用线程将会全部停止（相当于网站这段时间无法响应用户请求），进行压缩式垃圾收集（回退到 Serial Old 算法） 解决办法： 新生代提升过快问题：（1）如果频率太快的话，说明空间不足，首先可以尝试调大新生代空间和晋升阈值。（2）如果内存有限，可以设置 CMS 垃圾收集在老年代占比达到多少时启动来减少问题发生频率（越早启动问题发生频率越低，但是会降低吞吐量，具体得多调整几次找到平衡点），参数如下：如果没有第二个参数，会随着 JVM 动态调节 CMS 启动时间 -XX:CMSInitiatingOccupancyFraction=68 （默认是 68） -XX:+UseCMSInitiatingOccupancyOnly 老年代碎片严重问题：（1）如果频率太快或者 Full GC 后空间释放不多的话，说明空间不足，首先可以尝试调大老年代空间（2）如果内存不足，可以设置进行 n 次 CMS 后进行一次压缩式 Full GC，参数如下： -XX:+UseCMSCompactAtFullCollection：允许在 Full GC 时，启用压缩式 GC -XX:CMSFullGCBeforeCompaction=n 在进行 n 次，CMS 后，进行一次压缩的 Full GC，用以减少 CMS 产生的碎片 CMS (promotion failed)在 Minor GC 过程中，Survivor Unused 可能不足以容纳 Eden 和另一个 Survivor 中的存活对象， 那么多余的将被移到老年代， 称为过早提升（Premature Promotion）。 这会导致老年代中短期存活对象的增长， 可能会引发严重的性能问题。 再进一步， 如果老年代满了， Minor GC 后会进行 Full GC， 这将导致遍历整个堆， 称为提升失败（Promotion Failure）。 提升失败日志： 提升失败原因：Minor GC 时发现 Survivor 空间放不下，而老年代的空闲也不够 新生代提升太快 老年代碎片太多，放不下大对象提升（表现为老年代还有很多空间但是，出现了 promotion failed） 解决方法：是调整年轻代和年老代的比例，还有CMSGC的时机 ​ 两条和上面 concurrent mode failure 一样 ​ 另一条，是因为 Survivor Unused 不足，那么可以尝试调大 Survivor 来尝试下 三. 在 GC 的时候其他系统活动影响 有些时候系统活动诸如内存换入换出（vmstat）、网络活动（netstat）、I/O （iostat）在 GC 过程中发生会使 GC 时间变长。 前提是你的服务器上是有 SWAP 区域（用 top、 vmstat 等命令可以看出）用于内存的换入换出，那么操作系统可能会将 JVM 中不活跃的内存页换到 SWAP 区域用以释放内存给线程使用（这也透露出内存开始不够用了）。内存换入换出是一个开销巨大的磁盘操作，比内存访问慢好几个数量级。 看一段 GC 日志：耗时 29.47 秒 再看看此时的 vmstat 命令中 si、so 列的数值，如果数值大说明换入换出严重，这是内存不足的表现。 解决方法：减少线程，这样可以降低内存换入换出；增加内存；如果是 JVM 内存设置过大导致线程所用内存不足，则适当调低 -Xmx 和 -Xms。 五. 总结 ​ 长时间停顿问题的排查及解决首先需要一定的信息和方法论： 详细的 GC 日志 借助 Linux 平台下的 iostat、vmstat、netstat、mpstat 等命令监控系统情况 查看 GC 日志中是否出现了上述的典型内存异常问题（promotion failed, concurrent mode failure），整体来说把上述两个典型内存异常情况控制在可接受的发生频率即可，对 CMS 碎片问题来说杜绝以上问题似乎不太可能，只能靠 G1 来解决了 是不是 JVM 本身的 bug 导致的 如果程序没问题，参数调了几次还是不能解决，可能说明流量太大，需要加机器把压力分散到更多 JVM 上 gc常见错误java.lang.OutOfMemoryError: Java heap space 原因：Heap内存溢出，意味着Young和Old generation的内存不够。 解决：调整java启动参数-Xms -Xmx 来增加Heap内存。 java.lang.OutOfMemoryError: unable to create new native thread 原因：Stack空间不足以创建额外的线程，要么是创建的线程过多，要么是Stack空间确实小了。 解决：由于JVM没有提供参数设置总的stack空间大小，但可以设置单个线程栈的大小；而系统的用户空间一共是3G，除了Text/Data/BSS /MemoryMapping几个段之外，Heap和Stack空间的总量有限，是此消彼长的。因此遇到这个错误，可以通过两个途径解决：1.通过 -Xss启动参数减少单个线程栈大小，这样便能开更多线程（当然不能太小，太小会出现StackOverflowError）；2.通过-Xms -Xmx 两参数减少Heap大小，将内存让给Stack（前提是保证Heap空间够用）。 java.lang.OutOfMemoryError: Requested array size exceeds VM limit 原因：这个错误比较少见（试着new一个长度1亿的数组看看），同样是由于Heap空间不足。如果需要new一个如此之大的数组，程序逻辑多半是不合理的。 解决：修改程序逻辑吧。或者也可以通过-Xmx来增大堆内存。 在GC花费了大量时间，却仅回收了少量内存时，也会报出OutOfMemoryError ，我只遇到过一两次。当使用-XX:+UseParallelGC或-XX:+UseConcMarkSweepGC收集器时，在上述情况下会报错，在 HotSpot GC Turning文档 上有说明： The parallel(concurrent) collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. 对这个问题，一是需要进行GC turning，二是需要优化程序逻辑。 java.lang.StackOverflowError 原因：这也内存溢出错误的一种，即线程栈的溢出，要么是方法调用层次过多（比如存在无限递归调用），要么是线程栈太小。 解决：优化程序设计，减少方法调用层次；调整-Xss参数增加线程栈大小。 IOException: Too many open files 原因： 这个是由于TCP connections 的buffer 大小不够用了。 java.lang.OutOfMemoryError:Direct buffer memory 解决：调整-XX:MaxDirectMemorySize=]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾回收]]></title>
    <url>%2F2018%2F11%2F03%2FJVM%2F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[判断是否可回收引用计数算法 给对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收，JVM不使用 可达性算法 通过 GC Roots 作为起始点进行搜索，能够到达到的对象都是存活的，不可达的对象可被回收。 在 Java 中 GC Roots 一般包含以下内容： 虚拟机栈中局部变量表中引用的对象 本级方法栈(Native方法)引用的对象 方法区中类静态变量引用的对象 方法区中的常量引用的对象 方法区回收 主要是对常量池的回收和对类的卸载。在大量使用反射、动态代理、CGLib 等 ByteCode 框架、动态生成 JSP 以及 OSGi 这类频繁自定义 ClassLoader 的场景都需要虚拟机具备类卸载功能，以保证不会出现内存溢出。类的卸载条件很多，需要满足以下三个条件，并且满足了也不一定会被卸载： 该类所有的实例都已经被回收，也就是堆中不存在该类的任何实例 加载该类的 ClassLoader 已经被回收 该类对应的 Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法 垃圾收集算法标记 - 清除标记存活的对象，将未被标记的对象清除 缺点： 标记和清除过程效率都不高 会产生大量不连续的内存碎片，导致无法给大对象分配内存 复制算法将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理 JVM新生代中是这么做的：新生代分为一块较大的 Eden区和两块较小的Survivor区，每次使用Eden区和其中一块Survivor区。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor 标记 - 整理将存活的对象集中起来，使其内存连续，将边界以外的内存清除 新生代使用：复制算法 老年代使用：标记 - 清除 或者 标记 - 整理 垃圾收集器 SerialClient 模式下的默认新生代收集器，单线程的收集器，优点是简单高效，对于单个 CPU 环境来说，由于没有线程交互的开销，因此拥有最高的单线程收集效率。缺点是回收时会将正在执行的线程暂停。适用于单CPU、新生代空间较小及对暂停时间要求不是非常高的应用上 Parallel Scavenge吞吐量 = 运行用户代码时间 / （运行用户代码时间 + 垃圾收集时间）-XX:MaxGCPauseMillis=n 控制最大垃圾收集停顿时间-XX:GCTimeRatio=n 设置吞吐量大小的垃圾收集时间占总时间的比率，设置为19 最大gc时间就占总的1/20-XX:UseAdaptiveSizePolicy GC Ergonomics 动态调整java堆中各个区域的大小和年龄 多线程收集器。其它收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户代码的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互的任务。 缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 可以通过一个开关参数打开 GC 自适应的调节策略（GC Ergonomics），就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。 ParNew-XX:ParallelGCThreads Server 模式下的虚拟机首选新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合工作 默认开启的线程数量与 CPU 数量相同，可以使用 -XX:ParallelGCThreads 参数来设置线程数 在整个扫描和复制过程采用多线程的方式来进行，适用于多CPU、对暂停时间要求较短的应用上，用-XX:+UseParallelGC来强制指定，用-XX:ParallelGCThreads=4来指定线程数 Serial OldCMS备用预案 Concurrent Mode Failusre时使用标记-整理算法 Serial收集器的老年代版本，它同样使用一个单线程执行收集，基于标记整理法，作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用 Parallel Old标记-整理算法 Parallel Scavenge收集器的老年代版本，使用多线程和标记整理法，在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器，可以用在注重吞吐量和CPU资源敏感的场合,UsePallelOldGC打开 CMS标记-清除算法减少回收停顿时间碎片 -XX:CMSInitiatingOccupancyFraction 被使用多少后触发垃圾收集，提高cms触发百分比Concurrent Mode Failure 启用Serial Old 123-XX:+UseCMSCompactAtFullCollection fullgc时开启内存碎片合并整理-XX:CMSFullGCsBeforeCompaction 执行多少次不压缩FullGC后 来一次带压缩的 0 表示每次都压-XX:+UseConcMarkSweep Concurrent Mark Sweep，基于标记清除法。目标是解决Serial GC 的停顿问题，以达到最短回收时间，有高并发、高响应的特点 初始标记仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿 并发标记(CMS concurrenr mark) 进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿 重新标记(CMS remark) 为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿 并发清除(CMS concurrent sweep)不需要停顿 缺点： 产生大量碎片，大内存找不到连续的空间 会full gc 通过-XX:CMSFullGCBeforeCompaction参数设置执行多少次不压缩的Full GC之后，跟着来一次碎片整理，默认为0，即每次Full GC都对老生代进行碎片整理压缩。Full GC 不同于 老生代75%时触发的CMS GC，只在老生代达到100%，堆外内存满，老生代碎片过大无法分配空间给新晋升的大对象这些特殊情况里发生，所以设为每次都进行碎片整理是合适的 无法清除浮动垃圾 在默认设置下，CMS收集器在老年代使用了68%的空间时就会被激活，也可以通过参数-XX:CMSInitiatingOccupancyFraction的值来提供触发百分比。可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 占用cpu资源 导致程序变慢，吞吐量下降 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高 G1-XX:+UseG1GC 使用G1垃圾收集器 面向Server的垃圾收集器，相比CMS有不少改进，在多 CPU 和大内存的场景下有很好的性能。G1 可以直接对新生代和老年代一起回收 。优点： 整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片 能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒 通过引入 Region （区域）的概念，将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。通过记录每个 Region 垃圾回收时间以及回收所获得的空间，并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 收集器 串行/并行/并发 新生代/老年代 算法 优先目标 适用场景 Serial 串行 新生代 复制算法 响应速度 单CPU的Client模式 Serial Old 串行 老年代 标记-整理 响应速度 单CPU的Client模式、CMS的后备预案 ParNew 并行 新生代 复制算法 响应速度 多CPU时在Server模式下与CMS配合 Parallel Scavenge 并行 新生代 复制算法 吞吐量 在后台运算而不需要太多交互的任务 Parallel Old 并行 老年代 标记-整理 吞吐量 在后台运算而不需要太多交互的任务 CMS 并发 老年代 标记-清除 响应速度 集中在互联网站或B/S系统服务端上的Java应用 G1 并发 新生代和老年代 标记-整理+复制算法 响应速度 面向服务端应用，将来替换CMS 串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序 并行指的是垃圾收集器和用户程序同时执行 内存分配策略 对象优先在eden区分配内存，eden区空间不够时出发minor gc 大对象直接进入老年代 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 区和 Survivor 区之间的大量内存复制 长期存活的进入老年代在新生代多次gc存活下来的进入老年代 -XX:MaxTenuringThreshold 用来定义经过多少次minor gc还存活后进入老年代 动态对象年龄判定当 Survivor 中相同年龄所有对象大小的总和&gt; Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需经过 MaxTenuringThreshold 次gc 空间分配担保在 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 设置不允许冒险，那么就要进行一次 Full GC TLAB thread local allcationnnotion buffer 在eden区 每隔线程都有自己的 TLAB thread local allaction buffer 触发条件minor gc当 eden 空间满时，就将触发一次 minor gc，清理eden区 major gc是清理老年代 full gc清理整个堆空间—包括年轻代和老年代 空间分配担保失败触发full gc minor gc之前检查 老年代最大可用连续空间是否&gt;新生代所有对象总空间 调用System.gc时，系统建议执行Full GC，但是不必然执行 老年代空间不足 大对象直接进入老年代，长期存活的对象进入老年代，老年代没有足够大小的连续内存空间，触发full gc 方法区空间不足 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 如何查看当前的垃圾回收器​ -XX:+PrintFlagsFinal​ -XX:+PrintCommandLineFlags​ server client​ MBean GC日志​ 1.输出日志​ -XX:+PrintGCTimeStamps​ -XX:+PrintGCDetails​ -Xloggc:/home/administrator/james/gc.log​ -XX:+PrintHeapAtGC​ 2.日志文件控制​ -XX:-UseGCLogFileRotation​ -XX:GCLogFileSize=8K​ 3.怎么看 JDK自带的 监控工具https://docs.oracle.com/javase/8/docs/technotes/tools/windows/toc.html​ jmap -heap pid 堆使用情况​ jstat -gcutil pid 1000​ jstack 线程dump​ jvisualvm​ jconsole MAT​ http://help.eclipse.org/oxygen/index.jsp?topic=/org.eclipse.mat.ui.help/welcome.html​ -XX:+HeapDumpOnOutOfMemoryError​ -XX:HeapDumpPath=/home/administrator/james/error.hprof 怀疑：​ 1.看GC日志 126719K-&gt;126719K(126720K)​ 2.dump​ 3.MAT​ 1.占用Retained Heap​ 2.看有没有GC Root指向 什么条件触发STW的Full GC呢？Perm空间不足；CMS GC时出现promotion failed和concurrent mode failure（concurrent mode failure发生的原因一般是CMS正在进行，但是由于老年代空间不足，需要尽快回收老年代里面的不再被使用的对象，这时停止所有的线程，同时终止CMS，直接进行Serial Old GC）；（promontion faild产生的原因是EDEN空间不足的情况下将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc ） 统计得到的Young GC晋升到老年代的平均大小大于老年代的剩余空间； 主动触发Full GC（执行jmap -histo:live [pid]）来避免碎片问题。​​java -Xms8m -Xmx64m -verbose:gc -Xloggc:/home/administrator/james/gc.log -XX:+PrintHeapAtGC -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCTimeStamps -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XX:+PrintGCDetails -XX:+UseConcMarkSweepGC -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9004 -Djava.rmi.server.hostname=177.1.1.122 -jar jvm-demo1-0.0.1-SNAPSHOT.jar &gt; catalina.out 2&gt;&amp;1 &amp; java -Xms128m -Xmx128m -verbose:gc -Xloggc:/home/administrator/james/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/administrator/james/error.hprof -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCTimeStamps -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XX:+PrintGCDetails -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9004 -Djava.rmi.server.hostname=177.1.1.122 -jar jvm-demo1-0.0.1-SNAPSHOT.jar &gt; catalina.out 2&gt;&amp;1 &amp; java -Xms128m -Xmx128m -verbose:gc -Xloggc:/home/administrator/james/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:+PrintHeapAtGC -XX:HeapDumpPath=/home/administrator/james/error.hprof -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCTimeStamps -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XX:+PrintGCDetails -XX:+UseCMSCompactAtFullCollection -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9004 -Djava.rmi.server.hostname=177.1.1.122 -jar jvm-demo1-0.0.1-SNAPSHOT.jar &gt; catalina.out 2&gt;&amp;1 &amp; -XX:+CMSScavengeBeforeRemark]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM组成]]></title>
    <url>%2F2018%2F11%2F03%2FJVM%2FJVM%E7%BB%84%E6%88%90%2F</url>
    <content type="text"><![CDATA[方法区和堆内存是线程共享的。程序计数器、虚拟机栈、本地方法栈是线程私有的 方法区存放已经被JVM加载的类的信息，如常量，静态变量、即时编译器编译后的代码等。 从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中，1.8 metaspace 放类加载信息 堆内存 所有对象的创建都在这里进行分配，采取分代管理，分为新生代和老年代，执行不同的垃圾回收策略，所有实例域，静态域和数组元素都是放在堆内存中。常量池在堆中 程序计数器指向当前线程执行的字节码行号，多线程切换时可以知道上一次运行的状态和位置 虚拟机栈 由一个个栈帧组成，每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用、方法的调用返回等信息，每创建一个栈帧压栈，当一个方法执行完毕之后则出栈 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常 本地方法栈调用Native Method 直接内存在 JDK 1.4 中新加入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存（Native 堆），然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream流]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2FStream%E6%B5%81%2F</url>
    <content type="text"><![CDATA[StreamJava8的新增的Stream API（java.util.stream）将生成环境的函数式编程引入了Java库中。这是目前为止最大的一次对Java库的完善，以便开发者能够写出更加有效、更加简洁和紧凑的代码。Steam API极大得简化了集合操作，Stream流配合Lambda表达式使用起来真是美滋滋，steam的另一个价值是创造性地支持并行处理。 优点：性能高，灵活，表达清楚 方法api分为两大类，中间操作相当于流水线，能对流水线上的东西进行各种处理，不会消耗流；终端操作消耗流，不能再对流进行操作 中间操作(流水线) filter 过滤 map 抽取流 limit 限制个数 skip 跳过个数 sorted 排序 distinct 去重 flagmap 合并到一起 终端操作(消耗流) collect 收集 foreach 遍历 count 计数 anyMatch 至少匹配1个 allMatch 匹配所有 noneMatch 没有元素匹配 findAny 返回流中任意元素 reduce 举例12345678910111213141516171819202122232425import java.util.ArrayList;import java.util.Comparator;import java.util.List;import java.util.Map;import java.util.stream.Collectors;public class StreamTest &#123; public static void main(String[] args) &#123; List&lt;Apple&gt; appleList=new ArrayList&lt;&gt;(); appleList.parallelStream().filter((e)-&gt;e.getWeight()&gt;50) // 筛选重量&gt;50的苹果 .sorted(Comparator.comparing(Apple::getColor)) // 根据苹果的颜色进行排序 .map(Apple::getWeight) // 抽取苹果中的重量 .limit(3) // 限定选3个 .distinct() // 去掉相同的元素 .collect(Collectors.toList()); // 转化成list appleList.forEach(System.out::println); // 打印每个苹果 appleList.stream().count(); // 计算苹果的总个数 // 对list根据苹果的颜色进行分组 Map&lt;String,List&lt;Apple&gt;&gt; map=appleList.parallelStream().collect(Collectors.groupingBy(Apple::getColor)); &#125;&#125; flagmap合并流 123456789101112public class StreamTest &#123; public static void main(String[] args) &#123; List&lt;String&gt; names = new ArrayList&lt;&gt;(); names.add("hello"); names.add("world"); List&lt;String&gt; a=names.stream().map((String e)-&gt;e.split("")) .flatMap(Arrays::stream) .collect(Collectors.toList());// 将两个单词，拆成单个字母并存为list System.out.println(a.get(9)); &#125;&#125; reduce 123456789101112public class StreamTest &#123; public static void main(String[] args) &#123; List&lt;String&gt; names = new ArrayList&lt;&gt;(); names.add("hello"); names.add("world"); String newstr=names.stream().map((String e)-&gt;e.split("")) .flatMap(Arrays::stream) .reduce("",String::concat);// 拆成字母后，拼接成一个新的字符串，相当于.reduce("",(String a ,String b)-&gt;a+b); System.out.println(newstr); &#125;&#125; collect 123List&lt;Apple&gt; appleList=new ArrayList&lt;&gt;();appleList.stream().collect(Collectors.averagingInt(Apple::getWeight));// 计算重量的平均值appleList.stream().map(Apple::getColor).collect(Collectors.joining());// 连接字符串 创建流的方式值创建流 1Stream&lt;String&gt; strstream=Stream.of(&quot;dasd&quot;,&quot;dsa&quot;,&quot;das&quot;); 数据创建流 12String[] nums=&#123;&quot;qwe&quot;,&quot;ee&quot;,&quot;ds&quot;&#125;;Stream&lt;String&gt; aaa=Arrays.stream(nums); 文件创建流 123456try &#123; Stream&lt;String&gt; lines=Files.lines(Paths.get(&quot;C:\\Users\\maruami\\Desktop\\readbook\\book.md&quot;), Charset.defaultCharset()); lines.forEach(System.out::println);&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 函数生成流 1Stream.generate(Math::random).limit(10).forEach(System.out::println);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lambda表达式]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2FLambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Lambda表达式Lambda 表达式，它是推动 Java 8 发布的最重要新特性。Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中），可以使代码变的更加简洁紧凑。 基本语法：1(参数列表) -&gt; &#123;代码块&#125; 需要注意： 参数类型可省略，编译器可以自己推断 如果只有一个参数，圆括号可以省略 代码块如果只是一行代码，大括号也可以省略 如果代码块是一行，且是有结果的表达式，return可以省略 注意：事实上，把Lambda表达式可以看做是匿名内部类的一种简写方式。当然，前提是这个匿名内部类对应的必须是接口，而且接口中必须只有一个函数！Lambda表达式就是直接编写函数的：参数列表、代码体、返回值等信息。 用法示例排序12345// 准备一个集合List&lt;Integer&gt; list = Arrays.asList(10, 5, 25, -15, 20);list.sort((i1,i2) -&gt; &#123; return i1 - i2;&#125;);System.out.println(list);//output [-15, 5, 10, 20, 25] 还可以简写为： 1list.sort((i1,i2) -&gt; i1 - i2); 遍历1list.forEach(i -&gt; System.out.println(i)); 赋值Lambda表达式的实质其实还是匿名内部类，所以我们其实可以把Lambda表达式赋值给某个变量。 1Runnable task = () -&gt; &#123;System.out.println("hello lambda!");&#125;; 隐式finalLambda表达式的实质其实还是匿名内部类，而匿名内部类在访问外部局部变量时，要求变量必须声明为final。不过我们在使用Lambda表达式时无需声明final，因为Lambda底层会隐式的把变量设置为final，在后续的操作中，一定不能修改该变量。 1234// 定义一个局部变量int num = -1;// 在Lambda表达式中使用局部变量num，num会被隐式声明为final，不能进行任何修改操作Runnable r = () -&gt; &#123;System.out.println(num);&#125;; 函数式接口 Lambda表达式是接口的匿名内部类的简写形式 接口必须满足：内部只有一个函数 其实这样的接口，我们称为函数式接口，我们学过的Runnable、Comparator都是函数式接口的典型代表。但是在实践中，函数接口是非常脆弱的，只要有人在接口里添加多一个方法，那么这个接口就不是函数接口了，就会导致编译失败。Java 8提供了一个特殊的注解@FunctionalInterface来克服上面提到的脆弱性并且显示地表明函数接口。而且jdk8版本中，对很多已经存在的接口都添加了@FunctionalInterface注解，例如Runnable接口，另外，Jdk8默认提供了一些函数式接口供我们使用： Function类型接口12345@FunctionalInterfacepublic interface Function&lt;T, R&gt; &#123; // 接收一个参数T，返回一个结果R R apply(T t);&#125; Function代表的是有参数，有返回值的函数。还有很多类似的Function接口： 接口名 描述 BiFunction&lt;T,U,R&gt; 接收两个T和U类型的参数，并且返回R类型结果的函数 DoubleFunction&lt;R&gt; 接收double类型参数，并且返回R类型结果的函数 IntFunction&lt;R&gt; 接收int类型参数，并且返回R类型结果的函数 LongFunction&lt;R&gt; 接收long类型参数，并且返回R类型结果的函数 ToDoubleFunction&lt;T&gt; 接收T类型参数，并且返回double类型结果 ToIntFunction&lt;T&gt; 接收T类型参数，并且返回int类型结果 ToLongFunction&lt;T&gt; 接收T类型参数，并且返回long类型结果 DoubleToIntFunction 接收double类型参数，返回int类型结果 DoubleToLongFunction 接收double类型参数，返回long类型结果 这些都是一类函数接口，在Function基础上衍生出的，要么明确了参数不确定返回结果，要么明确结果不知道参数类型，要么两者都知道。 Consumer12345@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123; // 接收T类型参数，不返回结果 void accept(T t);&#125; 具备类似的特征：那就是不返回任何结果。 Predicate12345@FunctionalInterfacepublic interface Predicate&lt;T&gt; &#123; // 接收T类型参数，返回boolean类型结果 boolean test(T t);&#125; Predicate系列参数不固定，但是返回的一定是boolean类型。 Supplier12345@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123; // 无需参数，返回一个T类型结果 T get();&#125; 不接受任何参数，返回T类型结果。 方法引用方法引用使得开发者可以将已经存在的方法作为变量来传递使用。方法引用可以和Lambda表达式配合使用。 语法 语法 描述 类名::静态方法名 类的静态方法的引用 类名::非静态方法名 类的非静态方法的引用 实例对象::非静态方法名 类的指定实例对象的非静态方法引用 类名::new 类的构造方法引用 示例首先我们编写一个集合工具类，提供一个方法： 123456789101112131415public class CollectionUtil&#123; /** * 利用function将list集合中的每一个元素转换后形成新的集合返回 * @param list 要转换的源集合 * @param function 转换元素的方式 * @param &lt;T&gt; 源集合的元素类型 * @param &lt;R&gt; 转换后的元素类型 * @return */ public static &lt;T,R&gt; List&lt;R&gt; convert(List&lt;T&gt; list, Function&lt;T,R&gt; function)&#123; List&lt;R&gt; result = new ArrayList&lt;&gt;(); list.forEach(t -&gt; result.add(function.apply(t))); return result; &#125;&#125; 可以看到这个方法接收两个参数： List&lt;T&gt; list：需要进行转换的集合 Function&lt;T,R&gt;：函数接口，接收T类型，返回R类型。用这个函数接口对list中的元素T进行转换，变为R类型 类的静态方法引用1List&lt;Integer&gt; list = Arrays.asList(1000, 2000, 3000); 我们需要把这个集合中的元素转为十六进制保存，需要调用Integer.toHexString()方法： 123public static String toHexString(int i) &#123; return toUnsignedString0(i, 4);&#125; 这个方法接收一个 i 类型，返回一个String类型，可以用来构造一个Function的函数接口： 我们先按照Lambda原始写法，传入的Lambda表达式会被编译为Function接口，接口中通过Integer.toHexString(i)对原来集合的元素进行转换： 123// 通过Lambda表达式实现List&lt;String&gt; hexList = CollectionUtil.convert(list, i -&gt; Integer.toHexString(i));System.out.println(hexList);// [3e8, 7d0, bb8] 上面的Lambda表达式代码块中，只有对Integer.toHexString()方法的引用，没有其它代码，因此我们可以直接把方法作为参数传递，由编译器帮我们处理，这就是静态方法引用： 123// 类的静态方法引用List&lt;String&gt; hexList = CollectionUtil.convert(list, Integer::toHexString);System.out.println(hexList);// [3e8, 7d0, bb8] 类的非静态方法引用接下来，我们把刚刚生成的String集合hexList中的元素都变成大写，需要借助于String类的toUpperCase()方法： 123public String toUpperCase() &#123; return toUpperCase(Locale.getDefault());&#125; 这次是非静态方法，不能用类名调用，需要用实例对象，因此与刚刚的实现有一些差别，我们接收集合中的每一个字符串s。但与上面不同然后s不是toUpperCase()的参数，而是调用者： 123// 通过Lambda表达式，接收String数据，调用toUpperCase()List&lt;String&gt; upperList = CollectionUtil.convert(hexList, s -&gt; s.toUpperCase());System.out.println(upperList);// [3E8, 7D0, BB8] 因为代码体只有对toUpperCase()的调用，所以可以把方法作为参数引用传递，依然可以简写： 123// 类的成员方法List&lt;String&gt; upperList = CollectionUtil.convert(hexList, String::toUpperCase);System.out.println(upperList);// [3E8, 7D0, BB8] 指定实例的非静态方法引用下面一个需求是这样的，我们先定义一个数字Integer num = 2000，然后用这个数字和集合中的每个数字进行比较，比较的结果放入一个新的集合。比较对象，我们可以用Integer的compareTo方法: 123public int compareTo(Integer anotherInteger) &#123; return compare(this.value, anotherInteger.value);&#125; 先用Lambda实现， 123456List&lt;Integer&gt; list = Arrays.asList(1000, 2000, 3000);// 某个对象的成员方法Integer num = 2000;List&lt;Integer&gt; compareList = CollectionUtil.convert(list, i -&gt; num.compareTo(i));System.out.println(compareList);// [1, 0, -1] 与前面类似，这里Lambda的代码块中，依然只有对num.compareTo(i)的调用，所以可以简写。但是，需要注意的是，这次方法的调用者不是集合的元素，而是一个外部的局部变量num，因此不能使用 Integer::compareTo，因为这样是无法确定方法的调用者。要指定调用者，需要用 对象::方法名的方式： 1234// 某个对象的成员方法Integer num = 2000;List&lt;Integer&gt; compareList = CollectionUtil.convert(list, num::compareTo);System.out.println(compareList);// [1, 0, -1] 构造函数引用最后一个场景：把集合中的数字作为毫秒值，构建出Date对象并放入集合，这里我们就需要用到Date的构造函数： 1234567/** * @param date the milliseconds since January 1, 1970, 00:00:00 GMT. * @see java.lang.System#currentTimeMillis() */public Date(long date) &#123; fastTime = date;&#125; 我们可以接收集合中的每个元素，然后把元素作为Date的构造函数参数： 1234// 将数值类型集合，转为Date类型List&lt;Date&gt; dateList = CollectionUtil.convert(list, i -&gt; new Date(i));// 这里遍历元素后需要打印，因此直接把println作为方法引用传递了dateList.forEach(System.out::println); 上面的Lambda表达式实现方式，代码体只有new Date()一行代码，因此也可以采用方法引用进行简写。但问题是，构造函数没有名称，我们只能用new关键字来代替： 123// 构造方法List&lt;Date&gt; dateList = CollectionUtil.convert(list, Date::new);dateList.forEach(System.out::println); 注意两点： 上面代码中的System.out::println 其实是 指定对象System.out的非静态方法println的引用 如果构造函数有多个，可能无法区分导致传递失败]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Collections]]></title>
    <url>%2F2018%2F11%2F03%2FJava%2FCollections%2F</url>
    <content type="text"><![CDATA[method description max(coll)/max(coll, comp) Collections.max(map.values(),(e1,e2)-&gt;{return e1.length()-e2.length();}); min(coll)/min(coll, comp) indexOfSubList(source, target) 返回target在source中第一次出现的位置 lastIndexOfSubList(source, target); 返回target在source中最后一次出现的位置 reverse(source) 逆置元素 rotate(list, distance) 所有元素向后移动distance个位置，末尾元素循环到前面 Collections.swap(list, 1, 2); 交换list中元素的位置 disjoint(c1,c2) 没有相同元素，返回true frequency(collection, object) 返回collection中等于object的个数]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Collections</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala入门]]></title>
    <url>%2F2018%2F11%2F02%2FScala%2FScala%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Scala是一个面向对象和函数式编程的语言，运行于JVM之上，兼容Java程序 结尾不写；和Python有点像 类型声明，推测和Go有点像 scalac xxx.scala 编译Scala文件，生成Class字节码文件 scala xxx 运行字节码文件 卧槽和Java好像 scala没有原生类型 访问修饰符与Java不同 protected 只能被子类访问 private 只能内部可见 外部类调用不了内部类的私有方法 val 和 var 区别var 变量 val 常量相当于final 基本数据类型Byte/Char Short/Int/Long/Float/Double Boolean 转换类型asInstanceOf[Double] 判断类型asInstanceOf[Double] Lazy使用lazy可以延迟加载，第一次使用时，对应的表达式才会计算 lazy var a=10]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL语句优化]]></title>
    <url>%2F2018%2F10%2F26%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FSQL%E8%AF%AD%E5%8F%A5%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[只返回必要的列，减少查询字段数，不使用select * 只返回必要的行 limit限制返回的数据， 确定只要一行数据时使用limit 1 缓存重复查询的数据 使用索引减少扫描次数 切分大查询 如果一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询，所以要切分 分解大连接查询 让缓存更高效 对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用 在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好 对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符 冷热数据分离 历史数据归档 不使用uuid md5 hash 字符串作为主键 使用分区表 逻辑上是一张表 物理上存在不同的表上（list,hash,range） 避免使用子查询 会产生大量的临时表 子查询的没有索引 合理使用join，表关联尽量用主键 where 从句不要使用函数转换或计算，会导致无法使用索引 尽量不在数据库做运算，否则无法使用索引导致全表扫描 避免前缀模糊查询 用不了索引导致全表扫描 控制表单数据量 合理分表 单库不超过300-400个表 表字段少而精 字段上限控制在20-50个 效率优先 可以适当冗余 用好数据字段类型 tinyint int bigint 字符转化为数字 数字型更高效 查询更快 占用空间小 避免使用null 字段 难以进行查询优化 null列加索引，需要额外的空间按 含null复合索引无效 字符字段必须建前缀索引 ALTER TABLE messages_messagehistory ADD KEY (messagecontent(8)) 尽量不用外键 有额外开销 高并发容易死锁 大sql拆解成多条简单sql 缓存命中高 减少锁表时间 能用上更多的cpu 保持事务连接短小 与事务无关的操作放到事务外面 避免负向查询 如 not != &lt;&gt; !&lt; !&gt; not exists not in not like 减少count(*) 资源开销大 无需对结果去重时，用union all , union有去重开销 同数据类型的列值比较 数字对数字 字符对字符 字符列与数值类型比较 字符列转成数值，不会使用索引查询 两个表join的字段 数据类型要相同]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务]]></title>
    <url>%2F2018%2F10%2F26%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[不事务隔离带来的问题更新丢失：两事务同时更新，一个失败回滚覆盖另一个事务的更新。或事务1执行更细操作，在事务1结束前事务2也更新，则事务1的更细结果被事务2的覆盖了。 脏读：事务T2读取到事务T1修改了但是还未提交的数据，之后事务T1又回滚其更新操作，导致事务T2读到的是脏数据。 不可重复读：事务T1读取某个数据后，事务T2对其做了修改，当事务T1再次读该数据时得到与前一次不同的值。 虚读（幻读）：事务T1读取在读取某范围数据时，事务T2又插入一条数据，当事务T1再次数据这个范围数据时发现不一样了，出现了一些“幻影行”。 脏读和不可重复读的区别：脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 不可重复读和幻读的异同：都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 未提交读（READ UNCOMMITTED）事务中的修改，即使没有提交，对其它事务也是可见的。使用查询语句不会加锁，可能会读到未提交的行（Dirty Read） 造成：脏读；不可重复读；幻影读 所需的锁：排他写锁 提交读（READ COMMITTED）一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）大多数数据库默认级别是RC，比如SQL Server，Oracle 造成：不可重复读；幻影读 所需的锁：排他写锁、瞬间共享读锁 可重复读（REPEATABLE READ）保证在同一个事务中多次读取同样数据的结果是一样的。多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能发生幻读（Phantom Read）MySQL InnoDB 就是这个级别 事例：程序员某一天去消费，花了2千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了2千元，就在这个时候，程序员花了1万买了一部电脑，即新增INSERT了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。解决幻读的方法是增加范围锁（range lock）或者表锁。 造成：幻影读 所需的锁：排他写锁、共享读锁 可串行化（SERIALIZABLE）强制事务串行执行。InnoDB 隐式地将全部的查询语句加上共享锁，解决了幻读的问题； 所须的锁：范围锁或表锁 MySQLMySQL 中默认的事务隔离级别就是 REPEATABLE READ，它通过 Next-Key 锁也能够在某种程度上解决幻读的问题。MVCC 会产生幻读问题（更新时异常）在可重复读隔离级别下，通过多版本并发控制（MVCC）+ 间隙锁（Next-Key Locking）防止幻影读。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Transaction</tag>
        <tag>Isolation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务]]></title>
    <url>%2F2018%2F10%2F26%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[原子性（Atomicity）事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistency）数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对一个数据的读取结果都是相同的。 隔离性（Isolation）隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 持久性（Durability）一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。可以通过数据库备份和恢复来实现，在系统发生崩溃时，使用备份的数据库进行数据恢复。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[forward与redirect]]></title>
    <url>%2F2018%2F10%2F26%2F%E5%88%86%E5%B8%83%E5%BC%8F%2Fforward%E4%B8%8Eredirect%2F</url>
    <content type="text"><![CDATA[forward（转发）服务器请求资源,服务器直接访问目标地址的URL,把那个URL的响应内容读取过来,然后把这些内容再发给浏览器.浏览器根本不知道服务器发送的内容从哪里来的,因为这个跳转过程实在服务器实现的，并不是在客户端实现的所以客户端并不知道这个跳转动作，所以它的地址栏还是原来的地址. redirect（重定向）服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址.所以地址栏显示的是新的URL. 区别转发是服务器行为，重定向是客户端行为，通过302状态码响应及对应新的location，发起第二次请求 转发页面和转发到的页面可以共享request里面的数据，重定向不能共享数据，过程中传输的信息会被丢失]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>forward</tag>
        <tag>redirect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Get与Post]]></title>
    <url>%2F2018%2F10%2F26%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FGet%E4%B8%8EPost%2F</url>
    <content type="text"><![CDATA[HTTP定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE GETGET用于信息获取，而且应该是安全的和幂等的 GET请求的数据会附在URL之后 GET方式提交的数据最多只能是1024字节 POSTPOST表示可能修改变服务器上的资源的请求 POST把提交的数据则放置在是HTTP包的包体中 理论上POST没有限制，可传较大量的数据]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Get</tag>
        <tag>Post</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP2.0]]></title>
    <url>%2F2018%2F10%2F26%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FHTTP2.0%2F</url>
    <content type="text"><![CDATA[HTTP2.0 的目的是通过支持请求与响应的多路复用来减少延迟，通过压缩HTTPS首部字段将协议开销降低，同时增加请求优先级和服务器端推送的支持。 二进制分帧层，是HTTP 2.0性能增强的核心。HTTP 1.x在应用层以纯文本的形式进行通信，而HTTP 2.0将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。这样，客户端和服务端都需要引入新的二进制编码和解码的机制。 二进制分帧层HTTP/2.0 将报文分成 HEADERS 帧和 DATA 帧，它们都是二进制格式的。 在通信过程中，只会有一个 TCP 连接存在，它承载了任意数量的双向数据流（Stream）。 一个数据流（Stream）都有一个唯一标识符和可选的优先级信息，用于承载双向信息。 消息（Message）是与逻辑请求或响应对应的完整的一系列帧。 帧（Frame）是最小的通信单位，来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。 服务端推送HTTP/2.0 在客户端请求一个资源时，会把相关的资源一起发送给客户端，客户端就不需要再次发起请求了。例如客户端请求 page.html 页面，服务端就把 script.js 和 style.css 等与之相关的资源一起发给客户端。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>HTTP2.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS]]></title>
    <url>%2F2018%2F10%2F26%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FHTTPS%2F</url>
    <content type="text"><![CDATA[超文本传输协议HTTP 有以下安全性问题： 使用明文进行通信，内容可能会被窃听； 不验证通信方的身份，通信方的身份有可能遭遇伪装； 无法证明报文的完整性，报文有可能遭篡改。 因此HTTP协议不适合传输一些敏感信息，比如信用卡号、密码等。 为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS。HTTPs 并不是新协议，而是让 HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信。SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。 通过使用 SSL，HTTPs 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）。 HTTP是超文本传输协议，信息是明文传输，HTTPS 则是具有安全性的SSL加密传输协议。 HTTP和HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 HTTP的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比HTTP协议安全。 HTTPS 采用混合的加密机制，使用非对称密钥加密用于传输对称密钥来保证传输过程的安全性，之后使用对称密钥加密进行通信来保证通信过程的效率。 Nginx 配置 HTTPS 服务器]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Session]]></title>
    <url>%2F2018%2F10%2F26%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FSession%2F</url>
    <content type="text"><![CDATA[除了可以将用户信息通过 Cookie 存储在用户浏览器中，也可以利用 Session 存储在服务器端，存储在服务器端的信息更加安全。 Session 可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中，效率会更高。 session就是一种保存上下文信息的机制，它是针对每一个用户的，变量的值保存在服务器端，通过SessionID来区分不同的客户,session是以cookie或URL重写为基础的，默认使用cookie来实现，系统会创造一个名为JSESSIONID的输出cookie，我们叫做session cookie,以区别persistent cookies,也就是我们通常所说的cookie，当我们把浏览器的cookie禁止后，web服务器会采用URL重写的方式传递Sessionid 通常session cookie是不能跨窗口使用的，当你新开了一个浏览器窗口进入相同页面时，系统会赋予你一个新的sessionid，这样我们信息共享的目的就达不到了，此时我们可以先把sessionid保存在persistent cookie中，然后在新窗口中读出来，就可以得到上一个窗口SessionID了，这样通过session cookie和persistent cookie的结合我们就实现了跨窗口的session tracking（会话跟踪）。 使用 Session 维护用户登录状态的过程如下： 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中； 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID；通过SessionID来区分不同的客户 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中； 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。 应该注意 Session ID 的安全性问题，不能让它被恶意攻击者轻易获取，那么就不能产生一个容易被猜到的 Session ID 值。此外，还需要经常重新生成 Session ID。在对安全性要求极高的场景下，例如转账等操作，除了使用 Session 管理用户状态之外，还需要对用户进行重新验证，比如重新输入密码，或者使用短信验证码等方式。 Session和Cookie对比 session保存在服务器，客户端不知道其中的信息 cookie保存在客户端，服务器能够知道其中的信息 session中保存的是对象 cookie中保存的是字符串 session不能区分路径，同一个用户在访问一个网站期间，所有的session在任何一个地方都可以访问到 cookie中如果设置了路径参数，那么同一个网站中不同路径下的cookie互相是访问不到的 session需要借助cookie才能正常工作。如果客户端完全禁止cookie，session将失效。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cookie]]></title>
    <url>%2F2018%2F10%2F26%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FCookie%2F</url>
    <content type="text"><![CDATA[HTTP 协议是无状态的，主要是为了让 HTTP 协议尽可能简单，使得它能够处理大量事务，HTTP/1.1 引入 Cookie 来保存状态信息。 Cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器之后向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器。由于之后每次请求都会需要携带 Cookie 数据，因此会带来额外的性能开销（尤其是在移动环境下）。Cookie 曾一度用于客户端数据的存储，因为当时并没有其它合适的存储办法而作为唯一的存储手段，但现在随着现代浏览器开始支持各种各样的存储方式，Cookie 渐渐被淘汰。新的浏览器 API 已经允许开发者直接将数据存储到本地，如使用 Web storage API（本地存储和会话存储）或 IndexedDB。 如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户 用途 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） 分类 会话期 Cookie：浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效。 持久性 Cookie：指定一个特定的过期时间（Expires）或有效期（max-age）之后就成为了持久性的 Cookie。 1Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; JavaScript和Cookie通过 Document.cookie 属性可创建新的 Cookie，也可通过该属性访问非 HttpOnly 标记的 Cookie。 123document.cookie = "yummy_cookie=choco";document.cookie = "tasty_cookie=strawberry";console.log(document.cookie); 标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。跨站脚本攻击 (XSS) 常常使用 JavaScript 的 Document.cookie API 窃取用户的 Cookie 信息，因此使用 HttpOnly 标记可以在一定程度上避免 XSS 攻击。 1Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnly]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Cookie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP状态码和首部]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FHTTP%E7%8A%B6%E6%80%81%E7%A0%81%E5%92%8C%E9%A6%96%E9%83%A8%2F</url>
    <content type="text"><![CDATA[HTTP是一个应用层协议，也是一个无状态的协议，由请求和响应构成，是一个标准的客户端服务器模型。通常承载于TCP协议之上，有时也承载于TLS或SSL协议层之上，这个时候，就成了我们常说的HTTPS。 HTTP状态码服务器返回的 响应报文 中第一行为状态行，包含了状态码以及原因短语，用来告知客户端请求的结果。 状态码 类别 原因短语 1XX Informational（信息性状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 1XX 信息 100 Continue ：表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。 2XX 成功 200 OK ：客户端请求成功 204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端往服务器发送信息，而不需要返回数据时使用。 206 Partial Content ：表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。 3XX 重定向 301 Moved Permanently ：永久性重定向 302 Found ：临时性重定向 303 See Other ：和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。 注：虽然 HTTP 协议规定 301、302 状态下重定向时不允许把 POST 方法改成 GET 方法，但是大多数浏览器都会在 301、302 和 303 状态下的重定向把 POST 方法改成 GET 方法。 304 Not Modified ：如果请求报文首部包含一些条件，例如：If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。 307 Temporary Redirect ：临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。 4XX 客户端错误 400 Bad Request ：请求报文中存在语法错误。 401 Unauthorized ：该状态码表示发送的请求需要有认证信息（BASIC 认证、DIGEST 认证）。如果之前已进行过一次请求，则表示用户认证失败。 403 Forbidden ：请求被拒绝。 404 Not Found ：请求的资源不存在 5XX 服务器错误 500 Internal Server Error ：服务器正在执行请求时发生错误。 503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。 HTTP首部通用首部字段 首部字段名 说明 Cache-Control 控制缓存的行为 Connection 控制不再转发给代理的首部字段、管理持久连接 Date 创建报文的日期时间 Pragma 报文指令 Trailer 报文末端的首部一览 Transfer-Encoding 指定报文主体的传输编码方式 Upgrade 升级为其他协议 Via 代理服务器的相关信息 Warning 错误通知 请求首部字段 首部字段名 说明 Accept 用户代理可处理的媒体类型 Accept-Charset 优先的字符集 Accept-Encoding 优先的内容编码 Accept-Language 优先的语言（自然语言） Authorization Web 认证信息 Expect 期待服务器的特定行为 From 用户的电子邮箱地址 Host 请求资源所在服务器 If-Match 比较实体标记（ETag） If-Modified-Since 比较资源的更新时间 If-None-Match 比较实体标记（与 If-Match 相反） If-Range 资源未更新时发送实体 Byte 的范围请求 If-Unmodified-Since 比较资源的更新时间（与 If-Modified-Since 相反） Max-Forwards 最大传输逐跳数 Proxy-Authorization 代理服务器要求客户端的认证信息 Range 实体的字节范围请求 Referer 对请求中 URI 的原始获取方 TE 传输编码的优先级 User-Agent HTTP 客户端程序的信息 响应首部字段 首部字段名 说明 Accept-Ranges 是否接受字节范围请求 Age 推算资源创建经过时间 ETag 资源的匹配信息让服务端给每一个页面分配一个唯一的编号，通过编号来区分当前这个页面是否是最新的 Location 令客户端重定向至指定 URI Proxy-Authenticate 代理服务器对客户端的认证信息 Retry-After 对再次发起请求的时机要求 Server HTTP 服务器的安装信息 Vary 代理服务器缓存的管理信息 WWW-Authenticate 服务器对客户端的认证信息 实体首部字段 首部字段名 说明 Allow 资源可支持的 HTTP 方法 Content-Encoding 实体主体适用的编码方式 Content-Language 实体主体的自然语言 Content-Length 实体主体的大小 Content-Location 替代对应资源的 URI Content-MD5 实体主体的报文摘要 Content-Range 实体主体的位置范围 Content-Type 实体主体的媒体类型 Expires 实体主体过期的日期时间 Last-Modified 资源的最后修改日期时间 Cache-ControlHTTP/1.1 通过 Cache-Control 首部字段来控制缓存。 Cache-Control: no-store指令规定不能对请求或响应的任何一部分进行缓存。 Cache-Control: no-cache指令规定缓存服务器需要先向源服务器验证缓存资源的有效性，只有当缓存资源有效才将能使用该缓存对客户端的请求进行响应。 Cache-Control: max-age 指令出现在请求报文中，并且缓存资源的缓存时间小于该指令指定的时间，那么就能接受该缓存；出现在响应报文中，表示缓存资源在缓存服务器中保存的时间。 Expires 首部字段也可以用于告知缓存服务器该资源什么时候会过期。 缓存验证需要先了解 ETag 首部字段的含义，它是资源的唯一标识。URL 不能唯一表示资源。可以将缓存资源的 ETag 值放入 If-None-Match 首部，服务器收到该请求后，判断缓存资源的 ETag 值和资源的最新 ETag 值是否一致，如果一致则表示缓存资源有效，返回 304 Not Modified。 Last-Modified 首部字段也可以用于缓存验证，它包含在源服务器发送的响应报文中，指示源服务器对资源的最后修改时间。但是它是一种弱校验器，因为只能精确到一秒，所以它通常作为 ETag 的备用方案。如果响应首部字段里含有这个信息，客户端可以在后续的请求中带上 If-Modified-Since 来验证缓存。服务器只在所请求的资源在给定的日期时间之后对内容进行过修改的情况下才会将资源返回，状态码为 200 OK。如果请求的资源从那时起未经修改，那么返回一个不带有消息主体的 304 Not Modified 响应。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS域名解析]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FDNS%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[DNS域名解析流程 用户输入域名按下回车 浏览器检查缓存中是否这个域名对应的解析过的IP地址 没有则继续 查找操作系统缓存 没有则继续 请求本地域名服务器LDNS 没有则继续 请求Root DNS Server，返回主域名服务器（gTLD server）地址 LDNS请求gTLD，gTLD返回Name Server域名服务器的地址 给LDNS LDNS查询Name Server 得到IP地址 LDNS返回IP地址给用户]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP协议]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FTCP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[TCP通信原理对于TCP通信来说，每个Socket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的缓冲区和缓冲区填充状态。 接收缓冲区把数据缓存到内核，若应用进程一直没有调用Socket的read方法进行读取，那么该数据会一直被缓存在接收缓冲区内，不管进程是否读取Socket，对发送端发来的数据都会经过内核接收并缓存到Socket的内核接受缓冲区。 TCP报文段首部格式 序号seq：用于对字节流进行编号，例如序号为 101，表示第一个字节的编号为 101，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 201。 确认号ack：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认ACK：当 ACK=1 时确认号字段有效，否则无效。TCP 规定在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步SYN：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。 终止FIN：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。 三次握手 第一次握手：Client 向 Server 发送连接请求报文SYN=1，随机产生一个值seq= x给Server，Client 进入SYN_SENT状态，等待Server确认。 第二次握手：Server 收到连接请求报文，由标志位SYN=1知道Client请求建立连接,Server 向Client 发送连接确认报文SYN=1，ACK=1，seq为 x+1，同时随机产生一个值seq= y,Server进入SYN_RCVD状态 第三次握手：Client 收到 Server 的连接确认报文后,检查ACK是否为1，ack是否为x+1,如果正确则还要向 Server 发出确认，ack为 y+1,将标志位ACK置为1。Server检查ack是否为y+1,ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，开始传输数据。 SYN攻击时一种典型的DDOS攻击，就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。 三次握手的原因第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。客户端发送的连接请求如果在网络中延迟，那么就会隔很长一段时间才能收到服务器端发回的连接确认。客户端等待一个超时重传时间之后，就会重新请求连接。但是这个滞留的连接请求最后还是会到达服务器，如果不进行三次握手，那么服务器就会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。 四次挥手 第一次挥手：Client 发送连接释放报文，FIN=1，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server 收到FIN后之后发出确认，此时 TCP 属于半关闭状态，Server 能向 Client 发送数据但是 Client 不能向 Server 发送数据。发送一个ACK给Client，ack为u+1后进入CLOSE_WAIT状态。 第三次挥手：当Server 不再需要连接时，发送连接释放报文，FIN=1用来关闭Server到Client的数据传送，Server进入LAST_ACK状态 第四次挥手：Client 收到后FIN后进入 TIME-WAIT 状态，发出确认，等待 2 MSL（最大报文存活时间）后释放连接。Server 收到 Client 的确认后进入CLOSED状态 四次挥手的原因客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文，就进入了 CLOSE-WAIT 状态。这个状态是为了让服务器端发送还未传送完毕的数据，传送完毕之后，服务器会发送 FIN 连接释放报文。 TIME_WAIT客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL。这么做有两个理由 确保最后一个确认报文能够到达。如果Server 没收到Client 发送来的确认报文，那么就会重新发送连接释放请求报文，Client 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。 滑动窗口窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收。 超时重发当发送者向接收者发包后，如果过了一段时间(超时时间)依然没有收到消息，就当做本次包丢失，需要重新补发。并且如果一次性发了三个包，只要最后一个包确认收到之后就默认前面两个也收到了。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile语法]]></title>
    <url>%2F2018%2F10%2F24%2FDocker%2FDockerfile%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[12345678910111213FROM scratch # 制作baseimageFROM centos # 使用baseimageLABEL version="1.0" # 定义metadataRUN set -ex; # 执行命令并创建新的imagelayerWORKDIR demo # 创建目录并进入，默认根目录/testADD hello / # 将本地文件添加到里面ADD test.tar.gz / # 添加到根目录并解压COPY docker-entrypoint.sh /usr/local/bin/ # 添加文件，但不能解压ENV GOSU_VERSION 1.7 # 设置常量 使用$GOSU_VERSION引用常量VOLUME /var/lib/mysql # 存储EXPOSE 3306 33060 # 网络CMD ["mysqld"] # 设置容器启动后默认执行的命令和参数ENTRYPOINT ["docker-entrypoint.sh"] # 设置容器启动时运行的命令 docker-library里有官方收录的Dockerfile，可以作为参考 12docker build -t imagename path # 构建imagedocker run imagename # 运行]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker简介]]></title>
    <url>%2F2018%2F10%2F23%2FDocker%2FDocker%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Docker是一种虚拟化技术解决开发环境和生产环境环境一致的问题，通过Docker可以将程序运行的环境也纳入到版本控制中，解决一些项目交付时的麻烦。 解决的问题由于不同的机器有不同的操作系统，以及不同的库和组件，在将一个应用部署到多台机器上需要进行大量的环境配置操作。 Docker 主要解决环境配置问题，它是一种虚拟化技术，对进程进行隔离，被隔离的进程独立于宿主操作系统和其它隔离的进程。使用 Docker 可以不修改应用程序代码，不需要开发人员学习特定环境下的技术，就能够将现有的应用程序部署在其他机器中 优势 启动快 容器里面的应用，直接就是底层系统的一个进程，而不是虚拟机内部的进程。所以启动容器相当于启动本机的一个进程，而不是启动一个操作系统，速度就快很多 资源占用少 容器只占用需要的资源，不占用那些没有用到的资源；虚拟机由于是完整的操作系统，不可避免要占用所有资源。另外多个容器可以共享资源，虚拟机都是独享资源 体积小 容器只要包含用到的组件即可，而虚拟机是整个操作系统的打包，所以容器文件比虚拟机文件要小很多 容易迁移 提供一致性的运行环境，可以在不同的机器上进行迁移，而不用担心环境变化导致无法运行 容易维护 使用分层技术和镜像，使得应用可以更容易复用重复部分。复用程度越高，维护工作也越容易 容易扩展 可以使用基础镜像进一步扩展得到新的镜像，并且官方和开源社区提供了大量的镜像，通过扩展这些镜像可以非常容易得到我们想要的镜像 底层技术支持Namespaces：通过Linux的Namespaces对不同的容器实现了隔离，包括进程、网络等信息。通过挂载点映射和宿主机的目录。 ControlGroups：隔离宿主机器上的物理资源，例如CPU、内存、磁盘I/O和网络带宽。 UnionFileSystems：container和image的分层。 image可以使用docker images ls命令查看本机的image 文件和metadata的集合 分层，每层都可以添加、改变、删除文件 image本身是只读的 获取方式 可以通过Dockerfile构建自己的image，使用docker build获取image 1234FROM ubuntu:15.04COPY . /appRUN make /appCMD python /app/app.py docker pull rabbitmq:management拉取image container可以使用docker container ls命令查看本机的container 通过image创建 类比面向对象：image是类，container是实例 负责运行 docker run -it xxx 可以进container里面进行一些操作 镜像与容器镜像是一种静态的结构，可以看成面向对象里面的类，而容器是镜像的一个实例。 镜像包含着容器运行时所需要的代码以及其它组件，它是一种分层结构，每一层都是只读的（read-only layers）。构建镜像时，会一层一层构建，前一层是后一层的基础。镜像的这种分层存储结构很适合镜像的复用以及定制。 构建容器时，通过在镜像的基础上添加一个可写层（writable layer），用来保存着容器运行过程中的修改。 通过容器技术有效分配和管理物理资源，实现资源隔离 通过镜像技术从系统环境开始，自底至上打包应用]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS环境下安装Docker]]></title>
    <url>%2F2018%2F10%2F23%2FDocker%2FCentOS%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85Docker%2F</url>
    <content type="text"><![CDATA[官方安装文档 123456789101112131415yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engineyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum install -y docker-cesystemctl start dockerdocker version 也可以使用阿里的镜像，将第三步指令替换成： 1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库锁]]></title>
    <url>%2F2018%2F10%2F22%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%94%81%2F</url>
    <content type="text"><![CDATA[排它锁（Exclusive），简写为 X 锁，又称写锁。 共享锁（Shared），简写为 S 锁，又称读锁。 IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。 在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。 共享锁(S) 排他锁(X) 意向共享锁(IS) 意向排他锁(IX) 共享锁 兼容 冲突 兼容 冲突 排他锁 冲突 冲突 冲突 冲突 意向共享锁 兼容 冲突 兼容 兼容 意向排他锁 冲突 冲突 兼容 兼容 任意 IS/IX 锁之间都是兼容的，因为它们只是表示想要对表加锁，而不是真正加锁； S 锁只与 S 锁和 IS 锁兼容 三级封锁协议一级封锁协议：写-写；事务要修改数据时必须加 X 锁，直到事务结束才释放锁。可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。 二级封锁协议：写-读；在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。可以解决读脏数据问题，因为如果一个事务在对数据进行修改，根据一级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。 三级封锁协议：读-写；在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。可以解决不可重复读的问题，因为读数据时，其它事务不能对数据加 X 锁，从而避免了在读的期间数据发生改变。 InnoDB 锁Record Locks锁定一个记录上的索引，而不是记录本身。 如果我们对加了索引的列作为 SQL 中 WHERE 语句的过滤条件，那么 InnoDB 就可以通过索引建立的 B+ 树找到行记录并添加索引，但是如果使用的不是索引列作为过滤条件时，由于 InnoDB 不知道待修改的记录具体存放的位置，也无法对将要修改哪条记录提前做出判断就会锁定整个表。 Gap Locks记录锁是在存储引擎中最为常见的锁，除了记录锁之外，InnoDB 中还存在间隙锁（Gap Lock），间隙锁是对索引记录中的一段连续区域的锁；当使用类似下面的 SQL 语句时，就会阻止其他事务向表中插入 id=15 的记录，因为整个范围都被间隙锁锁定了。 1SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE; 间隙锁是存储引擎对于性能和并发做出的权衡，并且只用于某些事务隔离级别。 虽然间隙锁中也分为共享锁和互斥锁，不过它们之间并不是互斥的，也就是不同的事务可以同时持有一段相同范围的共享锁和互斥锁，它唯一阻止的就是其他事务向这个范围中添加新的记录。 Next-Key LocksNext-Key Locks 是 MySQL 的 InnoDB 存储引擎的一种锁实现。 可重复读： 同一个事务里确保每次读取的时候，获得的是同样的数据 MVCC 不能解决幻读的问题，Next-Key Locks 就是为了解决这个问题而存在的。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key Locks 可以解决幻读问题。 它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间： 12345(negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) 既然叫 Next-Key 锁，锁定的应该是当前值和后面的范围，但是实际上却不是，Next-Key 锁锁定的是当前值和前面的范围。 当我们更新一条记录，比如 SELECT*FROM users WHERE age=30FOR UPDATE;，InnoDB 不仅会在范围 (21,30] 上加 Next-Key 锁，还会在这条记录后面的范围 (30,40] 加间隙锁，所以插入 (21,40]范围内的记录都会被锁定。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库引擎]]></title>
    <url>%2F2018%2F10%2F22%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"><![CDATA[Innodb Myisam 存储文件 .frm表定义文件 ibd数据文件 .frm表定义文件 .myd数据文件 .myi索引文件 锁 表锁、行锁 表锁 事务 ACID 不支持 CURD 读、写 读多 count 扫表 专门存储的地方 索引结构 B+Tree B+Tree]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Innodb</tag>
        <tag>Myisam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BASE理论]]></title>
    <url>%2F2018%2F10%2F22%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FBASE%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[BASE理论是对CAP理论中一致性和可用性权衡的结果，如果无法做到强一致性，那就要采取合适的方法使系统达到最终一致性。传统的数据库系统要求强一致性(ACID)，BASE理论强调通过牺牲强一致性来达到可用性。在实际业务场景中，要结合业务对一致性的要求，将ACID和BASE结合起来使用。 基本可用(BasicallyAvailable)分布式系统在出现故障的时候，保证核心功能可用，允许损失部分可用性。 软状态(SoftState)允许系统中的数据存在中间状态，即系统不同节点的数据副本之间进行同步的过程存在时间延迟 最终一致性(EventuallyConsistent)系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>BASE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAP理论]]></title>
    <url>%2F2018%2F10%2F22%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FCAP%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[分布式系统不可能同时满足一致性(Consistency)、可用性(Availability)、分区容忍性(Partition Tolerance)，最多只能同时满足其中两项，这就是CAP理论。在分布式系统中分区容忍性必不可少，所以CAP理论实际上是要在可用性和一致性之间做取舍。 一致性多个数据副本能保持一致，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。如果系统的一个数据更新成功之后，所有用户都能够读取到最新的值，系统就被认为具有强一致性。 可用性分布式系统在面对各种异常时都可以提供正常服务，对于用户的每一个操作、请求总是能够在有限的时间内返回结果。 分区容忍性分布式系统在遇到任何网络分区故障的时候，仍然能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>CAP</tag>
      </tags>
  </entry>
</search>
